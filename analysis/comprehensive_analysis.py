#!/usr/bin/env python3
"""
ç»¼åˆåˆ†æå™¨ - é‡æ„åçš„é›†æˆç‰ˆæœ¬ + ç“¶é¢ˆæ¨¡å¼æ”¯æŒ
æ•´åˆRPCæ·±åº¦åˆ†æå™¨ã€éªŒè¯å™¨æ—¥å¿—åˆ†æå™¨å’ŒQPSåˆ†æå™¨
æä¾›ç»Ÿä¸€çš„åˆ†æå…¥å£å’Œå®Œæ•´çš„æŠ¥å‘Šç”Ÿæˆ
æ”¯æŒç“¶é¢ˆæ£€æµ‹æ¨¡å¼å’Œæ—¶é—´çª—å£åˆ†æ
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import glob
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.unified_logger import get_logger
import json
import argparse
import random
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, Tuple

class DataProcessor:
    """æ•°æ®å¤„ç†å·¥å…·ç±» - è§£å†³æ•°æ®å¤„ç†é‡å¤ä»£ç """
    
    @staticmethod
    def validate_dataframe_column(df: pd.DataFrame, column: str) -> bool:
        """éªŒè¯DataFrameæ˜¯å¦åŒ…å«æŒ‡å®šåˆ—ä¸”æœ‰æ•°æ®"""
        return column in df.columns and len(df) > 0 and not df[column].empty
    
    @staticmethod
    def safe_calculate_mean(df: pd.DataFrame, column: str) -> float:
        """å®‰å…¨è®¡ç®—åˆ—çš„å¹³å‡å€¼ - è§£å†³é‡å¤çš„å‡å€¼è®¡ç®—ä»£ç """
        if DataProcessor.validate_dataframe_column(df, column):
            return df[column].mean()
        return 0.0
    
    @staticmethod
    def safe_calculate_max(df: pd.DataFrame, column: str) -> float:
        """å®‰å…¨è®¡ç®—åˆ—çš„æœ€å¤§å€¼"""
        if DataProcessor.validate_dataframe_column(df, column):
            return df[column].max()
        return 0.0

class FileManager:
    """æ–‡ä»¶ç®¡ç†å·¥å…·ç±» - æ™ºèƒ½æ–‡ä»¶ä¿å­˜ï¼Œæ”¯æŒå¤‡ä»½å’Œå›ºå®šåç§°"""
    
    def __init__(self, output_dir: str, session_timestamp: str):
        self.output_dir = output_dir
        self.session_timestamp = session_timestamp
        self.reports_dir = os.path.join(output_dir, 'reports')
        os.makedirs(self.reports_dir, exist_ok=True)
    
    def save_chart_with_backup(self, chart_name: str, plt_figure) -> str:
        """ä¿å­˜å›¾è¡¨ï¼ŒåŒæ—¶åˆ›å»ºå¤‡ä»½å’Œå½“å‰ç‰ˆæœ¬"""
        # å›ºå®šåç§°æ–‡ä»¶ï¼ˆä¾›å…¶ä»–ç»„ä»¶å¼•ç”¨ï¼‰
        current_path = os.path.join(self.reports_dir, f'{chart_name}.png')
        
        # å¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶
        backup_path = os.path.join(self.reports_dir, f'{chart_name}_{self.session_timestamp}.png')
        
        # ä¿å­˜ä¸¤ä¸ªç‰ˆæœ¬
        plt_figure.savefig(current_path, dpi=300, bbox_inches='tight')
        plt_figure.savefig(backup_path, dpi=300, bbox_inches='tight')
        
        logger.info(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜: {current_path} (å½“å‰ç‰ˆæœ¬)")
        logger.info(f"ğŸ“Š å¤‡ä»½å·²åˆ›å»º: {backup_path}")
        
        return current_path
    
    def save_report_with_backup(self, report_name: str, content: str) -> str:
        """ä¿å­˜æŠ¥å‘Šï¼ŒåŒæ—¶åˆ›å»ºå¤‡ä»½å’Œå½“å‰ç‰ˆæœ¬"""
        # å›ºå®šåç§°æ–‡ä»¶ï¼ˆä¾›å…¶ä»–ç»„ä»¶å¼•ç”¨ï¼‰
        current_path = os.path.join(self.reports_dir, f'{report_name}.md')
        
        # å¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶
        backup_path = os.path.join(self.reports_dir, f'{report_name}_{self.session_timestamp}.md')
        
        # ä¿å­˜ä¸¤ä¸ªç‰ˆæœ¬
        with open(current_path, 'w', encoding='utf-8') as f:
            f.write(content)
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {current_path} (å½“å‰ç‰ˆæœ¬)")
        logger.info(f"ğŸ“„ å¤‡ä»½å·²åˆ›å»º: {backup_path}")
        
        return current_path

class OperationLogger:
    """æ“ä½œæ—¥å¿—è£…é¥°å™¨ - ç»Ÿä¸€æ—¥å¿—æ ¼å¼ï¼Œè§£å†³printè¯­å¥é‡å¤é—®é¢˜"""
    
    @staticmethod
    def log_operation(operation_name: str, emoji: str = "ğŸ“Š"):
        """è®°å½•æ“ä½œçš„è£…é¥°å™¨"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                print(f"\n{emoji} {operation_name}...")
                try:
                    result = func(*args, **kwargs)
                    logger.info(f"âœ… {operation_name} completed successfully")
                    return result
                except Exception as e:
                    logger.error(f"âŒ {operation_name} failed: {e}")
                    raise
            return wrapper
        return decorator

# é…ç½®æ—¥å¿—
logger = get_logger(__name__)

# æ·»åŠ è·¯å¾„ä»¥æ”¯æŒé‡ç»„åçš„ç›®å½•ç»“æ„
from pathlib import Path

# ä½¿ç”¨æ›´å¥å£®çš„è·¯å¾„ç®¡ç†
current_dir = Path(__file__).parent
project_root = current_dir.parent
utils_dir = project_root / 'utils'
visualization_dir = project_root / 'visualization'

# æ·»åŠ è·¯å¾„åˆ°sys.path
for path in [str(utils_dir), str(visualization_dir)]:
    if path not in sys.path:
        sys.path.insert(0, path)

# å¯¼å…¥æ‹†åˆ†åçš„æ¨¡å—
try:
    from rpc_deep_analyzer import RpcDeepAnalyzer
    from validator_log_analyzer import ValidatorLogAnalyzer
    from qps_analyzer import SolanaQPSAnalyzer
    logger.info("âœ… æ‰€æœ‰åˆ†ææ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    logger.error(f"âŒ åˆ†ææ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

class BottleneckAnalysisMode:
    """ç“¶é¢ˆåˆ†ææ¨¡å¼é…ç½®"""
    
    def __init__(self, bottleneck_info: Optional[Dict] = None):
        # åªæœ‰å½“æä¾›äº†æœ‰æ•ˆçš„ç“¶é¢ˆä¿¡æ¯æ—¶æ‰å¯ç”¨
        self.enabled = bottleneck_info is not None and len(bottleneck_info) > 0
        self.bottleneck_info = bottleneck_info or {}
        self.bottleneck_time = None
        self.analysis_window = None
        self.max_qps = 0
        self.bottleneck_qps = 0
        
        if self.enabled:
            self._parse_bottleneck_info()
    
    def _parse_bottleneck_info(self):
        """è§£æç“¶é¢ˆä¿¡æ¯"""
        try:
            self.bottleneck_time = self.bottleneck_info.get('detection_time')
            self.analysis_window = self.bottleneck_info.get('analysis_window', {})
            self.max_qps = self.bottleneck_info.get('max_qps_achieved', 0)
            self.bottleneck_qps = self.bottleneck_info.get('bottleneck_qps', 0)
            
            logger.info(f"ğŸš¨ ç“¶é¢ˆåˆ†ææ¨¡å¼: æœ€å¤§QPS={self.max_qps}, ç“¶é¢ˆQPS={self.bottleneck_qps}")
        except Exception as e:
            logger.error(f"âŒ ç“¶é¢ˆä¿¡æ¯è§£æå¤±è´¥: {e}")
            self.enabled = False

# å­—æ®µæ˜ å°„å™¨å·²ç§»é™¤
try:
    # ä¸å†ä½¿ç”¨å­—æ®µæ˜ å°„å™¨
    FIELD_MAPPER_AVAILABLE = False
    logger.info("âœ… ä½¿ç”¨ç›´æ¥å­—æ®µè®¿é—®æ¨¡å¼")
except ImportError as e:
    FIELD_MAPPER_AVAILABLE = False
    logger.warning(f"âš ï¸  å­—æ®µæ˜ å°„å™¨ä¸å¯ç”¨: {e}ï¼Œå°†ä½¿ç”¨åŸå§‹å­—æ®µå")


class ComprehensiveAnalyzer:
    """ç»¼åˆåˆ†æå™¨ - æ•´åˆæ‰€æœ‰åˆ†æåŠŸèƒ½çš„ä¸»æ§åˆ¶å™¨ + ç“¶é¢ˆæ¨¡å¼æ”¯æŒ"""

    def __init__(self, output_dir: Optional[str] = None, benchmark_mode: str = "standard", bottleneck_mode: Optional[BottleneckAnalysisMode] = None):
        """
        åˆå§‹åŒ–ç»¼åˆåˆ†æå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œå°†ä»ç¯å¢ƒå˜é‡è·å–ï¼‰
            benchmark_mode: åŸºå‡†æµ‹è¯•æ¨¡å¼ (quick/standard/intensive)
            bottleneck_mode: ç“¶é¢ˆåˆ†ææ¨¡å¼é…ç½®
        """
        if output_dir is None:
            output_dir = os.environ.get('DATA_DIR', os.path.join(os.path.expanduser('~'), 'blockchain-node-benchmark-result'))
        
        # åˆ›å»ºä¼šè¯æ—¶é—´æˆ³ï¼Œç”¨äºå¤‡ä»½æ–‡ä»¶å‘½å
        self.session_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        self.output_dir = output_dir
        self.benchmark_mode = benchmark_mode
        self.csv_file = self.get_latest_csv()
        self.bottleneck_mode = bottleneck_mode or BottleneckAnalysisMode()
        
        # åˆå§‹åŒ–å„ä¸ªåˆ†æå™¨
        self.qps_analyzer = SolanaQPSAnalyzer(output_dir, benchmark_mode, self.bottleneck_mode.enabled)
        self.log_analyzer = ValidatorLogAnalyzer()
        self.rpc_deep_analyzer = RpcDeepAnalyzer(self.csv_file)
        
        # åˆå§‹åŒ–æ–‡ä»¶ç®¡ç†å™¨
        self.file_manager = FileManager(self.output_dir, self.session_timestamp)
        
        logger.info(f"ğŸ” åˆå§‹åŒ–ç»¼åˆåˆ†æå™¨ï¼Œè¾“å‡ºç›®å½•: {output_dir}")
        if self.bottleneck_mode.enabled:
            logger.info(f"ğŸš¨ ç“¶é¢ˆåˆ†ææ¨¡å¼å·²å¯ç”¨")

    def get_latest_csv(self) -> Optional[str]:
        """è·å–æœ€æ–°çš„CSVç›‘æ§æ–‡ä»¶"""
        csv_files = glob.glob(f"{self.output_dir}/logs/*.csv")
        return max(csv_files, key=os.path.getctime) if csv_files else None

    @staticmethod
    def filter_data_by_time_window(df: pd.DataFrame, start_time: str, end_time: str) -> pd.DataFrame:
        """æ ¹æ®æ—¶é—´çª—å£è¿‡æ»¤æ•°æ® - é™æ€æ–¹æ³•"""
        try:
            if 'timestamp' not in df.columns:
                logger.warning("âš ï¸ æ•°æ®ä¸­æ²¡æœ‰timestampåˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´çª—å£è¿‡æ»¤")
                return df
            
            # è½¬æ¢æ—¶é—´æˆ³
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            start_dt = pd.to_datetime(start_time)
            end_dt = pd.to_datetime(end_time)
            
            # è¿‡æ»¤æ•°æ®
            filtered_df = df[(df['timestamp'] >= start_dt) & (df['timestamp'] <= end_dt)]
            logger.info(f"ğŸ“Š æ—¶é—´çª—å£è¿‡æ»¤: {len(df)} -> {len(filtered_df)} æ¡è®°å½•")
            
            return filtered_df
        except Exception as e:
            logger.error(f"âŒ æ—¶é—´çª—å£è¿‡æ»¤å¤±è´¥: {e}")
            return df

    def analyze_bottleneck_correlation(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æç“¶é¢ˆç›¸å…³æ€§"""
        if not self.bottleneck_mode.enabled:
            return {}
        
        try:
            analysis_result = {
                'bottleneck_detected': True,
                'max_qps': self.bottleneck_mode.max_qps,
                'bottleneck_qps': self.bottleneck_mode.bottleneck_qps,
                'performance_drop': 0.0,  # ä½¿ç”¨floatç±»å‹ä¿æŒä¸€è‡´æ€§
                'correlations': {},
                'bottleneck_factors': []
            }
            
            # è®¡ç®—æ€§èƒ½ä¸‹é™
            if self.bottleneck_mode.max_qps > 0:
                performance_drop = ((self.bottleneck_mode.bottleneck_qps - self.bottleneck_mode.max_qps) / 
                                  self.bottleneck_mode.max_qps * 100)
                analysis_result['performance_drop'] = performance_drop
            
            # åˆ†æå„æŒ‡æ ‡ä¸QPSçš„ç›¸å…³æ€§
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            qps_column = None
            
            # å¯»æ‰¾QPSåˆ—
            for col in ['current_qps', 'qps', 'requests_per_second']:
                if col in df.columns:
                    qps_column = col
                    break
            
            if qps_column:
                for col in numeric_columns:
                    if col != qps_column and len(df[col].dropna()) > 1:
                        try:
                            correlation = df[qps_column].corr(df[col])
                            if not np.isnan(correlation):
                                analysis_result['correlations'][col] = correlation
                                
                                # è¯†åˆ«ç“¶é¢ˆå› å­
                                if abs(correlation) > 0.7:
                                    analysis_result['bottleneck_factors'].append({
                                        'metric': col,
                                        'correlation': correlation,
                                        'impact': 'high' if abs(correlation) > 0.8 else 'medium'
                                    })
                        except Exception as e:
                            logger.warning(f"âš ï¸ è®¡ç®—{col}ç›¸å…³æ€§å¤±è´¥: {e}")
            
            logger.info(f"ğŸ” ç“¶é¢ˆç›¸å…³æ€§åˆ†æå®Œæˆï¼Œå‘ç°{len(analysis_result['bottleneck_factors'])}ä¸ªå…³é”®å› å­")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ ç“¶é¢ˆç›¸å…³æ€§åˆ†æå¤±è´¥: {e}")
            return {}

    def generate_bottleneck_analysis_chart(self, df: pd.DataFrame, bottleneck_analysis: Dict[str, Any]) -> Optional[plt.Figure]:
        """ç”Ÿæˆç“¶é¢ˆåˆ†æä¸“é¡¹å›¾è¡¨"""
        if not self.bottleneck_mode.enabled or not bottleneck_analysis:
            return None
        
        try:
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle('ğŸš¨ Bottleneck Analysis Dashboard', fontsize=16, fontweight='bold', color='red')
            
            # 1. QPSæ€§èƒ½æ›²çº¿ + ç“¶é¢ˆæ ‡è®°
            if 'current_qps' in df.columns and len(df) > 0:
                axes[0, 0].plot(df.index, df['current_qps'], 'b-', alpha=0.7, label='QPS')
                
                # æ ‡è®°æœ€å¤§æˆåŠŸQPSå’Œç“¶é¢ˆQPS
                max_qps = bottleneck_analysis.get('max_qps', 0)
                bottleneck_qps = bottleneck_analysis.get('bottleneck_qps', 0)
                
                if max_qps > 0:
                    axes[0, 0].axhline(y=max_qps, color='green', linestyle='--', 
                                     label=f'Max Successful QPS: {max_qps}')
                if bottleneck_qps > 0:
                    axes[0, 0].axhline(y=bottleneck_qps, color='red', linestyle='--', 
                                     label=f'Bottleneck QPS: {bottleneck_qps}')
                
                axes[0, 0].set_title('QPS Performance with Bottleneck Markers')
                axes[0, 0].set_xlabel('Time')
                axes[0, 0].set_ylabel('QPS')
                axes[0, 0].legend()
                axes[0, 0].grid(True, alpha=0.3)
            
            # 2. ç“¶é¢ˆå› å­ç›¸å…³æ€§
            correlations = bottleneck_analysis.get('correlations', {})
            if correlations:
                factors = list(correlations.keys())[:10]  # å–å‰10ä¸ª
                corr_values = [correlations[f] for f in factors]
                
                colors = ['red' if abs(c) > 0.7 else 'orange' if abs(c) > 0.5 else 'blue' for c in corr_values]
                axes[0, 1].barh(factors, corr_values, color=colors, alpha=0.7)
                axes[0, 1].set_title('Bottleneck Factor Correlations')
                axes[0, 1].set_xlabel('Correlation with QPS')
                axes[0, 1].axvline(x=0, color='black', linestyle='-', alpha=0.3)
                axes[0, 1].grid(True, alpha=0.3)
            
            # 3. æ€§èƒ½ä¸‹é™åˆ†æ
            performance_drop = bottleneck_analysis.get('performance_drop', 0.0)
            if performance_drop != 0:
                # ä»ç“¶é¢ˆåˆ†æç»“æœæˆ–ç“¶é¢ˆæ¨¡å¼å¯¹è±¡ä¸­è·å–QPSå€¼
                max_qps = bottleneck_analysis.get('max_qps', self.bottleneck_mode.max_qps)
                bottleneck_qps = bottleneck_analysis.get('bottleneck_qps', self.bottleneck_mode.bottleneck_qps)
                
                categories = ['Max QPS', 'Bottleneck QPS']
                values = [max_qps, bottleneck_qps]
                colors = ['green', 'red']
                
                bars = axes[1, 0].bar(categories, values, color=colors, alpha=0.7)
                axes[1, 0].set_title(f'Performance Drop: {performance_drop:.1f}%')
                axes[1, 0].set_ylabel('QPS')
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, value in zip(bars, values):
                    axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,
                                   f'{value}', ha='center', va='bottom', fontweight='bold')
            
            # 4. ç“¶é¢ˆå› å­é‡è¦æ€§
            bottleneck_factors = bottleneck_analysis.get('bottleneck_factors', [])
            if bottleneck_factors:
                factor_names = [f['metric'] for f in bottleneck_factors]
                factor_impacts = [abs(f['correlation']) for f in bottleneck_factors]
                
                axes[1, 1].pie(factor_impacts, labels=factor_names, autopct='%1.1f%%', startangle=90)
                axes[1, 1].set_title('Bottleneck Factor Impact Distribution')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨ - ä½¿ç”¨æ–‡ä»¶ç®¡ç†å™¨ï¼ŒåŒæ—¶åˆ›å»ºå½“å‰ç‰ˆæœ¬å’Œå¤‡ä»½
            chart_path = self.file_manager.save_chart_with_backup('bottleneck_analysis_chart', plt)
            logger.info(f"ğŸ“Š ç“¶é¢ˆåˆ†æå›¾è¡¨å·²ä¿å­˜: {chart_path}")
            
            return fig
            
        except Exception as e:
            logger.error(f"âŒ ç“¶é¢ˆåˆ†æå›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def generate_ultimate_performance_charts(self, df: pd.DataFrame, 
                                           log_analysis: Dict[str, Any], 
                                           rpc_deep_analysis: Dict[str, Any]) -> Optional[plt.Figure]:
        """ç”Ÿæˆç»ˆææ€§èƒ½å›¾è¡¨ï¼Œæ•´åˆæ‰€æœ‰åˆ†æç»“æœ"""
        print("\nğŸ“ˆ Generating ultimate performance charts...")

        if len(df) == 0:
            print("âŒ No QPS data for chart generation")
            return None

        plt.style.use('default')
        fig, axes = plt.subplots(4, 2, figsize=(16, 24))
        fig.suptitle('Solana QPS Ultimate Performance Analysis Dashboard', fontsize=16, fontweight='bold')

        # 1. CPUä½¿ç”¨ç‡ vs QPS
        if len(df) > 0 and 'cpu_usage' in df.columns:
            axes[0, 0].plot(df['current_qps'], df['cpu_usage'], 'bo-', alpha=0.7, markersize=4)
            axes[0, 0].axhline(y=85, color='red', linestyle='--', alpha=0.8, label='Warning (85%)')
            axes[0, 0].set_title('CPU Usage vs QPS')
            axes[0, 0].set_xlabel('QPS')
            axes[0, 0].set_ylabel('CPU %')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)

        # 2. å†…å­˜ä½¿ç”¨ç‡ vs QPS
        if len(df) > 0 and 'mem_usage' in df.columns:
            axes[0, 1].plot(df['current_qps'], df['mem_usage'], 'go-', alpha=0.7, markersize=4)
            axes[0, 1].axhline(y=90, color='red', linestyle='--', alpha=0.8, label='Warning (90%)')
            axes[0, 1].set_title('Memory Usage vs QPS')
            axes[0, 1].set_xlabel('QPS')
            axes[0, 1].set_ylabel('Memory %')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

        # 3. RPCå»¶è¿Ÿ vs QPS
        if len(df) > 0 and 'rpc_latency_ms' in df.columns:
            axes[1, 0].plot(df['current_qps'], df['rpc_latency_ms'], 'ro-', alpha=0.7, markersize=4)
            axes[1, 0].axhline(y=1000, color='orange', linestyle='--', alpha=0.8, label='High Latency (1s)')
            axes[1, 0].set_title('RPC Latency vs QPS')
            axes[1, 0].set_xlabel('QPS')
            axes[1, 0].set_ylabel('Latency (ms)')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

        # 4. RPCé”™è¯¯ç‡ï¼ˆæ¥è‡ªæ—¥å¿—åˆ†æï¼‰
        rpc_analysis = log_analysis.get('rpc_analysis', {})
        if rpc_analysis:
            error_rate = 100 - rpc_analysis.get('success_rate', 100)
            axes[1, 1].bar(['RPC Success Rate', 'RPC Error Rate'],
                           [rpc_analysis.get('success_rate', 100), error_rate],
                           color=['green', 'red'], alpha=0.7)
            axes[1, 1].set_title('RPC Success vs Error Rate')
            axes[1, 1].set_ylabel('Percentage')
            axes[1, 1].grid(True, alpha=0.3)
        else:
            axes[1, 1].text(0.5, 0.5, 'No RPC Analysis Data', ha='center', va='center',
                            transform=axes[1, 1].transAxes, fontsize=12)
            axes[1, 1].set_title('RPC Analysis (No Data)')

        # 5. ç“¶é¢ˆäº‹ä»¶åˆ†å¸ƒ
        bottleneck_analysis = log_analysis.get('bottleneck_analysis', {})
        if bottleneck_analysis:
            bottleneck_types = ['RPC Thread', 'Compute Unit', 'Memory', 'I/O', 'Network']
            bottleneck_counts = [
                bottleneck_analysis.get('rpc_thread_saturation', 0),
                bottleneck_analysis.get('compute_unit_limits', 0),
                bottleneck_analysis.get('memory_pressure', 0),
                bottleneck_analysis.get('io_bottlenecks', 0),
                bottleneck_analysis.get('network_issues', 0)
            ]

            colors = ['red', 'orange', 'purple', 'brown', 'pink']
            bars = axes[2, 0].bar(bottleneck_types, bottleneck_counts, color=colors, alpha=0.7)
            axes[2, 0].set_title('Bottleneck Events Distribution')
            axes[2, 0].set_ylabel('Event Count')
            axes[2, 0].tick_params(axis='x', rotation=45)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, bottleneck_counts):
                if count > 0:
                    axes[2, 0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                                    str(count), ha='center', va='bottom')
        else:
            axes[2, 0].text(0.5, 0.5, 'No Bottleneck Data', ha='center', va='center',
                            transform=axes[2, 0].transAxes, fontsize=12)
            axes[2, 0].set_title('Bottleneck Analysis (No Data)')

        # 6. RPCæ–¹æ³•åˆ†å¸ƒ
        if rpc_analysis and 'method_distribution' in rpc_analysis:
            method_dist = rpc_analysis['method_distribution']
            if method_dist:
                methods = list(method_dist.keys())[:5]  # å‰5ä¸ªæ–¹æ³•
                counts = [method_dist[method] for method in methods]

                axes[2, 1].pie(counts, labels=methods, autopct='%1.1f%%', startangle=90)
                axes[2, 1].set_title('RPC Method Distribution')
            else:
                axes[2, 1].text(0.5, 0.5, 'No RPC Method Data', ha='center', va='center',
                                transform=axes[2, 1].transAxes, fontsize=12)
                axes[2, 1].set_title('RPC Methods (No Data)')
        else:
            axes[2, 1].text(0.5, 0.5, 'No RPC Method Data', ha='center', va='center',
                            transform=axes[2, 1].transAxes, fontsize=12)
            axes[2, 1].set_title('RPC Methods (No Data)')

        # 7. RPCå»¶è¿Ÿåˆ†å¸ƒ
        if len(df) > 0 and 'rpc_latency_ms' in df.columns:
            axes[3, 0].hist(df['rpc_latency_ms'], bins=30, alpha=0.7, color='purple')
            axes[3, 0].axvline(df['rpc_latency_ms'].mean(), color='red', linestyle='--',
                               label=f'Mean: {df["rpc_latency_ms"].mean():.1f}ms')
            axes[3, 0].axvline(df['rpc_latency_ms'].quantile(0.95), color='orange', linestyle='--',
                               label=f'P95: {df["rpc_latency_ms"].quantile(0.95):.1f}ms')
            axes[3, 0].set_title('RPC Latency Distribution')
            axes[3, 0].set_xlabel('Latency (ms)')
            axes[3, 0].set_ylabel('Frequency')
            axes[3, 0].legend()
            axes[3, 0].grid(True, alpha=0.3)

        # 8. æ€§èƒ½æ‚¬å´–å¯è§†åŒ–
        cliff_analysis = rpc_deep_analysis.get('performance_cliff', {})
        if cliff_analysis and len(df) > 0:
            qps_latency = df.groupby('current_qps')['rpc_latency_ms'].mean().reset_index()
            qps_latency = qps_latency.sort_values('current_qps')

            axes[3, 1].plot(qps_latency['current_qps'], qps_latency['rpc_latency_ms'], 'bo-', alpha=0.7)

            # æ ‡è®°æ‚¬å´–ç‚¹
            cliff_points = cliff_analysis.get('performance_degradation_qps', [])
            for cliff_qps in cliff_points:
                cliff_latency = qps_latency[qps_latency['current_qps'] == cliff_qps]['rpc_latency_ms']
                if len(cliff_latency) > 0:
                    axes[3, 1].scatter(cliff_qps, cliff_latency.iloc[0], color='red', s=100, marker='x',
                                       label='Performance Cliff')

            axes[3, 1].set_title('Performance Cliff Detection')
            axes[3, 1].set_xlabel('QPS')
            axes[3, 1].set_ylabel('Average Latency (ms)')
            axes[3, 1].grid(True, alpha=0.3)
            if cliff_points:
                axes[3, 1].legend()

        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨ - ä½¿ç”¨æ–‡ä»¶ç®¡ç†å™¨ï¼ŒåŒæ—¶åˆ›å»ºå½“å‰ç‰ˆæœ¬å’Œå¤‡ä»½
        chart_file = self.file_manager.save_chart_with_backup('comprehensive_analysis_charts', plt)
        print(f"âœ… Ultimate performance charts saved: {chart_file}")

        return fig

    def _evaluate_comprehensive_performance(self, benchmark_mode: str, max_qps: int, 
                                          bottlenecks: Dict[str, Any], avg_cpu: float, 
                                          avg_mem: float, avg_rpc: float,
                                          bottleneck_analysis: Dict[str, Any],
                                          rpc_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        åŸºäºç»¼åˆåˆ†æçš„ç§‘å­¦æ€§èƒ½è¯„ä¼°
        æ•´åˆQPSã€æ—¥å¿—åˆ†æã€RPCåˆ†æç­‰å¤šç»´åº¦æ•°æ®
        """
        
        # åªæœ‰æ·±åº¦åŸºå‡†æµ‹è¯•æ¨¡å¼æ‰èƒ½è¿›è¡Œå‡†ç¡®çš„æ€§èƒ½ç­‰çº§è¯„ä¼°
        if benchmark_mode != "intensive":
            return {
                'performance_level': 'æ— æ³•è¯„ä¼°',
                'performance_grade': 'N/A',
                'evaluation_reason': f'{benchmark_mode}åŸºå‡†æµ‹è¯•æ¨¡å¼æ— æ³•å‡†ç¡®è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ç­‰çº§ï¼Œéœ€è¦intensiveæ¨¡å¼è¿›è¡Œæ·±åº¦åˆ†æ',
                'evaluation_basis': 'insufficient_benchmark_depth',
                'max_sustainable_qps': max_qps,
                'recommendations': [
                    f'å½“å‰{benchmark_mode}åŸºå‡†æµ‹è¯•ä»…ç”¨äºå¿«é€ŸéªŒè¯',
                    'å¦‚éœ€å‡†ç¡®çš„æ€§èƒ½ç­‰çº§è¯„ä¼°ï¼Œè¯·ä½¿ç”¨intensiveåŸºå‡†æµ‹è¯•æ¨¡å¼',
                    'æ·±åº¦åŸºå‡†æµ‹è¯•å°†è§¦å‘ç³»ç»Ÿç“¶é¢ˆä»¥è·å¾—å‡†ç¡®çš„æ€§èƒ½è¯„ä¼°'
                ]
            }
        
        # ç»¼åˆç“¶é¢ˆåˆ†æ
        bottleneck_types = bottlenecks.get('detected_bottlenecks', [])
        rpc_issues = rpc_analysis.get('critical_issues', [])
        validator_issues = bottleneck_analysis.get('critical_patterns', [])
        
        # è®¡ç®—ç»¼åˆç“¶é¢ˆè¯„åˆ†
        comprehensive_score = ComprehensiveAnalyzer._calculate_comprehensive_bottleneck_score(
            bottleneck_types, avg_cpu, avg_mem, avg_rpc, rpc_issues, validator_issues
        )
        
        # åŸºäºç»¼åˆè¯„åˆ†çš„ç§‘å­¦ç­‰çº§è¯„ä¼°
        if comprehensive_score < 0.2:
            level = "ä¼˜ç§€"
            grade = "A (Excellent)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹è¡¨ç°ä¼˜ç§€ï¼Œå„é¡¹æŒ‡æ ‡å‡åœ¨æ­£å¸¸èŒƒå›´å†…"
            
        elif comprehensive_score < 0.4:
            level = "è‰¯å¥½"
            grade = "B (Good)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹è¡¨ç°è‰¯å¥½ï¼Œå­˜åœ¨è½»å¾®ç“¶é¢ˆæˆ–é—®é¢˜"
            
        elif comprehensive_score < 0.7:
            level = "ä¸€èˆ¬"
            grade = "C (Acceptable)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹è¡¨ç°ä¸€èˆ¬ï¼Œå­˜åœ¨æ˜æ˜¾ç“¶é¢ˆéœ€è¦å…³æ³¨"
            
        else:
            level = "éœ€è¦ä¼˜åŒ–"
            grade = "D (Needs Improvement)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³ä¼˜åŒ–"
        
        return {
            'performance_level': level,
            'performance_grade': grade,
            'evaluation_reason': reason,
            'evaluation_basis': 'comprehensive_intensive_analysis',
            'max_sustainable_qps': max_qps,
            'comprehensive_score': comprehensive_score,
            'bottleneck_types': bottleneck_types,
            'rpc_issues_count': len(rpc_issues),
            'validator_issues_count': len(validator_issues),
            'recommendations': ComprehensiveAnalyzer._generate_comprehensive_recommendations(
                bottleneck_types, rpc_issues, validator_issues, comprehensive_score, max_qps
            )
        }
    
    @staticmethod
    def _calculate_comprehensive_bottleneck_score(bottleneck_types: list, 
                                                avg_cpu: float, avg_mem: float, avg_rpc: float,
                                                rpc_issues: list, validator_issues: list) -> float:
        """è®¡ç®—ç»¼åˆç“¶é¢ˆä¸¥é‡ç¨‹åº¦è¯„åˆ† - é™æ€æ–¹æ³•"""
        
        total_score = 0.0
        
        # ç³»ç»Ÿèµ„æºç“¶é¢ˆè¯„åˆ† (æƒé‡: 0.4)
        resource_score = 0.0
        if 'CPU' in bottleneck_types:
            resource_score += 0.15 * (1.5 if avg_cpu > 90 else 1.0)
        if 'Memory' in bottleneck_types:
            resource_score += 0.15 * (1.5 if avg_mem > 95 else 1.0)
        if 'EBS' in bottleneck_types:
            resource_score += 0.1
        
        # RPCé—®é¢˜è¯„åˆ† (æƒé‡: 0.3)
        rpc_score = min(len(rpc_issues) * 0.1, 0.3)
        if avg_rpc > 2000:
            rpc_score *= 1.5
        
        # éªŒè¯å™¨é—®é¢˜è¯„åˆ† (æƒé‡: 0.3)
        validator_score = min(len(validator_issues) * 0.1, 0.3)
        
        total_score = resource_score + rpc_score + validator_score
        
        return min(total_score, 1.0)
    
    @staticmethod
    def _generate_comprehensive_capacity_assessment(performance_evaluation: Dict[str, Any], max_qps: int) -> str:
        """åŸºäºç»¼åˆæ€§èƒ½è¯„ä¼°ç”Ÿæˆå®¹é‡è¯„ä¼° - é™æ€æ–¹æ³•"""
        performance_level = performance_evaluation.get('performance_level', 'æœªçŸ¥')
        comprehensive_score = performance_evaluation.get('comprehensive_score', 0)
        
        if performance_level == "ä¼˜ç§€":
            return f"å½“å‰é…ç½®å¯ç¨³å®šå¤„ç†é«˜è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œç»¼åˆè¯„åˆ†: {comprehensive_score:.3f})"
        elif performance_level == "è‰¯å¥½":
            return f"å½“å‰é…ç½®å¯å¤„ç†ä¸­é«˜è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œå­˜åœ¨è½»å¾®é—®é¢˜)"
        elif performance_level == "ä¸€èˆ¬":
            return f"å½“å‰é…ç½®é€‚åˆä¸­ç­‰è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œå­˜åœ¨æ˜æ˜¾é—®é¢˜)"
        elif performance_level == "éœ€è¦ä¼˜åŒ–":
            return f"å½“å‰é…ç½®éœ€è¦ä¼˜åŒ–ä»¥å¤„ç†é«˜è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œå­˜åœ¨ä¸¥é‡é—®é¢˜)"
        else:
            return f"éœ€è¦intensiveåŸºå‡†æµ‹è¯•æ¨¡å¼è¿›è¡Œå‡†ç¡®çš„å®¹é‡è¯„ä¼°"

    @staticmethod
    def _generate_comprehensive_recommendations(bottleneck_types: list, 
                                             rpc_issues: list, validator_issues: list,
                                             comprehensive_score: float, max_qps: int) -> list:
        """åŸºäºç»¼åˆåˆ†æç”Ÿæˆä¼˜åŒ–å»ºè®® - é™æ€æ–¹æ³•"""
        recommendations = []
        
        if comprehensive_score < 0.2:
            recommendations.extend([
                f"ğŸ‰ ç³»ç»Ÿç»¼åˆæ€§èƒ½ä¼˜ç§€ï¼Œå½“å‰é…ç½®å¯ç¨³å®šæ”¯æŒ {max_qps} QPS",
                "ğŸ’¡ å¯è€ƒè™‘è¿›ä¸€æ­¥æå‡QPSç›®æ ‡æˆ–ä¼˜åŒ–æˆæœ¬æ•ˆç‡",
                "ğŸ“Š å»ºè®®å®šæœŸç›‘æ§ä»¥ç»´æŒå½“å‰æ€§èƒ½æ°´å¹³"
            ])
        else:
            # ç³»ç»Ÿèµ„æºä¼˜åŒ–å»ºè®®
            if 'CPU' in bottleneck_types:
                recommendations.append("ğŸ”§ CPUç“¶é¢ˆï¼šè€ƒè™‘å‡çº§CPUæˆ–ä¼˜åŒ–è®¡ç®—å¯†é›†å‹è¿›ç¨‹")
            if 'Memory' in bottleneck_types:
                recommendations.append("ğŸ”§ å†…å­˜ç“¶é¢ˆï¼šè€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
            if 'EBS' in bottleneck_types:
                recommendations.append("ğŸ”§ å­˜å‚¨ç“¶é¢ˆï¼šè€ƒè™‘å‡çº§EBSç±»å‹æˆ–ä¼˜åŒ–I/Oæ¨¡å¼")
            
            # RPCä¼˜åŒ–å»ºè®®
            if rpc_issues:
                recommendations.append(f"ğŸ”§ RPCé—®é¢˜ï¼šå‘ç°{len(rpc_issues)}ä¸ªRPCç›¸å…³é—®é¢˜ï¼Œéœ€è¦ä¼˜åŒ–RPCé…ç½®")
            
            # éªŒè¯å™¨ä¼˜åŒ–å»ºè®®
            if validator_issues:
                recommendations.append(f"ğŸ”§ éªŒè¯å™¨é—®é¢˜ï¼šå‘ç°{len(validator_issues)}ä¸ªéªŒè¯å™¨ç›¸å…³é—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥éªŒè¯å™¨é…ç½®")
        
        return recommendations

    @OperationLogger.log_operation("Generating comprehensive report", "ğŸ“„")
    def generate_comprehensive_report(self, df: pd.DataFrame, max_qps: int, 
                                    bottlenecks: Dict[str, Any], 
                                    log_analysis: Dict[str, Any], 
                                    rpc_deep_analysis: Dict[str, Any],
                                    benchmark_mode: str = "standard") -> str:
        """ç”ŸæˆåŸºäºç“¶é¢ˆåˆ†æçš„ç»¼åˆæŠ¥å‘Šï¼Œæ•´åˆæ‰€æœ‰åˆ†æç»“æœ"""

        # åŸºæœ¬æ€§èƒ½æŒ‡æ ‡ - ä½¿ç”¨å·¥å…·ç±»é¿å…é‡å¤ä»£ç 
        avg_cpu = DataProcessor.safe_calculate_mean(df, 'cpu_usage')
        avg_mem = DataProcessor.safe_calculate_mean(df, 'mem_usage')
        avg_rpc = DataProcessor.safe_calculate_mean(df, 'rpc_latency_ms')
        avg_rpc = df['rpc_latency_ms'].mean() if len(df) > 0 and 'rpc_latency_ms' in df.columns else 0

        # æ—¥å¿—åˆ†æç»“æœ
        rpc_analysis = log_analysis.get('rpc_analysis', {})
        bottleneck_analysis = log_analysis.get('bottleneck_analysis', {})
        summary_stats = log_analysis.get('summary_stats', {})
        correlation_analysis = log_analysis.get('correlation_analysis', {})

        # åŸºäºåŸºå‡†æµ‹è¯•æ¨¡å¼å’Œç“¶é¢ˆåˆ†æçš„æ€§èƒ½è¯„ä¼°
        performance_evaluation = self._evaluate_comprehensive_performance(
            benchmark_mode, max_qps, bottlenecks, avg_cpu, avg_mem, avg_rpc, 
            bottleneck_analysis, rpc_analysis
        )

        report = f"""# Solana QPS Comprehensive Performance Analysis Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Executive Summary
- **Maximum QPS Achieved**: {max_qps:,}
- **Performance Grade**: {performance_evaluation['performance_grade']}
- **Performance Level**: {performance_evaluation['performance_level']}
- **Benchmark Mode**: {benchmark_mode}
- **Test Duration**: {len(df)} monitoring points
- **Log Entries Analyzed**: {summary_stats.get('total_log_entries', 0):,}
- **Analysis Period**: {summary_stats.get('analysis_period_hours', 0):.1f} hours

## Performance Evaluation
- **Evaluation Basis**: {performance_evaluation['evaluation_basis']}
- **Evaluation Reason**: {performance_evaluation['evaluation_reason']}
- **Comprehensive Score**: {performance_evaluation.get('comprehensive_score', 0):.3f}

## System Performance Metrics
- **Average CPU Usage**: {avg_cpu:.1f}%
- **Average Memory Usage**: {avg_mem:.1f}%
- **Average RPC Latency**: {avg_rpc:.1f}ms
- **CPU Peak**: {DataProcessor.safe_calculate_max(df, 'cpu_usage'):.1f}%
- **Memory Peak**: {DataProcessor.safe_calculate_max(df, 'mem_usage'):.1f}%
- **RPC Latency Peak**: {DataProcessor.safe_calculate_max(df, 'rpc_latency_ms'):.1f}ms

## ğŸ” Enhanced Log Analysis Results

### RPC Performance (From Validator Logs)
- **Total RPC Requests**: {rpc_analysis.get('total_rpc_requests', 0):,}
- **RPC Success Rate**: {rpc_analysis.get('success_rate', 0):.2f}%
- **RPC Error Count**: {rpc_analysis.get('rpc_errors', 0):,}
- **RPC Busy Incidents**: {rpc_analysis.get('rpc_busy_incidents', 0):,}
- **Average Response Time**: {rpc_analysis.get('avg_response_time', 0):.1f}ms
- **P95 Response Time**: {rpc_analysis.get('p95_response_time', 0):.1f}ms
- **P99 Response Time**: {rpc_analysis.get('p99_response_time', 0):.1f}ms

### Critical Bottlenecks Detected
- **RPC Thread Saturation**: {bottleneck_analysis.get('rpc_thread_saturation', 0)} incidents
- **Compute Unit Limits**: {bottleneck_analysis.get('compute_unit_limits', 0)} incidents
- **Memory Pressure Events**: {bottleneck_analysis.get('memory_pressure', 0)} incidents
- **I/O Bottlenecks**: {bottleneck_analysis.get('io_bottlenecks', 0)} incidents
- **Network Issues**: {bottleneck_analysis.get('network_issues', 0)} incidents

### QPS vs Error Correlation
"""

        # æ·»åŠ ç›¸å…³æ€§åˆ†æ
        if correlation_analysis and correlation_analysis.get('qps_error_correlation'):
            correlation = correlation_analysis['qps_error_correlation']
            report += f"- **QPS-Error Correlation**: {correlation:.3f}\n"

            if correlation > 0.7:
                report += "  âš ï¸  Strong positive correlation detected - errors increase with QPS\n"
            elif correlation > 0.3:
                report += "  ğŸ“Š Moderate correlation detected between QPS and errors\n"
            else:
                report += "  âœ… Low correlation - system handles QPS increases well\n"

        # æœ€å¸¸ç”¨çš„RPCæ–¹æ³•
        if rpc_analysis.get('method_distribution'):
            report += "\n### Most Active RPC Methods\n"
            for method, count in rpc_analysis['method_distribution'].most_common(5):
                report += f"- **{method}**: {count:,} requests\n"

        # æ·»åŠ RPCæ·±åº¦åˆ†æç»“æœ
        if rpc_deep_analysis:
            rpc_deep_report = self.rpc_deep_analyzer.generate_rpc_deep_analysis_report(rpc_deep_analysis)
            report += rpc_deep_report

        # ä¼˜åŒ–å»ºè®®
        report += f"""
## ğŸ’¡ Comprehensive Optimization Recommendations

### Immediate Actions
"""

        # åŸºäºæ—¥å¿—åˆ†æå’ŒRPCæ·±åº¦åˆ†æçš„å…·ä½“å»ºè®®
        rpc_busy_count = bottleneck_analysis.get('rpc_thread_saturation', 0)
        memory_pressure = bottleneck_analysis.get('memory_pressure', 0)

        if rpc_busy_count > 20:
            report += "- ğŸ”§ **High Priority**: Increase RPC thread pool size (current threads are saturated)\n"
            report += "- ğŸ”§ **High Priority**: Consider implementing RPC request rate limiting\n"

        if memory_pressure > 0:
            report += "- ğŸ”¥ **Critical**: Increase system memory allocation or optimize memory usage\n"
            report += "- ğŸ”§ Check for memory leaks in validator process\n"

        # ä½¿ç”¨åŸºäºç»¼åˆåˆ†æçš„å»ºè®®
        for recommendation in performance_evaluation.get('recommendations', []):
            report += f"- {recommendation}\n"

        # åŸºäºRPCæ·±åº¦åˆ†æçš„å»ºè®®
        if rpc_deep_analysis:
            bottleneck_classification = rpc_deep_analysis.get('bottleneck_classification', {})
            recommendations = bottleneck_classification.get('recommendations', [])
            if recommendations:
                report += "\n### RPC Deep Analysis Recommendations\n"
                for rec in recommendations:
                    report += f"- ğŸ”§ {rec}\n"

        report += f"""
### Production Deployment
- **Recommended Production QPS**: {int(max_qps * 0.8):,} (80% of maximum tested)
- **Monitoring Thresholds**: 
  - Alert if RPC success rate < 98%
  - Alert if RPC busy incidents > 10/hour
  - Alert if memory pressure events detected
  - Alert if RPC latency P99 > 500ms sustained
- **Capacity Assessment**: {ComprehensiveAnalyzer._generate_comprehensive_capacity_assessment(performance_evaluation, max_qps)}

## Files Generated
- **Comprehensive Charts**: `{self.output_dir}/reports/comprehensive_analysis_charts.png`
- **Raw QPS Monitoring Data**: `{self.csv_file or 'N/A'}`
- **Validator Log Analysis**: Included in this report
- **RPC Deep Analysis**: Included in this report
- **Vegeta Test Reports**: `{self.output_dir}/reports/`

---
*Report generated by Comprehensive Solana QPS Analyzer v4.0*
"""

        # ä¿å­˜ç»¼åˆæŠ¥å‘Š - ä½¿ç”¨æ–‡ä»¶ç®¡ç†å™¨ï¼ŒåŒæ—¶åˆ›å»ºå½“å‰ç‰ˆæœ¬å’Œå¤‡ä»½
        report_file = self.file_manager.save_report_with_backup('comprehensive_analysis_report', report)

        print(f"âœ… Comprehensive report saved: {report_file}")
        return report

    def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„ç»¼åˆåˆ†æ"""
        print("ğŸš€ Starting Comprehensive Solana QPS Analysis")
        print("=" * 80)

        # 1. è¿è¡ŒQPSåˆ†æ
        print("\nğŸ“Š Phase 1: QPS Performance Analysis")
        qps_results = self.qps_analyzer.run_qps_analysis()
        df = qps_results['dataframe']
        max_qps = qps_results['max_qps']
        bottlenecks = qps_results['bottlenecks']

        # 1.1 å­—æ®µæ˜ å°„å™¨å·²ç§»é™¤ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åˆ—å
        # æ³¨æ„: å­—æ®µæ˜ å°„å™¨åŠŸèƒ½å·²è¢«ç§»é™¤ï¼Œç°åœ¨ç›´æ¥ä½¿ç”¨CSVæ–‡ä»¶ä¸­çš„åŸå§‹åˆ—å
        if FIELD_MAPPER_AVAILABLE:
            # è¿™ä¸ªåˆ†æ”¯ä¸ä¼šæ‰§è¡Œï¼Œå› ä¸ºFIELD_MAPPER_AVAILABLE=False
            try:
                # å­—æ®µæ˜ å°„å™¨ç›¸å…³ä»£ç å·²ç§»é™¤
                logger.info("âœ… å­—æ®µæ˜ å°„å™¨åº”ç”¨æˆåŠŸ")
                print("  âœ… å­—æ®µæ ‡å‡†åŒ–å®Œæˆ")
            except Exception as e:
                logger.warning(f"âš ï¸  å­—æ®µæ˜ å°„å™¨åº”ç”¨å¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹åˆ—å")
                print(f"  âš ï¸  å­—æ®µæ˜ å°„å™¨åº”ç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åˆ—å")
        else:
            logger.info("â„¹ï¸  ä½¿ç”¨åŸå§‹CSVåˆ—åè¿›è¡Œåˆ†æ")
            print("  â„¹ï¸  ä½¿ç”¨åŸå§‹CSVåˆ—åè¿›è¡Œåˆ†æ")

        # 2. è¿è¡ŒéªŒè¯å™¨æ—¥å¿—åˆ†æ
        print("\nğŸ“‹ Phase 2: Validator Log Analysis")
        log_analysis = self.log_analyzer.analyze_validator_logs_during_test(df)

        # 3. è¿è¡ŒRPCæ·±åº¦åˆ†æ
        print("\nğŸ” Phase 3: RPC Deep Analysis")
        rpc_deep_analysis = self.rpc_deep_analyzer.analyze_rpc_deep_performance(df)

        # 4. ç”Ÿæˆç»¼åˆå›¾è¡¨å’ŒæŠ¥å‘Š
        print("\nğŸ“ˆ Phase 4: Comprehensive Reporting")
        self.generate_ultimate_performance_charts(df, log_analysis, rpc_deep_analysis)
        
        # 4.1 ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨ï¼ˆåŒ…å«é˜ˆå€¼åˆ†æï¼‰
        print("\nğŸ¨ Phase 4.1: Performance Visualization with Threshold Analysis")
        try:
            from performance_visualizer import PerformanceVisualizer
            
            # ä¿å­˜ä¸´æ—¶CSVæ–‡ä»¶ä¾›performance_visualizerä½¿ç”¨ - ä½¿ç”¨è¿›ç¨‹IDå’Œéšæœºæ•°é¿å…å†²çª
            process_id = os.getpid()
            random_id = random.randint(1000, 9999)
            temp_csv_path = os.path.join(self.output_dir, f'temp_performance_data_{process_id}_{random_id}.csv')
            df.to_csv(temp_csv_path, index=False)
            
            # æŸ¥æ‰¾ç›‘æ§å¼€é”€æ–‡ä»¶
            overhead_files = glob.glob(f"{self.output_dir}/logs/monitoring_overhead_*.csv")
            overhead_file = max(overhead_files, key=os.path.getctime) if overhead_files else None
            
            # åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å™¨å¹¶ç”Ÿæˆå›¾è¡¨
            visualizer = PerformanceVisualizer(temp_csv_path, overhead_file)
            chart_results = visualizer.generate_all_charts()
            
            if isinstance(chart_results, tuple) and len(chart_results) == 2:
                chart_files, threshold_analysis = chart_results
                print(f"âœ… ç”Ÿæˆäº† {len(chart_files)} å¼ æ€§èƒ½å›¾è¡¨ï¼ˆåŒ…å«é˜ˆå€¼åˆ†æï¼‰")
                
                # å°†é˜ˆå€¼åˆ†æç»“æœæ·»åŠ åˆ°ç»¼åˆç»“æœä¸­
                if threshold_analysis:
                    print("ğŸ“Š é˜ˆå€¼åˆ†æå·²å®Œæˆå¹¶é›†æˆåˆ°æŠ¥å‘Šä¸­")
            else:
                chart_files = chart_results if isinstance(chart_results, list) else []
                print(f"âœ… ç”Ÿæˆäº† {len(chart_files)} å¼ æ€§èƒ½å›¾è¡¨")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_csv_path):
                os.remove(temp_csv_path)
                
        except ImportError as e:
            print(f"âš ï¸ æ€§èƒ½å¯è§†åŒ–å™¨å¯¼å…¥å¤±è´¥: {e}")
        except Exception as e:
            print(f"âš ï¸ æ€§èƒ½å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
        
        comprehensive_report = self.generate_comprehensive_report(
            df, max_qps, bottlenecks, log_analysis, rpc_deep_analysis, self.benchmark_mode
        )

        # 5. æ˜¾ç¤ºæ‰€æœ‰åˆ†ææŠ¥å‘Š
        if log_analysis:
            log_report = self.log_analyzer.generate_log_analysis_report(log_analysis)
            print(log_report)

        if rpc_deep_analysis:
            rpc_report = self.rpc_deep_analyzer.generate_rpc_deep_analysis_report(rpc_deep_analysis)
            print(rpc_report)

        # è¿”å›å®Œæ•´çš„åˆ†æç»“æœ
        comprehensive_results = {
            'qps_analysis': qps_results,
            'log_analysis': log_analysis,
            'rpc_deep_analysis': rpc_deep_analysis,
            'comprehensive_report': comprehensive_report,
            'dataframe': df,
            'max_qps': max_qps,
            'bottlenecks': bottlenecks
        }

        print("\nğŸ‰ Comprehensive Analysis Completed Successfully!")
        print("Generated files:")
        print(f"  ğŸ“Š Charts: {self.output_dir}/reports/comprehensive_analysis_charts.png")
        print(f"  ğŸ“„ Report: {self.output_dir}/reports/comprehensive_analysis_report.md")
        print(f"  ğŸ’¾ Backups: Files with timestamp {self.session_timestamp} created for version history")
        print(f"  ğŸ“‹ Individual Analysis Reports: Check {self.output_dir}/reports/ for detailed reports")

        return comprehensive_results


def main():
    """ä¸»æ‰§è¡Œå‡½æ•° - æ”¯æŒç“¶é¢ˆæ¨¡å¼å’Œæ—¶é—´çª—å£åˆ†æ"""
    parser = argparse.ArgumentParser(description='ç»¼åˆåˆ†æå™¨ - æ”¯æŒç“¶é¢ˆæ¨¡å¼')
    parser.add_argument('csv_file', nargs='?', help='CSVæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--benchmark-mode', default='standard', choices=['quick', 'standard', 'intensive'], 
                       help='åŸºå‡†æµ‹è¯•æ¨¡å¼ (é»˜è®¤: standard)')
    parser.add_argument('--bottleneck-mode', action='store_true', help='å¯ç”¨ç“¶é¢ˆåˆ†ææ¨¡å¼')
    parser.add_argument('--bottleneck-info', help='ç“¶é¢ˆä¿¡æ¯JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--time-window', action='store_true', help='å¯ç”¨æ—¶é—´çª—å£åˆ†æ')
    parser.add_argument('--start-time', help='æ—¶é—´çª—å£å¼€å§‹æ—¶é—´')
    parser.add_argument('--end-time', help='æ—¶é—´çª—å£ç»“æŸæ—¶é—´')
    parser.add_argument('--bottleneck-time', help='ç“¶é¢ˆæ£€æµ‹æ—¶é—´')
    parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–ç“¶é¢ˆåˆ†ææ¨¡å¼
        bottleneck_mode = None
        if args.bottleneck_mode or args.bottleneck_info:
            bottleneck_info = {}
            
            if args.bottleneck_info and os.path.exists(args.bottleneck_info):
                try:
                    with open(args.bottleneck_info, 'r') as f:
                        bottleneck_info = json.load(f)
                    logger.info(f"ğŸ“Š åŠ è½½ç“¶é¢ˆä¿¡æ¯: {args.bottleneck_info}")
                except Exception as e:
                    logger.error(f"âŒ ç“¶é¢ˆä¿¡æ¯æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            
            bottleneck_mode = BottleneckAnalysisMode(bottleneck_info)
        
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = ComprehensiveAnalyzer(args.output_dir, args.benchmark_mode, bottleneck_mode)
        
        # ç¡®å®šCSVæ–‡ä»¶
        csv_file = args.csv_file or analyzer.csv_file
        if not csv_file or not os.path.exists(csv_file):
            logger.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„CSVæ•°æ®æ–‡ä»¶")
            return 1
        
        logger.info(f"ğŸ“ˆ å¼€å§‹ç»¼åˆåˆ†æ: {csv_file}")
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(csv_file)
        logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æ¡è®°å½•")
        
        # æ—¶é—´çª—å£è¿‡æ»¤
        if args.time_window and args.start_time and args.end_time:
            df = ComprehensiveAnalyzer.filter_data_by_time_window(df, args.start_time, args.end_time)
            logger.info(f"ğŸ• æ—¶é—´çª—å£åˆ†æ: {args.start_time} åˆ° {args.end_time}")
        
        # æ‰§è¡Œåˆ†æ
        if bottleneck_mode and bottleneck_mode.enabled:
            logger.info("ğŸš¨ æ‰§è¡Œç“¶é¢ˆæ¨¡å¼åˆ†æ")
            
            # ç“¶é¢ˆç›¸å…³æ€§åˆ†æ
            bottleneck_analysis = analyzer.analyze_bottleneck_correlation(df)
            
            # ç”Ÿæˆç“¶é¢ˆåˆ†æå›¾è¡¨
            bottleneck_chart = analyzer.generate_bottleneck_analysis_chart(df, bottleneck_analysis)
            
            # ä¿å­˜ç“¶é¢ˆåˆ†æç»“æœ
            bottleneck_result_file = os.path.join(analyzer.output_dir, 'reports', 'bottleneck_analysis_result.json')
            os.makedirs(os.path.dirname(bottleneck_result_file), exist_ok=True)
            with open(bottleneck_result_file, 'w') as f:
                json.dump(bottleneck_analysis, f, indent=2, default=str)
            logger.info(f"ğŸ“Š ç“¶é¢ˆåˆ†æç»“æœå·²ä¿å­˜: {bottleneck_result_file}")
        
        # æ‰§è¡Œæ ‡å‡†ç»¼åˆåˆ†æ
        result = analyzer.run_comprehensive_analysis()
        
        if result:
            logger.info("âœ… ç»¼åˆåˆ†æå®Œæˆ")
            return 0
        else:
            logger.error("âŒ ç»¼åˆåˆ†æå¤±è´¥")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ ç»¼åˆåˆ†ææ‰§è¡Œå¤±è´¥: {e}")
        return 1


# æ¼”ç¤ºåŠŸèƒ½ï¼ˆä»…åœ¨ç›´æ¥è¿è¡Œæ—¶ä½¿ç”¨ï¼‰
def demo_comprehensive_analysis():
    """æ¼”ç¤ºç»¼åˆåˆ†æåŠŸèƒ½"""
    try:
        analyzer = ComprehensiveAnalyzer(benchmark_mode="standard")
        results = analyzer.run_comprehensive_analysis()

        print("\nğŸ¯ Analysis Summary:")
        print(f"  Maximum QPS: {results['max_qps']:,}")
        print(f"  Data Points: {len(results['dataframe'])}")
        print(f"  Bottlenecks: {len(results['bottlenecks'])}")
        
        return results

    except Exception as e:
        logger.error(f"âŒ Comprehensive analysis failed: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()
