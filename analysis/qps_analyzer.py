#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
QPSåˆ†æå™¨ - ä»comprehensive_analysis.pyæ‹†åˆ†å‡ºæ¥çš„ç‹¬ç«‹æ¨¡å— + ç“¶é¢ˆæ¨¡å¼æ”¯æŒ
ä¸“é—¨è´Ÿè´£QPSæ€§èƒ½åˆ†æï¼ŒåŒ…æ‹¬æ€§èƒ½æŒ‡æ ‡åˆ†æã€ç“¶é¢ˆè¯†åˆ«ã€å›¾è¡¨ç”Ÿæˆç­‰
æ”¯æŒæ€§èƒ½æ‚¬å´–åˆ†æå’Œç“¶é¢ˆæ£€æµ‹æ¨¡å¼
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import glob
import os
import sys
import json
import argparse
import re
from datetime import datetime
from typing import Dict, Any, Optional, Tuple
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
sys.path.insert(0, project_root)

from visualization.chart_style_config import UnifiedChartStyle
from utils.unified_logger import get_logger

# ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç®¡ç†å™¨
logger = get_logger(__name__)
logger.info("âœ… ç»Ÿä¸€æ—¥å¿—ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

class NodeQPSAnalyzer:
    """åŒºå—é“¾èŠ‚ç‚¹ QPSæ€§èƒ½åˆ†æå™¨ + ç“¶é¢ˆæ¨¡å¼æ”¯æŒ - æ”¯æŒå¤šç§åŒºå—é“¾"""

    def __init__(self, output_dir: Optional[str] = None, benchmark_mode: str = "standard", bottleneck_mode: bool = False):
        """
        åˆå§‹åŒ–QPSåˆ†æå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œå°†ä»ç¯å¢ƒå˜é‡è·å–ï¼‰
            benchmark_mode: åŸºå‡†æµ‹è¯•æ¨¡å¼ (quick/standard/intensive)
            bottleneck_mode: æ˜¯å¦å¯ç”¨ç“¶é¢ˆåˆ†ææ¨¡å¼
        """
        # åº”ç”¨ç»Ÿä¸€æ ·å¼
        UnifiedChartStyle.setup_matplotlib()
        
        if output_dir is None:
            output_dir = os.environ.get('DATA_DIR', os.path.join(os.path.expanduser('~'), 'blockchain-node-benchmark-result'))
        
        self.output_dir = output_dir
        self.benchmark_mode = benchmark_mode
        self.bottleneck_mode = bottleneck_mode
        self.reports_dir = os.getenv('REPORTS_DIR', os.path.join(output_dir, 'current', 'reports'))
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # ç“¶é¢ˆæ£€æµ‹é˜ˆå€¼é…ç½®
        self.cpu_threshold = int(os.getenv('BOTTLENECK_CPU_THRESHOLD', 85))
        self.memory_threshold = int(os.getenv('BOTTLENECK_MEMORY_THRESHOLD', 90))
        self.rpc_threshold = int(os.getenv('MAX_LATENCY_THRESHOLD', 1000))
        
        # åˆå§‹åŒ–CSVæ–‡ä»¶è·¯å¾„ - ä¿®å¤ç¼ºå¤±çš„å±æ€§
        self.csv_file = self.get_latest_csv()

        # Using English labels system directly
        
        # åº”ç”¨ç»Ÿä¸€æ ·å¼é…ç½®
        UnifiedChartStyle.setup_matplotlib()
        
        logger.info(f"ğŸ” QPSåˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {output_dir}, åŸºå‡†æµ‹è¯•æ¨¡å¼: {benchmark_mode}")
        if bottleneck_mode:
            logger.info("ğŸš¨ ç“¶é¢ˆåˆ†ææ¨¡å¼å·²å¯ç”¨")

    def _get_dynamic_key_metrics(self, df: pd.DataFrame) -> list:
        """åŠ¨æ€è·å–å…³é”®æŒ‡æ ‡å­—æ®µï¼Œæ›¿ä»£ç¡¬ç¼–ç è®¾å¤‡å - å®Œæ•´ç‰ˆæœ¬"""
        base_metrics = ['cpu_usage', 'mem_usage']
        
        # åŠ¨æ€æŸ¥æ‰¾EBSåˆ©ç”¨ç‡å­—æ®µï¼ˆä¼˜å…ˆDATAè®¾å¤‡ï¼Œç„¶åACCOUNTSè®¾å¤‡ï¼‰
        ebs_util_field = None
        # é¦–å…ˆæŸ¥æ‰¾DATAè®¾å¤‡å­—æ®µï¼ˆå¿…é¡»å­˜åœ¨ï¼‰
        for col in df.columns:
            if col.startswith('data_') and col.endswith('_util'):
                ebs_util_field = col
                break
        
        # å¦‚æœæ²¡æœ‰DATAè®¾å¤‡å­—æ®µï¼ŒæŸ¥æ‰¾ACCOUNTSè®¾å¤‡å­—æ®µï¼ˆå¯é€‰ï¼‰
        if not ebs_util_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_util'):
                    ebs_util_field = col
                    break
        
        # åŠ¨æ€æŸ¥æ‰¾EBSå»¶è¿Ÿå­—æ®µï¼ˆä¼˜å…ˆDATAè®¾å¤‡çš„r_awaitï¼‰
        ebs_latency_field = None
        # é¦–å…ˆæŸ¥æ‰¾DATAè®¾å¤‡çš„r_awaitå­—æ®µ
        for col in df.columns:
            if col.startswith('data_') and col.endswith('_r_await'):
                ebs_latency_field = col
                break
        
        # å¦‚æœæ²¡æœ‰DATAè®¾å¤‡çš„r_awaitï¼ŒæŸ¥æ‰¾DATAè®¾å¤‡çš„avg_await
        if not ebs_latency_field:
            for col in df.columns:
                if col.startswith('data_') and col.endswith('_avg_await'):
                    ebs_latency_field = col
                    break
        
        # å¦‚æœDATAè®¾å¤‡éƒ½æ²¡æœ‰ï¼ŒæŸ¥æ‰¾ACCOUNTSè®¾å¤‡çš„å»¶è¿Ÿå­—æ®µï¼ˆå¯é€‰ï¼‰
        if not ebs_latency_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_r_await'):
                    ebs_latency_field = col
                    break
        
        if not ebs_latency_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_avg_await'):
                    ebs_latency_field = col
                    break
        
        # åŠ¨æ€æŸ¥æ‰¾å…¶ä»–é‡è¦EBSæŒ‡æ ‡ï¼ˆä¼˜å…ˆDATAè®¾å¤‡ï¼‰
        ebs_iops_field = None
        # é¦–å…ˆæŸ¥æ‰¾DATAè®¾å¤‡å­—æ®µ
        for col in df.columns:
            if col.startswith('data_') and col.endswith('_total_iops'):
                ebs_iops_field = col
                break
        # å¦‚æœæ²¡æœ‰DATAè®¾å¤‡å­—æ®µï¼ŒæŸ¥æ‰¾ACCOUNTSè®¾å¤‡å­—æ®µï¼ˆå¯é€‰ï¼‰
        if not ebs_iops_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_total_iops'):
                    ebs_iops_field = col
                    break
        
        ebs_throughput_field = None
        # é¦–å…ˆæŸ¥æ‰¾DATAè®¾å¤‡å­—æ®µ
        for col in df.columns:
            if col.startswith('data_') and col.endswith('_throughput_mibs'):
                ebs_throughput_field = col
                break
        # å¦‚æœæ²¡æœ‰DATAè®¾å¤‡å­—æ®µï¼ŒæŸ¥æ‰¾ACCOUNTSè®¾å¤‡å­—æ®µï¼ˆå¯é€‰ï¼‰
        if not ebs_throughput_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_throughput_mibs'):
                    ebs_throughput_field = col
                    break
        
        ebs_queue_field = None
        # é¦–å…ˆæŸ¥æ‰¾DATAè®¾å¤‡å­—æ®µ
        for col in df.columns:
            if col.startswith('data_') and col.endswith('_aqu_sz'):
                ebs_queue_field = col
                break
        # å¦‚æœæ²¡æœ‰DATAè®¾å¤‡å­—æ®µï¼ŒæŸ¥æ‰¾ACCOUNTSè®¾å¤‡å­—æ®µï¼ˆå¯é€‰ï¼‰
        if not ebs_queue_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_aqu_sz'):
                    ebs_queue_field = col
                    break
        
        # æ·»åŠ æ‰¾åˆ°çš„å­—æ®µ
        if ebs_util_field:
            base_metrics.append(ebs_util_field)
            logger.info(f"âœ… åŠ¨æ€å‘ç°EBSåˆ©ç”¨ç‡å­—æ®µ: {ebs_util_field}")
        
        if ebs_latency_field:
            base_metrics.append(ebs_latency_field)
            logger.info(f"âœ… åŠ¨æ€å‘ç°EBSå»¶è¿Ÿå­—æ®µ: {ebs_latency_field}")
        
        if ebs_iops_field:
            base_metrics.append(ebs_iops_field)
            logger.info(f"âœ… åŠ¨æ€å‘ç°EBS IOPSå­—æ®µ: {ebs_iops_field}")
        
        if ebs_throughput_field:
            base_metrics.append(ebs_throughput_field)
            logger.info(f"âœ… åŠ¨æ€å‘ç°EBSååé‡å­—æ®µ: {ebs_throughput_field}")
        
        if ebs_queue_field:
            base_metrics.append(ebs_queue_field)
            logger.info(f"âœ… åŠ¨æ€å‘ç°EBSé˜Ÿåˆ—æ·±åº¦å­—æ®µ: {ebs_queue_field}")
        
        if not any([ebs_util_field, ebs_latency_field, ebs_iops_field]):
            logger.warning("âš ï¸ æœªå‘ç°EBSç›¸å…³å­—æ®µï¼Œå¯èƒ½å½±å“ç“¶é¢ˆåˆ†æå‡†ç¡®æ€§")
        
        logger.info(f"ğŸ“Š åŠ¨æ€æŒ‡æ ‡å­—æ®µæ€»æ•°: {len(base_metrics)}")
        return base_metrics
    


    def analyze_performance_cliff(self, df: pd.DataFrame, max_qps: int, bottleneck_qps: int) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ‚¬å´– - è¯†åˆ«æ€§èƒ½æ€¥å‰§ä¸‹é™çš„ç‚¹"""
        try:
            cliff_analysis = {
                'max_qps': max_qps,
                'bottleneck_qps': bottleneck_qps,
                'performance_drop_percent': 0.0,  # ä½¿ç”¨floatç±»å‹ä¿æŒä¸€è‡´æ€§
                'cliff_detected': False,
                'cliff_factors': [],
                'recommendations': []
            }
            
            if max_qps > 0 and bottleneck_qps > 0:
                # è®¡ç®—æ€§èƒ½ä¸‹é™ç™¾åˆ†æ¯”
                drop_percent = ((bottleneck_qps - max_qps) / max_qps) * 100
                cliff_analysis['performance_drop_percent'] = drop_percent
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºæ€§èƒ½æ‚¬å´–ï¼ˆä¸‹é™è¶…è¿‡20%ï¼‰
                if abs(drop_percent) > 20:
                    cliff_analysis['cliff_detected'] = True
                    
                    # åˆ†ææ‚¬å´–å› å­
                    cliff_factors = self._identify_cliff_factors(df, max_qps, bottleneck_qps)
                    cliff_analysis['cliff_factors'] = cliff_factors
                    
                    # ç”Ÿæˆå»ºè®®
                    recommendations = self._generate_cliff_recommendations(cliff_factors, drop_percent)
                    cliff_analysis['recommendations'] = recommendations
                    
                    logger.info(f"ğŸš¨ æ£€æµ‹åˆ°æ€§èƒ½æ‚¬å´–: {drop_percent:.1f}% æ€§èƒ½ä¸‹é™")
                else:
                    logger.info(f"ğŸ“Š æ€§èƒ½å˜åŒ–: {drop_percent:.1f}% (æœªè¾¾åˆ°æ‚¬å´–é˜ˆå€¼)")
            
            return cliff_analysis
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æ‚¬å´–åˆ†æå¤±è´¥: {e}")
            return {}

    def _identify_cliff_factors(self, df: pd.DataFrame, max_qps: int, bottleneck_qps: int) -> list:
        """è¯†åˆ«å¯¼è‡´æ€§èƒ½æ‚¬å´–çš„å› å­"""
        cliff_factors = []
        
        try:
            # å¯»æ‰¾QPSåˆ—
            qps_column = None
            for col in ['current_qps', 'qps', 'requests_per_second']:
                if col in df.columns:
                    qps_column = col
                    break
            
            if not qps_column:
                return cliff_factors
            
            # æ‰¾åˆ°æœ€å¤§QPSå’Œç“¶é¢ˆQPSå¯¹åº”çš„æ•°æ®ç‚¹
            max_qps_data = df[df[qps_column] <= max_qps].tail(1)
            bottleneck_qps_data = df[df[qps_column] >= bottleneck_qps].head(1)
            
            if len(max_qps_data) == 0 or len(bottleneck_qps_data) == 0:
                return cliff_factors
            
            # æ¯”è¾ƒå…³é”®æŒ‡æ ‡çš„å˜åŒ– - ä½¿ç”¨åŠ¨æ€å­—æ®µæŸ¥æ‰¾æ›¿ä»£ç¡¬ç¼–ç 
            key_metrics = self._get_dynamic_key_metrics(df)
            
            for metric in key_metrics:
                if metric in df.columns:
                    try:
                        max_value = max_qps_data[metric].iloc[0]
                        bottleneck_value = bottleneck_qps_data[metric].iloc[0]
                        
                        if pd.notna(max_value) and pd.notna(bottleneck_value) and max_value != 0:
                            change_percent = ((bottleneck_value - max_value) / max_value) * 100
                            
                            # å¦‚æœå˜åŒ–è¶…è¿‡10%ï¼Œè®¤ä¸ºæ˜¯æ‚¬å´–å› å­
                            if abs(change_percent) > 10:
                                cliff_factors.append({
                                    'metric': metric,
                                    'max_qps_value': float(max_value),
                                    'bottleneck_value': float(bottleneck_value),
                                    'change_percent': float(change_percent),
                                    'impact': 'high' if abs(change_percent) > 50 else 'medium'
                                })
                    except Exception as e:
                        logger.warning(f"âš ï¸ åˆ†æ{metric}æ‚¬å´–å› å­å¤±è´¥: {e}")
            
            # æŒ‰å½±å“ç¨‹åº¦æ’åº
            cliff_factors.sort(key=lambda x: abs(x['change_percent']), reverse=True)
            
        except Exception as e:
            logger.error(f"âŒ æ‚¬å´–å› å­è¯†åˆ«å¤±è´¥: {e}")
        
        return cliff_factors

    def _generate_cliff_recommendations(self, cliff_factors: list, drop_percent: float) -> list:
        """åŸºäºæ‚¬å´–å› å­ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        try:
            # åŸºäºæ€§èƒ½ä¸‹é™ç¨‹åº¦çš„é€šç”¨å»ºè®®
            if abs(drop_percent) > 50:
                recommendations.append("ä¸¥é‡æ€§èƒ½æ‚¬å´–ï¼šå»ºè®®ç«‹å³åœæ­¢æµ‹è¯•å¹¶æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
                recommendations.append("è€ƒè™‘é™ä½æµ‹è¯•å¼ºåº¦æˆ–ä¼˜åŒ–ç³»ç»Ÿé…ç½®")
            elif abs(drop_percent) > 30:
                recommendations.append("æ˜¾è‘—æ€§èƒ½ä¸‹é™ï¼šå»ºè®®åˆ†æç³»ç»Ÿç“¶é¢ˆå¹¶è¿›è¡Œä¼˜åŒ–")
            
            # åŸºäºå…·ä½“æ‚¬å´–å› å­çš„å»ºè®®
            for factor in cliff_factors[:3]:  # åªå¤„ç†å‰3ä¸ªæœ€é‡è¦çš„å› å­
                metric = factor['metric']
                change = factor['change_percent']
                
                if 'cpu' in metric.lower():
                    if change > 0:
                        recommendations.append(f"CPUä½¿ç”¨ç‡æ€¥å‰§ä¸Šå‡{change:.1f}%ï¼šè€ƒè™‘å‡çº§CPUæˆ–ä¼˜åŒ–åº”ç”¨")
                    else:
                        recommendations.append(f"CPUä½¿ç”¨ç‡å¼‚å¸¸ä¸‹é™{abs(change):.1f}%ï¼šæ£€æŸ¥CPUè°ƒåº¦é—®é¢˜")
                
                elif 'mem' in metric.lower():
                    if change > 0:
                        recommendations.append(f"å†…å­˜ä½¿ç”¨ç‡æ€¥å‰§ä¸Šå‡{change:.1f}%ï¼šè€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
                    else:
                        recommendations.append(f"å†…å­˜ä½¿ç”¨ç‡å¼‚å¸¸ä¸‹é™{abs(change):.1f}%ï¼šæ£€æŸ¥å†…å­˜ç®¡ç†é—®é¢˜")
                
                elif 'util' in metric.lower():
                    if change > 0:
                        recommendations.append(f"ç£ç›˜åˆ©ç”¨ç‡æ€¥å‰§ä¸Šå‡{change:.1f}%ï¼šè€ƒè™‘å‡çº§å­˜å‚¨æˆ–ä¼˜åŒ–I/O")
                
                elif 'await' in metric.lower():
                    if change > 0:
                        recommendations.append(f"ç£ç›˜å»¶è¿Ÿæ€¥å‰§ä¸Šå‡{change:.1f}%ï¼šæ£€æŸ¥å­˜å‚¨æ€§èƒ½ç“¶é¢ˆ")
            
            # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„æ‚¬å´–å› å­ï¼Œæä¾›é€šç”¨å»ºè®®
            if not cliff_factors:
                recommendations.append("æœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½æ‚¬å´–å› å­ï¼Œå»ºè®®è¿›è¡Œå…¨é¢çš„ç³»ç»Ÿæ€§èƒ½åˆ†æ")
                recommendations.append("æ£€æŸ¥ç½‘ç»œã€åº”ç”¨é€»è¾‘å’Œç³»ç»Ÿé…ç½®")
        
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ‚¬å´–å»ºè®®å¤±è´¥: {e}")
        
        return recommendations

    def generate_cliff_analysis_chart(self, df: pd.DataFrame, cliff_analysis: Dict[str, Any]) -> Optional[plt.Figure]:
        """ç”Ÿæˆæ€§èƒ½æ‚¬å´–åˆ†æå›¾è¡¨"""
        try:
            if not cliff_analysis or not cliff_analysis.get('cliff_detected'):
                return None
            
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            # Using English title directly
            fig.suptitle('ğŸ“‰ Performance Cliff Analysis', fontsize=UnifiedChartStyle.FONT_CONFIG["title_size"], fontweight='bold', color=UnifiedChartStyle.COLORS["critical"])
            
            # 1. QPSæ€§èƒ½æ›²çº¿
            qps_column = None
            for col in ['current_qps', 'qps', 'requests_per_second']:
                if col in df.columns:
                    qps_column = col
                    break
            
            if qps_column and len(df) > 0:
                axes[0, 0].plot(df.index, df[qps_column], 'b-', alpha=0.7, linewidth=2)
                
                # æ ‡è®°æœ€å¤§QPSå’Œç“¶é¢ˆQPS
                max_qps = cliff_analysis['max_qps']
                bottleneck_qps = cliff_analysis['bottleneck_qps']
                
                axes[0, 0].axhline(y=max_qps, color=UnifiedChartStyle.COLORS["success"], linestyle='--', linewidth=2,
                                 label=f'Max QPS: {max_qps}')
                axes[0, 0].axhline(y=bottleneck_qps, color=UnifiedChartStyle.COLORS["critical"], linestyle='--', linewidth=2,
                                 label=f'Bottleneck QPS: {bottleneck_qps}')
                
                # å¡«å……æ‚¬å´–åŒºåŸŸ
                axes[0, 0].fill_between(df.index, max_qps, bottleneck_qps, 
                                      alpha=0.3, color=UnifiedChartStyle.COLORS["critical"], label='Performance Cliff')
                
                axes[0, 0].set_title('QPS Performance Cliff')
                axes[0, 0].set_xlabel('Time')
                axes[0, 0].set_ylabel('QPS')
                axes[0, 0].legend()
                axes[0, 0].grid(True, alpha=0.3)
            
            # 2. æ‚¬å´–å› å­å½±å“
            cliff_factors = cliff_analysis.get('cliff_factors', [])
            if cliff_factors:
                factor_names = [f['metric'] for f in cliff_factors[:5]]
                factor_changes = [abs(f['change_percent']) for f in cliff_factors[:5]]
                
                colors = ['red' if abs(f['change_percent']) > 50 else 'orange' 
                         for f in cliff_factors[:5]]
                
                axes[0, 1].barh(factor_names, factor_changes, color=colors, alpha=0.7)
                axes[0, 1].set_title('Cliff Factor Impact (%)')
                axes[0, 1].set_xlabel('Change Percentage')
                axes[0, 1].grid(True, alpha=0.3)
            
            # 3. æ€§èƒ½ä¸‹é™å¯è§†åŒ–
            drop_percent = cliff_analysis.get('performance_drop_percent', 0)
            categories = ['Before Cliff', 'After Cliff']
            values = [100, 100 + drop_percent]  # ç›¸å¯¹æ€§èƒ½
            colors = ['green', 'red']
            
            bars = axes[1, 0].bar(categories, values, color=colors, alpha=0.7)
            axes[1, 0].set_title(f'Performance Drop: {abs(drop_percent):.1f}%')
            axes[1, 0].set_ylabel('Relative Performance (%)')
            axes[1, 0].axhline(y=100, color='black', linestyle='-', alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                               f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')
            
            # 4. å»ºè®®æ‘˜è¦
            recommendations = cliff_analysis.get('recommendations', [])
            if recommendations:
                axes[1, 1].text(0.05, 0.95, 'Optimization Recommendations:', 
                               transform=axes[1, 1].transAxes, fontsize=UnifiedChartStyle.FONT_CONFIG["subtitle_size"], fontweight='bold',
                               verticalalignment='top')
                
                for i, rec in enumerate(recommendations[:5]):
                    axes[1, 1].text(0.05, 0.85 - i*0.15, f"â€¢ {rec}", 
                                   transform=axes[1, 1].transAxes, fontsize=UnifiedChartStyle.FONT_CONFIG["label_size"],
                                   verticalalignment='top', wrap=True)
            
            axes[1, 1].set_xlim(0, 1)
            axes[1, 1].set_ylim(0, 1)
            axes[1, 1].axis('off')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_path = os.path.join(self.reports_dir, 'performance_cliff_analysis.png')
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            logger.info(f"ğŸ“Š æ€§èƒ½æ‚¬å´–åˆ†æå›¾è¡¨å·²ä¿å­˜: {chart_path}")
            
            return fig
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æ‚¬å´–å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def get_latest_csv(self) -> Optional[str]:
        """CSVæ–‡ä»¶æŸ¥æ‰¾é€»è¾‘ï¼Œæ”¯æŒå¤šç§è·¯å¾„æ¨¡å¼"""
        # ä½¿ç”¨ç¯å¢ƒå˜é‡LOGS_DIRï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾
        logs_dir = os.getenv('LOGS_DIR', os.path.join(self.output_dir, 'current', 'logs'))
        csv_patterns = [
            f"{logs_dir}/performance_latest.csv",
            f"{logs_dir}/performance_*.csv",
            f"{self.output_dir}/current/logs/performance_latest.csv",
            f"{self.output_dir}/current/logs/performance_*.csv",
            f"{self.output_dir}/archives/*/logs/performance_latest.csv",
            f"{self.output_dir}/archives/*/logs/performance_*.csv"
        ]
        
        for pattern in csv_patterns:
            csv_files = glob.glob(pattern)
            if csv_files:
                return max(csv_files, key=os.path.getctime)
        return None

    def load_and_clean_data(self) -> pd.DataFrame:
        """åŠ è½½å’Œæ¸…ç†ç›‘æ§æ•°æ®ï¼Œæ”¹è¿›é”™è¯¯å¤„ç†"""
        try:
            if not self.csv_file:
                print("âš ï¸  No CSV monitoring file found, proceeding with log analysis only")
                return pd.DataFrame()

            print(f"ğŸ“Š Loading QPS monitoring data from: {os.path.basename(self.csv_file)}")
            
            # ç›´æ¥ä½¿ç”¨pandasè¯»å–CSV - å­—æ®µæ˜ å°„å™¨å·²ç§»é™¤
            df = pd.read_csv(self.csv_file)

            print(f"ğŸ“‹ Raw data shape: {df.shape}")

            # æ£€æŸ¥æ˜¯å¦æœ‰QPSç›¸å…³æ•°æ®
            qps_columns = ['current_qps', 'qps', 'target_qps']
            qps_column = None
            for col in qps_columns:
                if col in df.columns:
                    qps_column = col
                    break
            
            if qps_column is None:
                print("âš ï¸  No QPS data found in CSV, this appears to be system monitoring data only")
                print("ğŸ“Š Available columns:", ', '.join(df.columns[:10]))
                
                # ä¸ºç³»ç»Ÿç›‘æ§æ•°æ®æ·»åŠ è™šæ‹ŸQPSåˆ—ï¼Œé¿å…åç»­KeyError
                df['current_qps'] = 0  # ä½¿ç”¨æ•°å€¼0è€Œä¸æ˜¯å­—ç¬¦ä¸²'0'
                df['rpc_latency_ms'] = 0.0  # æ·»åŠ è™šæ‹ŸRPCå»¶è¿Ÿå­—æ®µ
                df['elapsed_time'] = 0.0    # æ·»åŠ è™šæ‹Ÿæ—¶é—´å­—æ®µ
                df['remaining_time'] = 0.0  # æ·»åŠ è™šæ‹Ÿå‰©ä½™æ—¶é—´å­—æ®µ
                df['qps_data_available'] = False
                df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
                return df

            # å¤„ç†current_qpsåˆ—
            df['current_qps'] = df[qps_column].astype(str)
            df['qps_data_available'] = True
            numeric_mask = pd.to_numeric(df['current_qps'], errors='coerce').notna()
            numeric_df = df[numeric_mask].copy()

            if len(numeric_df) == 0:
                print("âš ï¸  No numeric QPS data found")
                df['qps_data_available'] = False
                return df

            # æ•°æ®ç±»å‹è½¬æ¢
            numeric_df['current_qps'] = pd.to_numeric(numeric_df['current_qps'])
            numeric_df['timestamp'] = pd.to_datetime(numeric_df['timestamp'], errors='coerce')

            # è¿‡æ»¤ QPS=0 çš„ç›‘æ§æ•°æ®ï¼Œåªä¿ç•™å®é™…æµ‹è¯•æ•°æ®
            if 'current_qps' in numeric_df.columns:
                original_count = len(numeric_df)
                numeric_df = numeric_df[numeric_df['current_qps'] > 0].copy()
                filtered_count = len(numeric_df)
                print(f"ğŸ“Š Filtered QPS data: {filtered_count}/{original_count} active test points (QPS > 0)")

            # æ¸…ç†æ•°å€¼åˆ— - ä½¿ç”¨æ˜ å°„åçš„æ ‡å‡†å­—æ®µå
            numeric_cols = ['cpu_usage', 'mem_usage', 'rpc_latency_ms', 'elapsed_time', 'remaining_time']
            for col in numeric_cols:
                if col in numeric_df.columns:
                    numeric_df[col] = pd.to_numeric(numeric_df[col], errors='coerce')

            print(f"ğŸ“Š Processed {len(numeric_df)} QPS monitoring data points")
            return numeric_df
            
        except Exception as e:
            logger.error(f"âŒ Data loading and cleaning failed: {e}")
            return pd.DataFrame()

    def analyze_performance_metrics(self, df: pd.DataFrame) -> Tuple[Optional[pd.DataFrame], int]:
        """åˆ†æå…³é”®æ€§èƒ½æŒ‡æ ‡"""
        print("\nğŸ¯ QPS Performance Metrics Analysis")
        print("=" * 50)

        if 'current_qps' not in df.columns or len(df) == 0:
            print("âŒ No valid QPS data for analysis")
            return None, 0

        max_qps = df['current_qps'].max()
        qps_range = sorted(df['current_qps'].unique())

        print(f"Maximum QPS tested: {max_qps}")
        print(f"QPS range: {min(qps_range)} - {max_qps}")
        print(f"Number of QPS levels: {len(qps_range)}")

        # æŒ‰QPSåˆ†ç»„ç»Ÿè®¡
        qps_stats_dict = {
            'cpu_usage': ['mean', 'max'],
            'mem_usage': ['mean', 'max']
        }
        
        # åªæœ‰å½“rpc_latency_mså­—æ®µå­˜åœ¨ä¸”æœ‰æœ‰æ•ˆæ•°æ®æ—¶æ‰æ·»åŠ 
        if 'rpc_latency_ms' in df.columns and df['rpc_latency_ms'].notna().any():
            qps_stats_dict['rpc_latency_ms'] = ['mean', 'max']
        
        qps_stats = df.groupby('current_qps').agg(qps_stats_dict).round(2)

        print("\nQPS Performance Statistics:")
        print(qps_stats.to_string())

        return qps_stats, max_qps

    def identify_bottlenecks(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        print("\nğŸ” QPS Performance Bottleneck Analysis")
        print("=" * 50)

        if len(df) == 0:
            print("âŒ No data for bottleneck analysis")
            return {}

        bottlenecks = {}

        # CPUç“¶é¢ˆ
        if 'cpu_usage' in df.columns and 'current_qps' in df.columns:
            cpu_bottleneck = df[df['cpu_usage'] > self.cpu_threshold]['current_qps'].min()
            if pd.notna(cpu_bottleneck):
                bottlenecks['CPU'] = cpu_bottleneck

        # å†…å­˜ç“¶é¢ˆ
        mem_bottleneck = df[df['mem_usage'] > self.memory_threshold]['current_qps'].min()
        if pd.notna(mem_bottleneck):
            bottlenecks['Memory'] = mem_bottleneck

        # RPCå»¶è¿Ÿç“¶é¢ˆ
        if 'rpc_latency_ms' in df.columns and df['rpc_latency_ms'].notna().any():
            rpc_bottleneck = df[df['rpc_latency_ms'] > self.rpc_threshold]['current_qps'].min()
            if pd.notna(rpc_bottleneck):
                bottlenecks['RPC_Latency'] = rpc_bottleneck

        if bottlenecks:
            print("System bottlenecks detected:")
            for bottleneck_type, qps in bottlenecks.items():
                print(f"  {bottleneck_type}: First occurs at QPS {qps:,}" if not pd.isna(qps) else f"  {bottleneck_type}: First occurs at QPS N/A")
        else:
            print("âœ… No critical system bottlenecks detected in tested range")

        return bottlenecks

    def generate_performance_charts(self, df: pd.DataFrame) -> Optional[plt.Figure]:
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨ - 2x3 å¸ƒå±€"""
        print("\nğŸ“ˆ Generating performance charts...")

        if len(df) == 0:
            print("âŒ No QPS data for chart generation")
            return None

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Blockchain Node QPS Performance Analysis Dashboard', fontsize=UnifiedChartStyle.FONT_CONFIG["title_size"], fontweight='bold')

        # åŠ è½½ Vegeta Success Rate æ•°æ®
        success_df = self.load_vegeta_success_rates()
        has_success_data = not success_df.empty
        
        # ä¸º Vegeta æ•°æ®æ·»åŠ å»¶è¿Ÿæ•°å€¼åˆ—
        if has_success_data:
            success_df['avg_latency_ms'] = success_df['avg_latency'].apply(self._parse_latency_to_ms)

        # [0,0] CPU Time Series
        ax1 = axes[0, 0]
        qps_levels = sorted(df['current_qps'].unique())
        colors = plt.cm.tab10(np.linspace(0, 1, len(qps_levels)))
        
        for idx, qps in enumerate(qps_levels):
            df_step = df[df['current_qps'] == qps]
            ax1.plot(df_step['timestamp'], df_step['cpu_usage'], 
                    color=colors[idx], label=f'{int(qps)} QPS', linewidth=1.5, alpha=0.7)
        
        ax1.axhline(y=85, color=UnifiedChartStyle.COLORS["critical"], linestyle='--', alpha=0.8, linewidth=2, label='Threshold (85%)')
        ax1.set_title('CPU Usage Time Series', fontsize=UnifiedChartStyle.FONT_CONFIG['subtitle_size'])
        ax1.set_xlabel('Time', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
        ax1.set_ylabel('CPU %', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
        ax1.legend(loc='upper left', bbox_to_anchor=(1.02, 1), ncol=1, fontsize=7)
        ax1.grid(True, alpha=0.3)
        UnifiedChartStyle.format_time_axis(ax1, df['timestamp'])

        # [0,1] Memory Time Series
        ax2 = axes[0, 1]
        for idx, qps in enumerate(qps_levels):
            df_step = df[df['current_qps'] == qps]
            ax2.plot(df_step['timestamp'], df_step['mem_usage'], 
                    color=colors[idx], label=f'{int(qps)} QPS', linewidth=1.5, alpha=0.7)
        
        ax2.axhline(y=90, color=UnifiedChartStyle.COLORS["critical"], linestyle='--', alpha=0.8, linewidth=2, label='Threshold (90%)')
        ax2.set_title('Memory Usage Time Series', fontsize=UnifiedChartStyle.FONT_CONFIG['subtitle_size'])
        ax2.set_xlabel('Time', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
        ax2.set_ylabel('Memory %', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
        ax2.legend(loc='upper left', bbox_to_anchor=(1.02, 1), ncol=1, fontsize=7)
        ax2.grid(True, alpha=0.3)
        UnifiedChartStyle.format_time_axis(ax2, df['timestamp'])

        # [0,2] Latency Time Series
        ax3 = axes[0, 2]
        df_latency = df[df['rpc_latency_ms'] > 0]
        
        for idx, qps in enumerate(qps_levels):
            df_step = df_latency[df_latency['current_qps'] == qps]
            if len(df_step) > 0:
                ax3.plot(df_step['timestamp'], df_step['rpc_latency_ms'], 
                        color=colors[idx], label=f'{int(qps)} QPS', linewidth=1.5, marker='o', markersize=3, alpha=0.7)
        
        ax3.axhline(y=1000, color=UnifiedChartStyle.COLORS["warning"], linestyle='--', alpha=0.8, linewidth=2, label='Threshold (1s)')
        ax3.set_title('RPC Latency Time Series', fontsize=UnifiedChartStyle.FONT_CONFIG['subtitle_size'])
        ax3.set_xlabel('Time', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
        ax3.set_ylabel('Latency (ms)', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
        ax3.legend(loc='upper left', bbox_to_anchor=(1.02, 1), ncol=1, fontsize=7)
        ax3.grid(True, alpha=0.3)
        UnifiedChartStyle.format_time_axis(ax3, df_latency['timestamp'])

        # [1,0] Latency & Success Rate vs QPS (åŒYè½´)
        ax4 = axes[1, 0]
        if has_success_data:
            line1 = ax4.plot(success_df['qps'], success_df['avg_latency_ms'], 
                            'ro-', alpha=0.7, markersize=6, linewidth=2, label='Avg Latency')
            ax4.axhline(y=1000, color=UnifiedChartStyle.COLORS["warning"], 
                       linestyle='--', alpha=0.8, linewidth=2, label='Latency Threshold (1s)')
            ax4.set_xlabel('QPS', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
            ax4.set_ylabel('Latency (ms)', color='r', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
            ax4.tick_params(axis='y', labelcolor='r')
            ax4.grid(True, alpha=0.3)
            
            ax4_right = ax4.twinx()
            line2 = ax4_right.plot(success_df['qps'], success_df['success_rate'], 
                                  'g^-', alpha=0.7, markersize=8, linewidth=2, label='Success Rate')
            ax4_right.axhline(y=95, color=UnifiedChartStyle.COLORS["warning"], 
                             linestyle='--', alpha=0.8, linewidth=2, label='Success Threshold (95%)')
            ax4_right.set_ylabel('Success Rate (%)', color='g', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
            ax4_right.set_ylim(0, 105)
            ax4_right.tick_params(axis='y', labelcolor='g')
            
            lines = line1 + line2
            labels = [l.get_label() for l in lines]
            ax4.legend(lines, labels, loc='upper left', fontsize=UnifiedChartStyle.FONT_CONFIG['legend_size'])
        else:
            ax4.plot(df['current_qps'], df['rpc_latency_ms'], 'ro-', alpha=0.7, markersize=4)
            ax4.axhline(y=1000, color=UnifiedChartStyle.COLORS["warning"], 
                       linestyle='--', alpha=0.8, label='High Latency (1s)')
            ax4.set_xlabel('QPS', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
            ax4.set_ylabel('Latency (ms)', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
            ax4.legend(fontsize=UnifiedChartStyle.FONT_CONFIG['legend_size'])
            ax4.grid(True, alpha=0.3)
        
        ax4.set_title('RPC Latency & Success Rate vs QPS', fontsize=UnifiedChartStyle.FONT_CONFIG['subtitle_size'])
        
        # Disable scientific notation on axes
        ax4.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:,.0f}'))
        ax4.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:,.0f}'))

        # [1,1] QPS vs Success Rate Scatter
        ax5 = axes[1, 1]
        if has_success_data:
            # çº¯æ•£ç‚¹å›¾ï¼ˆä¸ç”»è¿æ¥çº¿ï¼Œå› ä¸ºæ²¡æœ‰ä¸­é—´æ•°æ®ï¼‰
            colors_scatter = []
            for sr in success_df['success_rate']:
                if sr >= 95:
                    colors_scatter.append(UnifiedChartStyle.COLORS["success"])
                elif sr >= 80:
                    colors_scatter.append(UnifiedChartStyle.COLORS["warning"])
                else:
                    colors_scatter.append(UnifiedChartStyle.COLORS["critical"])
            
            # ç»˜åˆ¶æ•£ç‚¹ï¼ˆå‚è€ƒ EBS å›¾è¡¨æ ·å¼ï¼šå°åœ†ç‚¹ï¼Œæ— é»‘è‰²è¾¹æ¡†ï¼‰
            ax5.scatter(success_df['qps'], success_df['success_rate'], 
                       c=colors_scatter, s=60, alpha=0.8, zorder=2)
            
            # é˜ˆå€¼çº¿
            ax5.axhline(y=95, color=UnifiedChartStyle.COLORS["warning"], 
                       linestyle='--', alpha=0.8, linewidth=2, label='Threshold (95%)')
            
            # æ ‡æ³¨ä½æˆåŠŸç‡çš„ç‚¹
            for idx, row in success_df.iterrows():
                if row['success_rate'] < 95:
                    ax5.annotate(f"{int(row['qps'])}\n{row['success_rate']:.1f}%", 
                               xy=(row['qps'], row['success_rate']),
                               xytext=(0, -15), textcoords='offset points',
                               fontsize=8, color='red', ha='center', fontweight='bold')
            
            # å¢åŠ é¢œè‰²å›¾ä¾‹
            from matplotlib.patches import Patch
            legend_elements = [
                Patch(facecolor=UnifiedChartStyle.COLORS["success"], label='Healthy (â‰¥95%)'),
                Patch(facecolor=UnifiedChartStyle.COLORS["warning"], label='Warning (80-95%)'),
                Patch(facecolor=UnifiedChartStyle.COLORS["critical"], label='Critical (<80%)')
            ]
            ax5.legend(handles=legend_elements, loc='lower left', fontsize=UnifiedChartStyle.FONT_CONFIG['legend_size'])
            
            ax5.set_ylim(0, 105)
        
        ax5.set_title('QPS vs Success Rate (Performance Cliff Detection)', fontsize=UnifiedChartStyle.FONT_CONFIG['subtitle_size'])
        ax5.set_xlabel('QPS', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
        ax5.set_ylabel('Success Rate (%)', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
        ax5.grid(True, alpha=0.3)

        # [1,2] Success Rate Distribution
        ax6 = axes[1, 2]
        if has_success_data:
            ax6.hist(success_df['success_rate'], bins=15, alpha=0.7, color='skyblue', edgecolor='black', linewidth=1.5)
            
            mean_sr = success_df['success_rate'].mean()
            median_sr = success_df['success_rate'].median()
            ax6.axvline(mean_sr, color=UnifiedChartStyle.COLORS["critical"], 
                       linestyle='--', linewidth=2, label=f'Mean: {mean_sr:.1f}%')
            ax6.axvline(median_sr, color=UnifiedChartStyle.COLORS["warning"], 
                       linestyle='--', linewidth=2, label=f'Median: {median_sr:.1f}%')
            ax6.axvline(95, color=UnifiedChartStyle.COLORS["success"], 
                       linestyle='--', linewidth=2, label='Threshold (95%)')
        
        ax6.set_title('Success Rate Distribution', fontsize=UnifiedChartStyle.FONT_CONFIG['subtitle_size'])
        ax6.set_xlabel('Success Rate (%)', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
        ax6.set_ylabel('Frequency', fontsize=UnifiedChartStyle.FONT_CONFIG['label_size'])
        ax6.legend(fontsize=UnifiedChartStyle.FONT_CONFIG['legend_size'])
        ax6.grid(True, alpha=0.3)

        # ä½¿ç”¨ç»Ÿä¸€æ ·å¼åº”ç”¨å¸ƒå±€
        UnifiedChartStyle.apply_layout('auto')
        
        # ä¿å­˜å›¾è¡¨
        reports_dir = os.getenv('REPORTS_DIR', os.path.join(self.output_dir, 'current', 'reports'))
        chart_file = os.path.join(reports_dir, 'qps_performance_analysis.png')
        os.makedirs(os.path.dirname(chart_file), exist_ok=True)
        plt.savefig(chart_file, dpi=300, bbox_inches='tight', 
                   facecolor='white', edgecolor='none')
        print(f"âœ… Performance charts saved: {chart_file}")
        plt.close()

        return fig

    def load_vegeta_success_rates(self) -> pd.DataFrame:
        """ä» vegeta txt æŠ¥å‘Šæå– QPS, Success Rate, Latency"""
        reports_dir = os.getenv('REPORTS_DIR', os.path.join(self.output_dir, 'current', 'reports'))
        vegeta_reports = glob.glob(f"{reports_dir}/vegeta_*qps_*.txt")
        data = []
        
        for report in vegeta_reports:
            try:
                filename = os.path.basename(report)
                qps = int(re.search(r'vegeta_(\d+)qps_', filename).group(1))
                
                with open(report, 'r') as f:
                    content = f.read()
                
                # æå– Success Rate
                success_match = re.search(r'Success\s+\[ratio\]\s+([\d.]+)%', content)
                success_rate = float(success_match.group(1)) if success_match else 0.0
                
                # æå– Latency (mean)
                latency_match = re.search(r'Latencies\s+\[min, mean,.*?\]\s+[\d.Âµms]+,\s+([\d.Âµmsh]+),', content)
                avg_latency = latency_match.group(1) if latency_match else 'N/A'
                
                data.append({
                    'qps': qps,
                    'success_rate': success_rate,
                    'avg_latency': avg_latency
                })
            except Exception as e:
                logger.warning(f"âš ï¸  Failed to parse {filename}: {e}")
        
        if data:
            df = pd.DataFrame(data).sort_values('qps')
            print(f"âœ… Loaded success rate data for {len(df)} QPS levels")
            return df
        else:
            print("âš ï¸  No success rate data found")
            return pd.DataFrame()
    
    def _parse_latency_to_ms(self, latency_str: str) -> float:
        """å°† Vegeta çš„å»¶è¿Ÿå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¯«ç§’æ•°å€¼"""
        try:
            if 'm' in latency_str and 's' in latency_str:  # å¦‚ "1m27s"
                parts = latency_str.replace('m', ' ').replace('s', '').split()
                minutes = float(parts[0]) if len(parts) > 0 else 0
                seconds = float(parts[1]) if len(parts) > 1 else 0
                return (minutes * 60 + seconds) * 1000
            elif 's' in latency_str and 'ms' not in latency_str:  # å¦‚ "31.43s"
                return float(latency_str.replace('s', '')) * 1000
            elif 'ms' in latency_str:  # å¦‚ "110.256ms"
                return float(latency_str.replace('ms', ''))
            elif 'Âµs' in latency_str:  # å¦‚ "76.231Âµs"
                return float(latency_str.replace('Âµs', '')) / 1000
            else:
                return 0.0
        except:
            return 0.0

    def analyze_vegeta_reports(self) -> Optional[pd.DataFrame]:
        """åˆ†æVegetaæµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“‹ Vegeta Reports Analysis")
        print("=" * 50)

        reports_dir = os.getenv('REPORTS_DIR', os.path.join(self.output_dir, 'current', 'reports'))
        reports = glob.glob(f"{reports_dir}/vegeta_*.txt")  # Only parse vegeta reports
        if not reports:
            print("No Vegeta reports found")
            return None

        report_data = []
        for report_file in sorted(reports):
            try:
                # ä¿®å¤æ–‡ä»¶åè§£æé€»è¾‘ï¼šå¤„ç†vegeta_1000qps_timestamp.txtæ ¼å¼
                filename = os.path.basename(report_file)
                qps_part = filename.split('_')[1]  # è·å–"1000qps"éƒ¨åˆ†
                qps = int(qps_part.replace('qps', ''))  # ç§»é™¤"qps"åç¼€å¹¶è½¬æ¢ä¸ºæ•´æ•°
                with open(report_file, 'r') as f:
                    content = f.read()

                success_rate = 0
                avg_latency = 'N/A'
                p99_latency = 'N/A'

                for line in content.split('\n'):
                    if 'Success' in line and '[ratio]' in line:
                        success_rate = float(line.split()[-1].replace('%', ''))
                    elif 'Latencies' in line and '[min, mean,' in line:
                        parts = line.split()
                        if len(parts) >= 8:
                            avg_latency = parts[6].replace(',', '')
                            p99_latency = parts[8].replace(',', '')

                report_data.append({
                    'QPS': qps,
                    'Success_Rate': success_rate,
                    'Avg_Latency': avg_latency,
                    'P99_Latency': p99_latency
                })
            except Exception as e:
                print(f"Warning: Could not parse {report_file}: {e}")

        if report_data:
            vegeta_df = pd.DataFrame(report_data)
            print(vegeta_df.to_string(index=False))
            return vegeta_df

        return None

    def _evaluate_performance_by_bottleneck_analysis(self, benchmark_mode: str, max_qps: int, 
                                                   bottlenecks: Dict[str, Any], avg_cpu: float, 
                                                   avg_mem: float, avg_rpc: float) -> Dict[str, Any]:
        """
        åŸºäºç“¶é¢ˆåˆ†æçš„ç§‘å­¦æ€§èƒ½è¯„ä¼°
        æ›¿ä»£ç¡¬ç¼–ç çš„60000/40000/20000é€»è¾‘
        """
        
        # åªæœ‰æ·±åº¦åŸºå‡†æµ‹è¯•æ¨¡å¼æ‰èƒ½è¿›è¡Œå‡†ç¡®çš„æ€§èƒ½ç­‰çº§è¯„ä¼°
        if benchmark_mode != "intensive":
            return {
                'performance_level': 'æ— æ³•è¯„ä¼°',
                'performance_grade': 'N/A',
                'evaluation_reason': f'{benchmark_mode}åŸºå‡†æµ‹è¯•æ¨¡å¼æ— æ³•å‡†ç¡®è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ç­‰çº§ï¼Œéœ€è¦intensiveæ¨¡å¼è¿›è¡Œæ·±åº¦åˆ†æ',
                'evaluation_basis': 'insufficient_benchmark_depth',
                'max_sustainable_qps': max_qps,
                'recommendations': [
                    f'å½“å‰{benchmark_mode}åŸºå‡†æµ‹è¯•ä»…ç”¨äºå¿«é€ŸéªŒè¯',
                    'å¦‚éœ€å‡†ç¡®çš„æ€§èƒ½ç­‰çº§è¯„ä¼°ï¼Œè¯·ä½¿ç”¨intensiveåŸºå‡†æµ‹è¯•æ¨¡å¼',
                    'æ·±åº¦åŸºå‡†æµ‹è¯•å°†è§¦å‘ç³»ç»Ÿç“¶é¢ˆä»¥è·å¾—å‡†ç¡®çš„æ€§èƒ½è¯„ä¼°'
                ]
            }
        
        # æ·±åº¦åŸºå‡†æµ‹è¯•æ¨¡å¼ä¸‹çš„ç“¶é¢ˆåˆ†æè¯„ä¼°
        bottleneck_types = bottlenecks.get('detected_bottlenecks', [])
        bottleneck_count = len(bottleneck_types)
        
        # è®¡ç®—ç“¶é¢ˆä¸¥é‡ç¨‹åº¦è¯„åˆ†
        bottleneck_score = self._calculate_bottleneck_severity_score(
            bottleneck_types, avg_cpu, avg_mem, avg_rpc
        )
        
        # åŸºäºç“¶é¢ˆè¯„åˆ†çš„ç§‘å­¦ç­‰çº§è¯„ä¼°
        if bottleneck_score < 0.2:
            # ä½ç“¶é¢ˆè¯„åˆ† = ä¼˜ç§€æ€§èƒ½
            level = "ä¼˜ç§€"
            grade = "A (Excellent)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹æœªå‡ºç°æ˜æ˜¾ç“¶é¢ˆï¼Œæ€§èƒ½è¡¨ç°ä¼˜ç§€"
            
        elif bottleneck_score < 0.4:
            # ä¸­ç­‰ç“¶é¢ˆè¯„åˆ† = è‰¯å¥½æ€§èƒ½
            level = "è‰¯å¥½"
            grade = "B (Good)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹å‡ºç°è½»å¾®ç“¶é¢ˆ: {', '.join(bottleneck_types)}"
            
        elif bottleneck_score < 0.7:
            # è¾ƒé«˜ç“¶é¢ˆè¯„åˆ† = ä¸€èˆ¬æ€§èƒ½
            level = "ä¸€èˆ¬"
            grade = "C (Acceptable)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹å‡ºç°æ˜æ˜¾ç“¶é¢ˆ: {', '.join(bottleneck_types)}"
            
        else:
            # é«˜ç“¶é¢ˆè¯„åˆ† = éœ€è¦ä¼˜åŒ–
            level = "éœ€è¦ä¼˜åŒ–"
            grade = "D (Needs Improvement)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹å‡ºç°ä¸¥é‡ç“¶é¢ˆ: {', '.join(bottleneck_types)}"
        
        return {
            'performance_level': level,
            'performance_grade': grade,
            'evaluation_reason': reason,
            'evaluation_basis': 'intensive_bottleneck_analysis',
            'max_sustainable_qps': max_qps,
            'bottleneck_score': bottleneck_score,
            'bottleneck_types': bottleneck_types,
            'bottleneck_count': bottleneck_count,
            'recommendations': self._generate_bottleneck_based_recommendations(
                bottleneck_types, bottleneck_score, max_qps
            )
        }
    
    def _calculate_bottleneck_severity_score(self, bottleneck_types: list, 
                                           avg_cpu: float, avg_mem: float, avg_rpc: float) -> float:
        """è®¡ç®—ç“¶é¢ˆä¸¥é‡ç¨‹åº¦è¯„åˆ†"""
        
        # ç“¶é¢ˆç±»å‹æƒé‡
        bottleneck_weights = {
            'CPU': 0.2,
            'Memory': 0.25,
            'EBS': 0.3,
            'Network': 0.15,
            'RPC': 0.1
        }
        
        total_score = 0.0
        
        # åŸºäºæ£€æµ‹åˆ°çš„ç“¶é¢ˆç±»å‹è®¡ç®—è¯„åˆ†
        for bottleneck_type in bottleneck_types:
            weight = bottleneck_weights.get(bottleneck_type, 0.1)
            
            # æ ¹æ®å…·ä½“æŒ‡æ ‡è°ƒæ•´ä¸¥é‡ç¨‹åº¦
            severity_multiplier = 1.0
            if bottleneck_type == 'CPU' and avg_cpu > (self.cpu_threshold + 5):
                severity_multiplier = 1.5
            elif bottleneck_type == 'Memory' and avg_mem > (self.memory_threshold + 5):
                severity_multiplier = 1.5
            elif bottleneck_type == 'RPC' and avg_rpc > (self.rpc_threshold * 2):
                severity_multiplier = 1.5
            
            total_score += weight * severity_multiplier
        
        # å½’ä¸€åŒ–è¯„åˆ†åˆ°0-1èŒƒå›´
        return min(total_score, 1.0)
    
    def _generate_capacity_assessment(self, performance_evaluation: Dict[str, Any], max_qps: int) -> str:
        """åŸºäºæ€§èƒ½è¯„ä¼°ç”Ÿæˆå®¹é‡è¯„ä¼°"""
        performance_level = performance_evaluation.get('performance_level', 'æœªçŸ¥')
        bottleneck_score = performance_evaluation.get('bottleneck_score', 0)
        
        if performance_level == "ä¼˜ç§€":
            return f"å½“å‰é…ç½®å¯ç¨³å®šå¤„ç†é«˜è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œç“¶é¢ˆè¯„åˆ†: {bottleneck_score:.3f})" if not pd.isna(max_qps) else f"å½“å‰é…ç½®å¯ç¨³å®šå¤„ç†é«˜è´Ÿè½½ (æµ‹è¯•æ•°æ®ä¸è¶³ï¼Œç“¶é¢ˆè¯„åˆ†: {bottleneck_score:.3f})"
        elif performance_level == "è‰¯å¥½":
            return f"å½“å‰é…ç½®å¯å¤„ç†ä¸­é«˜è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œç“¶é¢ˆè¯„åˆ†: {bottleneck_score:.3f})" if not pd.isna(max_qps) else f"å½“å‰é…ç½®å¯å¤„ç†ä¸­é«˜è´Ÿè½½ (æµ‹è¯•æ•°æ®ä¸è¶³ï¼Œç“¶é¢ˆè¯„åˆ†: {bottleneck_score:.3f})"
        elif performance_level == "ä¸€èˆ¬":
            return f"å½“å‰é…ç½®é€‚åˆä¸­ç­‰è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œç“¶é¢ˆè¯„åˆ†: {bottleneck_score:.3f})" if not pd.isna(max_qps) else f"å½“å‰é…ç½®é€‚åˆä¸­ç­‰è´Ÿè½½ (æµ‹è¯•æ•°æ®ä¸è¶³ï¼Œç“¶é¢ˆè¯„åˆ†: {bottleneck_score:.3f})"
        elif performance_level == "éœ€è¦ä¼˜åŒ–":
            return f"å½“å‰é…ç½®éœ€è¦ä¼˜åŒ–ä»¥å¤„ç†é«˜è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œç“¶é¢ˆè¯„åˆ†: {bottleneck_score:.3f})" if not pd.isna(max_qps) else f"å½“å‰é…ç½®éœ€è¦ä¼˜åŒ–ä»¥å¤„ç†é«˜è´Ÿè½½ (æµ‹è¯•æ•°æ®ä¸è¶³ï¼Œç“¶é¢ˆè¯„åˆ†: {bottleneck_score:.3f})"
        else:
            return f"éœ€è¦intensiveåŸºå‡†æµ‹è¯•æ¨¡å¼è¿›è¡Œå‡†ç¡®çš„å®¹é‡è¯„ä¼°"

    def _generate_bottleneck_based_recommendations(self, bottleneck_types: list, 
                                                 bottleneck_score: float, max_qps: int) -> list:
        """åŸºäºç“¶é¢ˆåˆ†æç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if bottleneck_score < 0.2:
            recommendations.extend([
                f"ğŸ‰ ç³»ç»Ÿæ€§èƒ½ä¼˜ç§€ï¼Œå½“å‰é…ç½®å¯ç¨³å®šæ”¯æŒ {max_qps} QPS",
                "ğŸ’¡ å¯è€ƒè™‘è¿›ä¸€æ­¥æå‡QPSç›®æ ‡æˆ–ä¼˜åŒ–æˆæœ¬æ•ˆç‡",
                "ğŸ“Š å»ºè®®å®šæœŸç›‘æ§ä»¥ç»´æŒå½“å‰æ€§èƒ½æ°´å¹³"
            ])
        else:
            # åŸºäºå…·ä½“ç“¶é¢ˆç±»å‹çš„é’ˆå¯¹æ€§å»ºè®®
            if 'CPU' in bottleneck_types:
                recommendations.append("ğŸ”§ CPUç“¶é¢ˆï¼šè€ƒè™‘å‡çº§CPUæˆ–ä¼˜åŒ–è®¡ç®—å¯†é›†å‹è¿›ç¨‹")
            if 'Memory' in bottleneck_types:
                recommendations.append("ğŸ”§ å†…å­˜ç“¶é¢ˆï¼šè€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
            if 'EBS' in bottleneck_types:
                recommendations.append("ğŸ”§ å­˜å‚¨ç“¶é¢ˆï¼šè€ƒè™‘å‡çº§EBSç±»å‹æˆ–ä¼˜åŒ–I/Oæ¨¡å¼")
            if 'Network' in bottleneck_types:
                recommendations.append("ğŸ”§ ç½‘ç»œç“¶é¢ˆï¼šè€ƒè™‘å‡çº§ç½‘ç»œå¸¦å®½æˆ–ä¼˜åŒ–ç½‘ç»œé…ç½®")
            if 'RPC' in bottleneck_types:
                recommendations.append("ğŸ”§ RPCç“¶é¢ˆï¼šè€ƒè™‘ä¼˜åŒ–RPCé…ç½®æˆ–å¢åŠ RPCè¿æ¥æ± ")
        
        return recommendations

    def generate_performance_report(self, df: pd.DataFrame, max_qps: int, 
                                  bottlenecks: Dict[str, Any], benchmark_mode: str = "standard") -> str:
        """ç”ŸæˆåŸºäºç“¶é¢ˆåˆ†æçš„æ€§èƒ½æŠ¥å‘Š"""
        print("\nğŸ“„ Generating performance report...")

        # åŸºæœ¬æ€§èƒ½æŒ‡æ ‡
        avg_cpu = df['cpu_usage'].mean() if len(df) > 0 and 'cpu_usage' in df.columns else 0
        avg_mem = df['mem_usage'].mean() if len(df) > 0 and 'mem_usage' in df.columns else 0
        avg_rpc = df['rpc_latency_ms'].mean() if len(df) > 0 and 'rpc_latency_ms' in df.columns and df['rpc_latency_ms'].notna().any() else 0

        # åŸºäºåŸºå‡†æµ‹è¯•æ¨¡å¼å’Œç“¶é¢ˆåˆ†æçš„æ€§èƒ½è¯„ä¼°
        performance_evaluation = self._evaluate_performance_by_bottleneck_analysis(
            benchmark_mode, max_qps, bottlenecks, avg_cpu, avg_mem, avg_rpc
        )

        # å¤„ç†å¯èƒ½çš„NaNå€¼
        max_qps_display = f"{max_qps:,}" if not pd.isna(max_qps) else "N/A"
        
        report = f"""# Blockchain Node QPS Performance Analysis Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Executive Summary
- **Maximum QPS Achieved**: {max_qps_display}
- **Performance Grade**: {performance_evaluation['performance_grade']}
- **Performance Level**: {performance_evaluation['performance_level']}
- **Benchmark Mode**: {benchmark_mode}
- **Test Duration**: {len(df)} monitoring points

## Performance Evaluation
- **Evaluation Basis**: {performance_evaluation['evaluation_basis']}
- **Evaluation Reason**: {performance_evaluation['evaluation_reason']}

## System Performance Metrics
- **Average CPU Usage**: {avg_cpu:.1f}%
- **Average Memory Usage**: {avg_mem:.1f}%
- **Average RPC Latency**: {avg_rpc:.1f}ms
- **CPU Peak**: {(df['cpu_usage'].max() if len(df) > 0 and 'cpu_usage' in df.columns else 0):.1f}%
- **Memory Peak**: {(df['mem_usage'].max() if len(df) > 0 and 'mem_usage' in df.columns else 0):.1f}%
- **RPC Latency Peak**: {(df['rpc_latency_ms'].max() if len(df) > 0 and 'rpc_latency_ms' in df.columns and df['rpc_latency_ms'].notna().any() else 0):.1f}ms

## Performance Bottlenecks Analysis
"""

        if performance_evaluation.get('bottleneck_types'):
            report += f"- **Bottleneck Score**: {performance_evaluation.get('bottleneck_score', 0):.3f}\n"
            report += f"- **Detected Bottlenecks**: {', '.join(performance_evaluation['bottleneck_types'])}\n"
            for bottleneck_type in performance_evaluation['bottleneck_types']:
                qps = bottlenecks.get(bottleneck_type, 'Unknown')
                report += f"  - **{bottleneck_type}**: First detected at {qps} QPS\n" if isinstance(qps, int) else f"  - **{bottleneck_type}**: {qps}\n"
        else:
            report += "- âœ… No critical bottlenecks detected in tested range\n"

        report += f"""
## Optimization Recommendations

### Based on Bottleneck Analysis
"""

        # ä½¿ç”¨åŸºäºç“¶é¢ˆåˆ†æçš„å»ºè®®
        for recommendation in performance_evaluation.get('recommendations', []):
            report += f"- {recommendation}\n"

        # è®¡ç®—æ¨èç”Ÿäº§QPS
        recommended_qps_display = f"{int(max_qps * 0.8):,} (80% of maximum tested)" if not pd.isna(max_qps) else "N/A (insufficient test data)"
        
        report += f"""
### Production Deployment Guidelines
- **Recommended Production QPS**: {recommended_qps_display}
- **Monitoring Thresholds**: 
  - Alert if CPU usage > 85%
  - Alert if Memory usage > 90%
  - Alert if RPC latency > 1000ms sustained
- **Capacity Assessment**: {self._generate_capacity_assessment(performance_evaluation, max_qps)}

## Files Generated
- **Performance Charts**: `{self.reports_dir}/qps_performance_analysis.png`
- **Raw QPS Monitoring Data**: `{self.csv_file or 'N/A'}`
- **Vegeta Test Reports**: `{self.reports_dir}/`

---
*Report generated by Blockchain Node QPS Analyzer*
"""

        # ä¿å­˜æŠ¥å‘Š
        reports_dir = os.getenv('REPORTS_DIR', os.path.join(self.output_dir, 'current', 'reports'))
        report_file = os.path.join(reports_dir, 'qps_performance_report.md')
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        with open(report_file, 'w') as f:
            f.write(report)

        print(f"âœ… Performance report saved: {report_file}")
        return report

    def run_qps_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„QPSåˆ†æ"""
        print("ğŸš€ Starting Blockchain Node QPS Performance Analysis")
        print("=" * 60)

        # åŠ è½½QPSç›‘æ§æ•°æ®
        df = self.load_and_clean_data()

        # æ‰§è¡ŒQPSæ€§èƒ½åˆ†æ
        qps_stats, max_qps = self.analyze_performance_metrics(df)
        bottlenecks = self.identify_bottlenecks(df)

        # ç”Ÿæˆå›¾è¡¨å’ŒæŠ¥å‘Š
        self.generate_performance_charts(df)
        vegeta_analysis = self.analyze_vegeta_reports()
        report = self.generate_performance_report(df, max_qps, bottlenecks, self.benchmark_mode)

        analysis_results = {
            'dataframe': df,
            'qps_stats': qps_stats,
            'max_qps': max_qps,
            'bottlenecks': bottlenecks,
            'vegeta_analysis': vegeta_analysis,
            'report': report
        }

        print("\nğŸ‰ QPS Analysis Completed Successfully!")
        print("Generated files:")
        print(f"  ğŸ“Š Charts: {self.reports_dir}/qps_performance_analysis.png")
        print(f"  ğŸ“„ Report: {self.reports_dir}/qps_performance_report.md")

        return analysis_results


def main():
    """ä¸»æ‰§è¡Œå‡½æ•° - æ”¯æŒç“¶é¢ˆæ¨¡å¼å’Œæ€§èƒ½æ‚¬å´–åˆ†æ"""
    parser = argparse.ArgumentParser(description='QPSåˆ†æå™¨ - æ”¯æŒç“¶é¢ˆæ¨¡å¼')
    parser.add_argument('csv_file', help='CSVæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--benchmark-mode', default='standard', choices=['quick', 'standard', 'intensive'], 
                       help='åŸºå‡†æµ‹è¯•æ¨¡å¼ (é»˜è®¤: standard)')
    parser.add_argument('--bottleneck-mode', action='store_true', help='å¯ç”¨ç“¶é¢ˆåˆ†ææ¨¡å¼')
    parser.add_argument('--cliff-analysis', action='store_true', help='å¯ç”¨æ€§èƒ½æ‚¬å´–åˆ†æ')
    parser.add_argument('--max-qps', type=int, help='æœ€å¤§æˆåŠŸQPS')
    parser.add_argument('--bottleneck-qps', type=int, help='ç“¶é¢ˆè§¦å‘QPS')
    parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    try:
        if not os.path.exists(args.csv_file):
            logger.error(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {args.csv_file}")
            return 1
        
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = NodeQPSAnalyzer(args.output_dir, args.benchmark_mode, args.bottleneck_mode)
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(args.csv_file)
        logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æ¡è®°å½•")
        
        # æ€§èƒ½æ‚¬å´–åˆ†æ
        if args.cliff_analysis and args.max_qps and args.bottleneck_qps:
            logger.info("ğŸ“‰ æ‰§è¡Œæ€§èƒ½æ‚¬å´–åˆ†æ")
            cliff_analysis = analyzer.analyze_performance_cliff(df, args.max_qps, args.bottleneck_qps)
            
            # ç”Ÿæˆæ‚¬å´–åˆ†æå›¾è¡¨
            cliff_chart = analyzer.generate_cliff_analysis_chart(df, cliff_analysis)
            
            # ä¿å­˜åˆ†æç»“æœ
            cliff_result_file = os.path.join(analyzer.reports_dir, 'performance_cliff_analysis.json')
            with open(cliff_result_file, 'w') as f:
                json.dump(cliff_analysis, f, indent=2, default=str)
            logger.info(f"ğŸ“Š æ€§èƒ½æ‚¬å´–åˆ†æç»“æœå·²ä¿å­˜: {cliff_result_file}")
        
        # æ‰§è¡Œæ ‡å‡†QPSåˆ†æ
        result = analyzer.run_qps_analysis()
        
        if result:
            logger.info("âœ… QPSåˆ†æå®Œæˆ")
            return 0
        else:
            logger.error("âŒ QPSåˆ†æå¤±è´¥")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ QPSåˆ†ææ‰§è¡Œå¤±è´¥: {e}")
        return 1

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    if len(sys.argv) == 1:
        print("ğŸ“‹ QPSåˆ†æå™¨ä½¿ç”¨ç¤ºä¾‹:")
        print("python qps_analyzer.py data.csv")
        print("python qps_analyzer.py data.csv --bottleneck-mode")
        print("python qps_analyzer.py data.csv --cliff-analysis --max-qps 5000 --bottleneck-qps 3000")
    else:
        sys.exit(main())
