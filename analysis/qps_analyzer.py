#!/usr/bin/env python3
"""
QPSåˆ†æå™¨ - ä»comprehensive_analysis.pyæ‹†åˆ†å‡ºæ¥çš„ç‹¬ç«‹æ¨¡å— + ç“¶é¢ˆæ¨¡å¼æ”¯æŒ
ä¸“é—¨è´Ÿè´£QPSæ€§èƒ½åˆ†æï¼ŒåŒ…æ‹¬æ€§èƒ½æŒ‡æ ‡åˆ†æã€ç“¶é¢ˆè¯†åˆ«ã€å›¾è¡¨ç”Ÿæˆç­‰
æ”¯æŒæ€§èƒ½æ‚¬å´–åˆ†æå’Œç“¶é¢ˆæ£€æµ‹æ¨¡å¼
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import glob
import os
import json
import argparse
from datetime import datetime
from typing import Dict, Any, Optional, Tuple
from pathlib import Path

# ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç®¡ç†å™¨
import sys

# ä½¿ç”¨æ›´å¥å£®çš„è·¯å¾„ç®¡ç†
current_dir = Path(__file__).parent
utils_dir = current_dir.parent / 'utils'
if str(utils_dir) not in sys.path:
    sys.path.insert(0, str(utils_dir))

try:
    from unified_logger import get_logger
    logger = get_logger(__name__)
    logger.info("âœ… ç»Ÿä¸€æ—¥å¿—ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
except ImportError as e:
    # é™çº§åˆ°æ ‡å‡†logging
    import logging
    logger = logging.getLogger(__name__)
    logger.warning(f"âš ï¸ ç»Ÿä¸€æ—¥å¿—ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†logging: {e}")

class NodeQPSAnalyzer:
    """åŒºå—é“¾èŠ‚ç‚¹ QPSæ€§èƒ½åˆ†æå™¨ + ç“¶é¢ˆæ¨¡å¼æ”¯æŒ - æ”¯æŒå¤šç§åŒºå—é“¾"""

    def __init__(self, output_dir: Optional[str] = None, benchmark_mode: str = "standard", bottleneck_mode: bool = False):
        """
        åˆå§‹åŒ–QPSåˆ†æå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œå°†ä»ç¯å¢ƒå˜é‡è·å–ï¼‰
            benchmark_mode: åŸºå‡†æµ‹è¯•æ¨¡å¼ (quick/standard/intensive)
            bottleneck_mode: æ˜¯å¦å¯ç”¨ç“¶é¢ˆåˆ†ææ¨¡å¼
        """
        if output_dir is None:
            output_dir = os.environ.get('DATA_DIR', os.path.join(os.path.expanduser('~'), 'blockchain-node-benchmark-result'))
        
        self.output_dir = output_dir
        self.benchmark_mode = benchmark_mode
        self.bottleneck_mode = bottleneck_mode
        self.reports_dir = os.path.join(output_dir, 'reports')
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # ç“¶é¢ˆæ£€æµ‹é˜ˆå€¼é…ç½®
        self.cpu_threshold = int(os.getenv('BOTTLENECK_CPU_THRESHOLD', 85))
        self.memory_threshold = int(os.getenv('BOTTLENECK_MEMORY_THRESHOLD', 90))
        self.rpc_threshold = int(os.getenv('MAX_LATENCY_THRESHOLD', 1000))
        
        # åˆå§‹åŒ–CSVæ–‡ä»¶è·¯å¾„ - ä¿®å¤ç¼ºå¤±çš„å±æ€§
        self.csv_file = self.get_latest_csv()

        # Using English labels system directly
        
        logger.info(f"ğŸ” QPSåˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {output_dir}, åŸºå‡†æµ‹è¯•æ¨¡å¼: {benchmark_mode}")
        if bottleneck_mode:
            logger.info("ğŸš¨ ç“¶é¢ˆåˆ†ææ¨¡å¼å·²å¯ç”¨")

    def _get_dynamic_key_metrics(self, df: pd.DataFrame) -> list:
        """åŠ¨æ€è·å–å…³é”®æŒ‡æ ‡å­—æ®µï¼Œæ›¿ä»£ç¡¬ç¼–ç è®¾å¤‡å - å®Œæ•´ç‰ˆæœ¬"""
        base_metrics = ['cpu_usage', 'mem_usage']
        
        # åŠ¨æ€æŸ¥æ‰¾EBSåˆ©ç”¨ç‡å­—æ®µï¼ˆä¼˜å…ˆDATAè®¾å¤‡ï¼Œç„¶åACCOUNTSè®¾å¤‡ï¼‰
        ebs_util_field = None
        # é¦–å…ˆæŸ¥æ‰¾DATAè®¾å¤‡å­—æ®µï¼ˆå¿…é¡»å­˜åœ¨ï¼‰
        for col in df.columns:
            if col.startswith('data_') and col.endswith('_util'):
                ebs_util_field = col
                break
        
        # å¦‚æœæ²¡æœ‰DATAè®¾å¤‡å­—æ®µï¼ŒæŸ¥æ‰¾ACCOUNTSè®¾å¤‡å­—æ®µï¼ˆå¯é€‰ï¼‰
        if not ebs_util_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_util'):
                    ebs_util_field = col
                    break
        
        # åŠ¨æ€æŸ¥æ‰¾EBSå»¶è¿Ÿå­—æ®µï¼ˆä¼˜å…ˆDATAè®¾å¤‡çš„r_awaitï¼‰
        ebs_latency_field = None
        # é¦–å…ˆæŸ¥æ‰¾DATAè®¾å¤‡çš„r_awaitå­—æ®µ
        for col in df.columns:
            if col.startswith('data_') and col.endswith('_r_await'):
                ebs_latency_field = col
                break
        
        # å¦‚æœæ²¡æœ‰DATAè®¾å¤‡çš„r_awaitï¼ŒæŸ¥æ‰¾DATAè®¾å¤‡çš„avg_await
        if not ebs_latency_field:
            for col in df.columns:
                if col.startswith('data_') and col.endswith('_avg_await'):
                    ebs_latency_field = col
                    break
        
        # å¦‚æœDATAè®¾å¤‡éƒ½æ²¡æœ‰ï¼ŒæŸ¥æ‰¾ACCOUNTSè®¾å¤‡çš„å»¶è¿Ÿå­—æ®µï¼ˆå¯é€‰ï¼‰
        if not ebs_latency_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_r_await'):
                    ebs_latency_field = col
                    break
        
        if not ebs_latency_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_avg_await'):
                    ebs_latency_field = col
                    break
        
        # åŠ¨æ€æŸ¥æ‰¾å…¶ä»–é‡è¦EBSæŒ‡æ ‡ï¼ˆä¼˜å…ˆDATAè®¾å¤‡ï¼‰
        ebs_iops_field = None
        # é¦–å…ˆæŸ¥æ‰¾DATAè®¾å¤‡å­—æ®µ
        for col in df.columns:
            if col.startswith('data_') and col.endswith('_total_iops'):
                ebs_iops_field = col
                break
        # å¦‚æœæ²¡æœ‰DATAè®¾å¤‡å­—æ®µï¼ŒæŸ¥æ‰¾ACCOUNTSè®¾å¤‡å­—æ®µï¼ˆå¯é€‰ï¼‰
        if not ebs_iops_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_total_iops'):
                    ebs_iops_field = col
                    break
        
        ebs_throughput_field = None
        # é¦–å…ˆæŸ¥æ‰¾DATAè®¾å¤‡å­—æ®µ
        for col in df.columns:
            if col.startswith('data_') and col.endswith('_throughput_mibs'):
                ebs_throughput_field = col
                break
        # å¦‚æœæ²¡æœ‰DATAè®¾å¤‡å­—æ®µï¼ŒæŸ¥æ‰¾ACCOUNTSè®¾å¤‡å­—æ®µï¼ˆå¯é€‰ï¼‰
        if not ebs_throughput_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_throughput_mibs'):
                    ebs_throughput_field = col
                    break
        
        ebs_queue_field = None
        # é¦–å…ˆæŸ¥æ‰¾DATAè®¾å¤‡å­—æ®µ
        for col in df.columns:
            if col.startswith('data_') and col.endswith('_aqu_sz'):
                ebs_queue_field = col
                break
        # å¦‚æœæ²¡æœ‰DATAè®¾å¤‡å­—æ®µï¼ŒæŸ¥æ‰¾ACCOUNTSè®¾å¤‡å­—æ®µï¼ˆå¯é€‰ï¼‰
        if not ebs_queue_field:
            for col in df.columns:
                if col.startswith('accounts_') and col.endswith('_aqu_sz'):
                    ebs_queue_field = col
                    break
        
        # æ·»åŠ æ‰¾åˆ°çš„å­—æ®µ
        if ebs_util_field:
            base_metrics.append(ebs_util_field)
            logger.info(f"âœ… åŠ¨æ€å‘ç°EBSåˆ©ç”¨ç‡å­—æ®µ: {ebs_util_field}")
        
        if ebs_latency_field:
            base_metrics.append(ebs_latency_field)
            logger.info(f"âœ… åŠ¨æ€å‘ç°EBSå»¶è¿Ÿå­—æ®µ: {ebs_latency_field}")
        
        if ebs_iops_field:
            base_metrics.append(ebs_iops_field)
            logger.info(f"âœ… åŠ¨æ€å‘ç°EBS IOPSå­—æ®µ: {ebs_iops_field}")
        
        if ebs_throughput_field:
            base_metrics.append(ebs_throughput_field)
            logger.info(f"âœ… åŠ¨æ€å‘ç°EBSååé‡å­—æ®µ: {ebs_throughput_field}")
        
        if ebs_queue_field:
            base_metrics.append(ebs_queue_field)
            logger.info(f"âœ… åŠ¨æ€å‘ç°EBSé˜Ÿåˆ—æ·±åº¦å­—æ®µ: {ebs_queue_field}")
        
        if not any([ebs_util_field, ebs_latency_field, ebs_iops_field]):
            logger.warning("âš ï¸ æœªå‘ç°EBSç›¸å…³å­—æ®µï¼Œå¯èƒ½å½±å“ç“¶é¢ˆåˆ†æå‡†ç¡®æ€§")
        
        logger.info(f"ğŸ“Š åŠ¨æ€æŒ‡æ ‡å­—æ®µæ€»æ•°: {len(base_metrics)}")
        return base_metrics
    


    def analyze_performance_cliff(self, df: pd.DataFrame, max_qps: int, bottleneck_qps: int) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ‚¬å´– - è¯†åˆ«æ€§èƒ½æ€¥å‰§ä¸‹é™çš„ç‚¹"""
        try:
            cliff_analysis = {
                'max_qps': max_qps,
                'bottleneck_qps': bottleneck_qps,
                'performance_drop_percent': 0.0,  # ä½¿ç”¨floatç±»å‹ä¿æŒä¸€è‡´æ€§
                'cliff_detected': False,
                'cliff_factors': [],
                'recommendations': []
            }
            
            if max_qps > 0 and bottleneck_qps > 0:
                # è®¡ç®—æ€§èƒ½ä¸‹é™ç™¾åˆ†æ¯”
                drop_percent = ((bottleneck_qps - max_qps) / max_qps) * 100
                cliff_analysis['performance_drop_percent'] = drop_percent
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºæ€§èƒ½æ‚¬å´–ï¼ˆä¸‹é™è¶…è¿‡20%ï¼‰
                if abs(drop_percent) > 20:
                    cliff_analysis['cliff_detected'] = True
                    
                    # åˆ†ææ‚¬å´–å› å­
                    cliff_factors = self._identify_cliff_factors(df, max_qps, bottleneck_qps)
                    cliff_analysis['cliff_factors'] = cliff_factors
                    
                    # ç”Ÿæˆå»ºè®®
                    recommendations = self._generate_cliff_recommendations(cliff_factors, drop_percent)
                    cliff_analysis['recommendations'] = recommendations
                    
                    logger.info(f"ğŸš¨ æ£€æµ‹åˆ°æ€§èƒ½æ‚¬å´–: {drop_percent:.1f}% æ€§èƒ½ä¸‹é™")
                else:
                    logger.info(f"ğŸ“Š æ€§èƒ½å˜åŒ–: {drop_percent:.1f}% (æœªè¾¾åˆ°æ‚¬å´–é˜ˆå€¼)")
            
            return cliff_analysis
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æ‚¬å´–åˆ†æå¤±è´¥: {e}")
            return {}

    def _identify_cliff_factors(self, df: pd.DataFrame, max_qps: int, bottleneck_qps: int) -> list:
        """è¯†åˆ«å¯¼è‡´æ€§èƒ½æ‚¬å´–çš„å› å­"""
        cliff_factors = []
        
        try:
            # å¯»æ‰¾QPSåˆ—
            qps_column = None
            for col in ['current_qps', 'qps', 'requests_per_second']:
                if col in df.columns:
                    qps_column = col
                    break
            
            if not qps_column:
                return cliff_factors
            
            # æ‰¾åˆ°æœ€å¤§QPSå’Œç“¶é¢ˆQPSå¯¹åº”çš„æ•°æ®ç‚¹
            max_qps_data = df[df[qps_column] <= max_qps].tail(1)
            bottleneck_qps_data = df[df[qps_column] >= bottleneck_qps].head(1)
            
            if len(max_qps_data) == 0 or len(bottleneck_qps_data) == 0:
                return cliff_factors
            
            # æ¯”è¾ƒå…³é”®æŒ‡æ ‡çš„å˜åŒ– - ä½¿ç”¨åŠ¨æ€å­—æ®µæŸ¥æ‰¾æ›¿ä»£ç¡¬ç¼–ç 
            key_metrics = self._get_dynamic_key_metrics(df)
            
            for metric in key_metrics:
                if metric in df.columns:
                    try:
                        max_value = max_qps_data[metric].iloc[0]
                        bottleneck_value = bottleneck_qps_data[metric].iloc[0]
                        
                        if pd.notna(max_value) and pd.notna(bottleneck_value) and max_value != 0:
                            change_percent = ((bottleneck_value - max_value) / max_value) * 100
                            
                            # å¦‚æœå˜åŒ–è¶…è¿‡10%ï¼Œè®¤ä¸ºæ˜¯æ‚¬å´–å› å­
                            if abs(change_percent) > 10:
                                cliff_factors.append({
                                    'metric': metric,
                                    'max_qps_value': float(max_value),
                                    'bottleneck_value': float(bottleneck_value),
                                    'change_percent': float(change_percent),
                                    'impact': 'high' if abs(change_percent) > 50 else 'medium'
                                })
                    except Exception as e:
                        logger.warning(f"âš ï¸ åˆ†æ{metric}æ‚¬å´–å› å­å¤±è´¥: {e}")
            
            # æŒ‰å½±å“ç¨‹åº¦æ’åº
            cliff_factors.sort(key=lambda x: abs(x['change_percent']), reverse=True)
            
        except Exception as e:
            logger.error(f"âŒ æ‚¬å´–å› å­è¯†åˆ«å¤±è´¥: {e}")
        
        return cliff_factors

    def _generate_cliff_recommendations(self, cliff_factors: list, drop_percent: float) -> list:
        """åŸºäºæ‚¬å´–å› å­ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        try:
            # åŸºäºæ€§èƒ½ä¸‹é™ç¨‹åº¦çš„é€šç”¨å»ºè®®
            if abs(drop_percent) > 50:
                recommendations.append("ä¸¥é‡æ€§èƒ½æ‚¬å´–ï¼šå»ºè®®ç«‹å³åœæ­¢æµ‹è¯•å¹¶æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
                recommendations.append("è€ƒè™‘é™ä½æµ‹è¯•å¼ºåº¦æˆ–ä¼˜åŒ–ç³»ç»Ÿé…ç½®")
            elif abs(drop_percent) > 30:
                recommendations.append("æ˜¾è‘—æ€§èƒ½ä¸‹é™ï¼šå»ºè®®åˆ†æç³»ç»Ÿç“¶é¢ˆå¹¶è¿›è¡Œä¼˜åŒ–")
            
            # åŸºäºå…·ä½“æ‚¬å´–å› å­çš„å»ºè®®
            for factor in cliff_factors[:3]:  # åªå¤„ç†å‰3ä¸ªæœ€é‡è¦çš„å› å­
                metric = factor['metric']
                change = factor['change_percent']
                
                if 'cpu' in metric.lower():
                    if change > 0:
                        recommendations.append(f"CPUä½¿ç”¨ç‡æ€¥å‰§ä¸Šå‡{change:.1f}%ï¼šè€ƒè™‘å‡çº§CPUæˆ–ä¼˜åŒ–åº”ç”¨")
                    else:
                        recommendations.append(f"CPUä½¿ç”¨ç‡å¼‚å¸¸ä¸‹é™{abs(change):.1f}%ï¼šæ£€æŸ¥CPUè°ƒåº¦é—®é¢˜")
                
                elif 'mem' in metric.lower():
                    if change > 0:
                        recommendations.append(f"å†…å­˜ä½¿ç”¨ç‡æ€¥å‰§ä¸Šå‡{change:.1f}%ï¼šè€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
                    else:
                        recommendations.append(f"å†…å­˜ä½¿ç”¨ç‡å¼‚å¸¸ä¸‹é™{abs(change):.1f}%ï¼šæ£€æŸ¥å†…å­˜ç®¡ç†é—®é¢˜")
                
                elif 'util' in metric.lower():
                    if change > 0:
                        recommendations.append(f"ç£ç›˜åˆ©ç”¨ç‡æ€¥å‰§ä¸Šå‡{change:.1f}%ï¼šè€ƒè™‘å‡çº§å­˜å‚¨æˆ–ä¼˜åŒ–I/O")
                
                elif 'await' in metric.lower():
                    if change > 0:
                        recommendations.append(f"ç£ç›˜å»¶è¿Ÿæ€¥å‰§ä¸Šå‡{change:.1f}%ï¼šæ£€æŸ¥å­˜å‚¨æ€§èƒ½ç“¶é¢ˆ")
            
            # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„æ‚¬å´–å› å­ï¼Œæä¾›é€šç”¨å»ºè®®
            if not cliff_factors:
                recommendations.append("æœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½æ‚¬å´–å› å­ï¼Œå»ºè®®è¿›è¡Œå…¨é¢çš„ç³»ç»Ÿæ€§èƒ½åˆ†æ")
                recommendations.append("æ£€æŸ¥ç½‘ç»œã€åº”ç”¨é€»è¾‘å’Œç³»ç»Ÿé…ç½®")
        
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ‚¬å´–å»ºè®®å¤±è´¥: {e}")
        
        return recommendations

    def generate_cliff_analysis_chart(self, df: pd.DataFrame, cliff_analysis: Dict[str, Any]) -> Optional[plt.Figure]:
        """ç”Ÿæˆæ€§èƒ½æ‚¬å´–åˆ†æå›¾è¡¨"""
        try:
            if not cliff_analysis or not cliff_analysis.get('cliff_detected'):
                return None
            
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            # Using English title directly
            fig.suptitle('ğŸ“‰ Performance Cliff Analysis', fontsize=16, fontweight='bold', color='red')
            
            # 1. QPSæ€§èƒ½æ›²çº¿
            qps_column = None
            for col in ['current_qps', 'qps', 'requests_per_second']:
                if col in df.columns:
                    qps_column = col
                    break
            
            if qps_column and len(df) > 0:
                axes[0, 0].plot(df.index, df[qps_column], 'b-', alpha=0.7, linewidth=2)
                
                # æ ‡è®°æœ€å¤§QPSå’Œç“¶é¢ˆQPS
                max_qps = cliff_analysis['max_qps']
                bottleneck_qps = cliff_analysis['bottleneck_qps']
                
                axes[0, 0].axhline(y=max_qps, color='green', linestyle='--', linewidth=2,
                                 label=f'Max QPS: {max_qps}')
                axes[0, 0].axhline(y=bottleneck_qps, color='red', linestyle='--', linewidth=2,
                                 label=f'Bottleneck QPS: {bottleneck_qps}')
                
                # å¡«å……æ‚¬å´–åŒºåŸŸ
                axes[0, 0].fill_between(df.index, max_qps, bottleneck_qps, 
                                      alpha=0.3, color='red', label='Performance Cliff')
                
                axes[0, 0].set_title('QPS Performance Cliff')
                axes[0, 0].set_xlabel('Time')
                axes[0, 0].set_ylabel('QPS')
                axes[0, 0].legend()
                axes[0, 0].grid(True, alpha=0.3)
            
            # 2. æ‚¬å´–å› å­å½±å“
            cliff_factors = cliff_analysis.get('cliff_factors', [])
            if cliff_factors:
                factor_names = [f['metric'] for f in cliff_factors[:5]]
                factor_changes = [abs(f['change_percent']) for f in cliff_factors[:5]]
                
                colors = ['red' if abs(f['change_percent']) > 50 else 'orange' 
                         for f in cliff_factors[:5]]
                
                axes[0, 1].barh(factor_names, factor_changes, color=colors, alpha=0.7)
                axes[0, 1].set_title('Cliff Factor Impact (%)')
                axes[0, 1].set_xlabel('Change Percentage')
                axes[0, 1].grid(True, alpha=0.3)
            
            # 3. æ€§èƒ½ä¸‹é™å¯è§†åŒ–
            drop_percent = cliff_analysis.get('performance_drop_percent', 0)
            categories = ['Before Cliff', 'After Cliff']
            values = [100, 100 + drop_percent]  # ç›¸å¯¹æ€§èƒ½
            colors = ['green', 'red']
            
            bars = axes[1, 0].bar(categories, values, color=colors, alpha=0.7)
            axes[1, 0].set_title(f'Performance Drop: {abs(drop_percent):.1f}%')
            axes[1, 0].set_ylabel('Relative Performance (%)')
            axes[1, 0].axhline(y=100, color='black', linestyle='-', alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                               f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')
            
            # 4. å»ºè®®æ‘˜è¦
            recommendations = cliff_analysis.get('recommendations', [])
            if recommendations:
                axes[1, 1].text(0.05, 0.95, 'Optimization Recommendations:', 
                               transform=axes[1, 1].transAxes, fontsize=12, fontweight='bold',
                               verticalalignment='top')
                
                for i, rec in enumerate(recommendations[:5]):
                    axes[1, 1].text(0.05, 0.85 - i*0.15, f"â€¢ {rec}", 
                                   transform=axes[1, 1].transAxes, fontsize=10,
                                   verticalalignment='top', wrap=True)
                
                axes[1, 1].set_xlim(0, 1)
                axes[1, 1].set_ylim(0, 1)
                axes[1, 1].axis('off')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_path = os.path.join(self.reports_dir, 'performance_cliff_analysis.png')
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            logger.info(f"ğŸ“Š æ€§èƒ½æ‚¬å´–åˆ†æå›¾è¡¨å·²ä¿å­˜: {chart_path}")
            
            return fig
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æ‚¬å´–å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def get_latest_csv(self) -> Optional[str]:
        """æ”¹è¿›çš„CSVæ–‡ä»¶æŸ¥æ‰¾é€»è¾‘ï¼Œæ”¯æŒå¤šç§è·¯å¾„æ¨¡å¼"""
        csv_patterns = [
            f"{self.output_dir}/logs/performance_latest.csv",
            f"{self.output_dir}/logs/performance_*.csv",
            f"{self.output_dir}/current/logs/performance_latest.csv",
            f"{self.output_dir}/current/logs/performance_*.csv"
        ]
        
        for pattern in csv_patterns:
            csv_files = glob.glob(pattern)
            if csv_files:
                return max(csv_files, key=os.path.getctime)
        return None

    def load_and_clean_data(self) -> pd.DataFrame:
        """åŠ è½½å’Œæ¸…ç†ç›‘æ§æ•°æ®ï¼Œæ”¹è¿›é”™è¯¯å¤„ç†"""
        try:
            if not self.csv_file:
                print("âš ï¸  No CSV monitoring file found, proceeding with log analysis only")
                return pd.DataFrame()

            print(f"ğŸ“Š Loading QPS monitoring data from: {os.path.basename(self.csv_file)}")
            
            # ç›´æ¥ä½¿ç”¨pandasè¯»å–CSV - å­—æ®µæ˜ å°„å™¨å·²ç§»é™¤
            df = pd.read_csv(self.csv_file)

            print(f"ğŸ“‹ Raw data shape: {df.shape}")

            # æ£€æŸ¥æ˜¯å¦æœ‰QPSç›¸å…³æ•°æ®
            qps_columns = ['current_qps', 'qps', 'target_qps']
            qps_column = None
            for col in qps_columns:
                if col in df.columns:
                    qps_column = col
                    break
            
            if qps_column is None:
                print("âš ï¸  No QPS data found in CSV, this appears to be system monitoring data only")
                print("ğŸ“Š Available columns:", ', '.join(df.columns[:10]))
                # ä»ç„¶è¿”å›æ•°æ®ï¼Œç”¨äºç³»ç»Ÿæ€§èƒ½åˆ†æ
                df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
                return df

            # å¤„ç†current_qpsåˆ—
            df['current_qps'] = df[qps_column].astype(str)
            numeric_mask = df['current_qps'].str.isdigit()
            numeric_df = df[numeric_mask].copy()

            if len(numeric_df) == 0:
                print("âš ï¸  No numeric QPS data found")
                return df

            # æ•°æ®ç±»å‹è½¬æ¢
            numeric_df['current_qps'] = pd.to_numeric(numeric_df['current_qps'])
            numeric_df['timestamp'] = pd.to_datetime(numeric_df['timestamp'], errors='coerce')

            # æ¸…ç†æ•°å€¼åˆ— - ä½¿ç”¨æ˜ å°„åçš„æ ‡å‡†å­—æ®µå
            numeric_cols = ['cpu_usage', 'mem_usage', 'rpc_latency_ms', 'elapsed_time', 'remaining_time']
            for col in numeric_cols:
                if col in numeric_df.columns:
                    numeric_df[col] = pd.to_numeric(numeric_df[col], errors='coerce')

            print(f"ğŸ“Š Processed {len(numeric_df)} QPS monitoring data points")
            return numeric_df
            
        except Exception as e:
            logger.error(f"âŒ Data loading and cleaning failed: {e}")
            return pd.DataFrame()

    def analyze_performance_metrics(self, df: pd.DataFrame) -> Tuple[Optional[pd.DataFrame], int]:
        """åˆ†æå…³é”®æ€§èƒ½æŒ‡æ ‡"""
        print("\nğŸ¯ QPS Performance Metrics Analysis")
        print("=" * 50)

        if 'current_qps' not in df.columns or len(df) == 0:
            print("âŒ No valid QPS data for analysis")
            return None, 0

        max_qps = df['current_qps'].max()
        qps_range = sorted(df['current_qps'].unique())

        print(f"Maximum QPS tested: {max_qps:,}")
        print(f"QPS range: {min(qps_range):,} - {max_qps:,}")
        print(f"Number of QPS levels: {len(qps_range)}")

        # æŒ‰QPSåˆ†ç»„ç»Ÿè®¡
        qps_stats = df.groupby('current_qps').agg({
            'cpu_usage': ['mean', 'max'],
            'mem_usage': ['mean', 'max'],
            'rpc_latency_ms': ['mean', 'max']
        }).round(2)

        print("\nQPS Performance Statistics:")
        print(qps_stats.to_string())

        return qps_stats, max_qps

    def identify_bottlenecks(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        print("\nğŸ” QPS Performance Bottleneck Analysis")
        print("=" * 50)

        if len(df) == 0:
            print("âŒ No data for bottleneck analysis")
            return {}

        bottlenecks = {}

        # CPUç“¶é¢ˆ
        if 'cpu_usage' in df.columns and 'current_qps' in df.columns:
            cpu_bottleneck = df[df['cpu_usage'] > self.cpu_threshold]['current_qps'].min()
            if pd.notna(cpu_bottleneck):
                bottlenecks['CPU'] = cpu_bottleneck

        # å†…å­˜ç“¶é¢ˆ
        mem_bottleneck = df[df['mem_usage'] > self.memory_threshold]['current_qps'].min()
        if pd.notna(mem_bottleneck):
            bottlenecks['Memory'] = mem_bottleneck

        # RPCå»¶è¿Ÿç“¶é¢ˆ
        rpc_bottleneck = df[df['rpc_latency_ms'] > self.rpc_threshold]['current_qps'].min()
        if pd.notna(rpc_bottleneck):
            bottlenecks['RPC_Latency'] = rpc_bottleneck

        if bottlenecks:
            print("System bottlenecks detected:")
            for bottleneck_type, qps in bottlenecks.items():
                print(f"  {bottleneck_type}: First occurs at QPS {qps:,}")
        else:
            print("âœ… No critical system bottlenecks detected in tested range")

        return bottlenecks

    def generate_performance_charts(self, df: pd.DataFrame) -> Optional[plt.Figure]:
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
        print("\nğŸ“ˆ Generating performance charts...")

        if len(df) == 0:
            print("âŒ No QPS data for chart generation")
            return None

        plt.style.use('default')
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        # Using English title directly
        fig.suptitle('Solana QPS Performance Analysis Dashboard', fontsize=16, fontweight='bold')

        # 1. CPUä½¿ç”¨ç‡ vs QPS
        if len(df) > 0:
            axes[0, 0].plot(df['current_qps'], df['cpu_usage'], 'bo-', alpha=0.7, markersize=4)
            axes[0, 0].axhline(y=85, color='red', linestyle='--', alpha=0.8, label='Warning (85%)')
            axes[0, 0].set_title('CPU Usage vs QPS')
            axes[0, 0].set_xlabel('QPS')
            axes[0, 0].set_ylabel('CPU %')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)

        # 2. å†…å­˜ä½¿ç”¨ç‡ vs QPS
        if len(df) > 0:
            axes[0, 1].plot(df['current_qps'], df['mem_usage'], 'go-', alpha=0.7, markersize=4)
            axes[0, 1].axhline(y=90, color='red', linestyle='--', alpha=0.8, label='Warning (90%)')
            axes[0, 1].set_title('Memory Usage vs QPS')
            axes[0, 1].set_xlabel('QPS')
            axes[0, 1].set_ylabel('Memory %')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

        # 3. RPCå»¶è¿Ÿ vs QPS
        if len(df) > 0:
            axes[1, 0].plot(df['current_qps'], df['rpc_latency_ms'], 'ro-', alpha=0.7, markersize=4)
            axes[1, 0].axhline(y=1000, color='orange', linestyle='--', alpha=0.8, label='High Latency (1s)')
            axes[1, 0].set_title('RPC Latency vs QPS')
            axes[1, 0].set_xlabel('QPS')
            axes[1, 0].set_ylabel('Latency (ms)')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

        # 4. RPCå»¶è¿Ÿåˆ†å¸ƒ
        if len(df) > 0 and 'rpc_latency_ms' in df.columns:
            axes[1, 1].hist(df['rpc_latency_ms'], bins=30, alpha=0.7, color='purple')
            if 'rpc_latency_ms' in df.columns:
                mean_latency = df['rpc_latency_ms'].mean()
                p95_latency = df['rpc_latency_ms'].quantile(0.95)
                axes[1, 1].axvline(mean_latency, color='red', linestyle='--',
                                   label=f'Mean: {mean_latency:.1f}ms')
                axes[1, 1].axvline(p95_latency, color='orange', linestyle='--',
                                   label=f'P95: {p95_latency:.1f}ms')
            axes[1, 1].set_title('RPC Latency Distribution')
            axes[1, 1].set_xlabel('Latency (ms)')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_file = os.path.join(self.output_dir, 'reports', 'qps_performance_analysis.png')
        os.makedirs(os.path.dirname(chart_file), exist_ok=True)
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        print(f"âœ… Performance charts saved: {chart_file}")

        return fig

    def analyze_vegeta_reports(self) -> Optional[pd.DataFrame]:
        """åˆ†æVegetaæµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“‹ Vegeta Reports Analysis")
        print("=" * 50)

        reports = glob.glob(f"{self.output_dir}/reports/*.txt")
        if not reports:
            print("No Vegeta reports found")
            return None

        report_data = []
        for report_file in sorted(reports):
            try:
                qps = int(os.path.basename(report_file).split('_')[1])
                with open(report_file, 'r') as f:
                    content = f.read()

                success_rate = 0
                avg_latency = 'N/A'
                p99_latency = 'N/A'

                for line in content.split('\n'):
                    if 'Success' in line and '[ratio]' in line:
                        success_rate = float(line.split()[-1].replace('%', ''))
                    elif 'Latencies' in line and '[min, mean,' in line:
                        parts = line.split()
                        if len(parts) >= 8:
                            avg_latency = parts[6].replace(',', '')
                            p99_latency = parts[8].replace(',', '')

                report_data.append({
                    'QPS': qps,
                    'Success_Rate': success_rate,
                    'Avg_Latency': avg_latency,
                    'P99_Latency': p99_latency
                })
            except Exception as e:
                print(f"Warning: Could not parse {report_file}: {e}")

        if report_data:
            vegeta_df = pd.DataFrame(report_data)
            print(vegeta_df.to_string(index=False))
            return vegeta_df

        return None

    def _evaluate_performance_by_bottleneck_analysis(self, benchmark_mode: str, max_qps: int, 
                                                   bottlenecks: Dict[str, Any], avg_cpu: float, 
                                                   avg_mem: float, avg_rpc: float) -> Dict[str, Any]:
        """
        åŸºäºç“¶é¢ˆåˆ†æçš„ç§‘å­¦æ€§èƒ½è¯„ä¼°
        æ›¿ä»£ç¡¬ç¼–ç çš„60000/40000/20000é€»è¾‘
        """
        
        # åªæœ‰æ·±åº¦åŸºå‡†æµ‹è¯•æ¨¡å¼æ‰èƒ½è¿›è¡Œå‡†ç¡®çš„æ€§èƒ½ç­‰çº§è¯„ä¼°
        if benchmark_mode != "intensive":
            return {
                'performance_level': 'æ— æ³•è¯„ä¼°',
                'performance_grade': 'N/A',
                'evaluation_reason': f'{benchmark_mode}åŸºå‡†æµ‹è¯•æ¨¡å¼æ— æ³•å‡†ç¡®è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ç­‰çº§ï¼Œéœ€è¦intensiveæ¨¡å¼è¿›è¡Œæ·±åº¦åˆ†æ',
                'evaluation_basis': 'insufficient_benchmark_depth',
                'max_sustainable_qps': max_qps,
                'recommendations': [
                    f'å½“å‰{benchmark_mode}åŸºå‡†æµ‹è¯•ä»…ç”¨äºå¿«é€ŸéªŒè¯',
                    'å¦‚éœ€å‡†ç¡®çš„æ€§èƒ½ç­‰çº§è¯„ä¼°ï¼Œè¯·ä½¿ç”¨intensiveåŸºå‡†æµ‹è¯•æ¨¡å¼',
                    'æ·±åº¦åŸºå‡†æµ‹è¯•å°†è§¦å‘ç³»ç»Ÿç“¶é¢ˆä»¥è·å¾—å‡†ç¡®çš„æ€§èƒ½è¯„ä¼°'
                ]
            }
        
        # æ·±åº¦åŸºå‡†æµ‹è¯•æ¨¡å¼ä¸‹çš„ç“¶é¢ˆåˆ†æè¯„ä¼°
        bottleneck_types = bottlenecks.get('detected_bottlenecks', [])
        bottleneck_count = len(bottleneck_types)
        
        # è®¡ç®—ç“¶é¢ˆä¸¥é‡ç¨‹åº¦è¯„åˆ†
        bottleneck_score = self._calculate_bottleneck_severity_score(
            bottleneck_types, avg_cpu, avg_mem, avg_rpc
        )
        
        # åŸºäºç“¶é¢ˆè¯„åˆ†çš„ç§‘å­¦ç­‰çº§è¯„ä¼°
        if bottleneck_score < 0.2:
            # ä½ç“¶é¢ˆè¯„åˆ† = ä¼˜ç§€æ€§èƒ½
            level = "ä¼˜ç§€"
            grade = "A (Excellent)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹æœªå‡ºç°æ˜æ˜¾ç“¶é¢ˆï¼Œæ€§èƒ½è¡¨ç°ä¼˜ç§€"
            
        elif bottleneck_score < 0.4:
            # ä¸­ç­‰ç“¶é¢ˆè¯„åˆ† = è‰¯å¥½æ€§èƒ½
            level = "è‰¯å¥½"
            grade = "B (Good)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹å‡ºç°è½»å¾®ç“¶é¢ˆ: {', '.join(bottleneck_types)}"
            
        elif bottleneck_score < 0.7:
            # è¾ƒé«˜ç“¶é¢ˆè¯„åˆ† = ä¸€èˆ¬æ€§èƒ½
            level = "ä¸€èˆ¬"
            grade = "C (Acceptable)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹å‡ºç°æ˜æ˜¾ç“¶é¢ˆ: {', '.join(bottleneck_types)}"
            
        else:
            # é«˜ç“¶é¢ˆè¯„åˆ† = éœ€è¦ä¼˜åŒ–
            level = "éœ€è¦ä¼˜åŒ–"
            grade = "D (Needs Improvement)"
            reason = f"ç³»ç»Ÿåœ¨{max_qps} QPSä¸‹å‡ºç°ä¸¥é‡ç“¶é¢ˆ: {', '.join(bottleneck_types)}"
        
        return {
            'performance_level': level,
            'performance_grade': grade,
            'evaluation_reason': reason,
            'evaluation_basis': 'intensive_bottleneck_analysis',
            'max_sustainable_qps': max_qps,
            'bottleneck_score': bottleneck_score,
            'bottleneck_types': bottleneck_types,
            'bottleneck_count': bottleneck_count,
            'recommendations': self._generate_bottleneck_based_recommendations(
                bottleneck_types, bottleneck_score, max_qps
            )
        }
    
    def _calculate_bottleneck_severity_score(self, bottleneck_types: list, 
                                           avg_cpu: float, avg_mem: float, avg_rpc: float) -> float:
        """è®¡ç®—ç“¶é¢ˆä¸¥é‡ç¨‹åº¦è¯„åˆ†"""
        
        # ç“¶é¢ˆç±»å‹æƒé‡
        bottleneck_weights = {
            'CPU': 0.2,
            'Memory': 0.25,
            'EBS': 0.3,
            'Network': 0.15,
            'RPC': 0.1
        }
        
        total_score = 0.0
        
        # åŸºäºæ£€æµ‹åˆ°çš„ç“¶é¢ˆç±»å‹è®¡ç®—è¯„åˆ†
        for bottleneck_type in bottleneck_types:
            weight = bottleneck_weights.get(bottleneck_type, 0.1)
            
            # æ ¹æ®å…·ä½“æŒ‡æ ‡è°ƒæ•´ä¸¥é‡ç¨‹åº¦
            severity_multiplier = 1.0
            if bottleneck_type == 'CPU' and avg_cpu > (self.cpu_threshold + 5):
                severity_multiplier = 1.5
            elif bottleneck_type == 'Memory' and avg_mem > (self.memory_threshold + 5):
                severity_multiplier = 1.5
            elif bottleneck_type == 'RPC' and avg_rpc > (self.rpc_threshold * 2):
                severity_multiplier = 1.5
            
            total_score += weight * severity_multiplier
        
        # å½’ä¸€åŒ–è¯„åˆ†åˆ°0-1èŒƒå›´
        return min(total_score, 1.0)
    
    def _generate_capacity_assessment(self, performance_evaluation: Dict[str, Any], max_qps: int) -> str:
        """åŸºäºæ€§èƒ½è¯„ä¼°ç”Ÿæˆå®¹é‡è¯„ä¼°"""
        performance_level = performance_evaluation.get('performance_level', 'æœªçŸ¥')
        bottleneck_score = performance_evaluation.get('bottleneck_score', 0)
        
        if performance_level == "ä¼˜ç§€":
            return f"å½“å‰é…ç½®å¯ç¨³å®šå¤„ç†é«˜è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œæ— æ˜æ˜¾ç“¶é¢ˆ)"
        elif performance_level == "è‰¯å¥½":
            return f"å½“å‰é…ç½®å¯å¤„ç†ä¸­é«˜è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œè½»å¾®ç“¶é¢ˆ)"
        elif performance_level == "ä¸€èˆ¬":
            return f"å½“å‰é…ç½®é€‚åˆä¸­ç­‰è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œå­˜åœ¨ç“¶é¢ˆ)"
        elif performance_level == "éœ€è¦ä¼˜åŒ–":
            return f"å½“å‰é…ç½®éœ€è¦ä¼˜åŒ–ä»¥å¤„ç†é«˜è´Ÿè½½ (å·²æµ‹è¯•è‡³ {max_qps:,} QPSï¼Œä¸¥é‡ç“¶é¢ˆ)"
        else:
            return f"éœ€è¦intensiveåŸºå‡†æµ‹è¯•æ¨¡å¼è¿›è¡Œå‡†ç¡®çš„å®¹é‡è¯„ä¼°"

    def _generate_bottleneck_based_recommendations(self, bottleneck_types: list, 
                                                 bottleneck_score: float, max_qps: int) -> list:
        """åŸºäºç“¶é¢ˆåˆ†æç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if bottleneck_score < 0.2:
            recommendations.extend([
                f"ğŸ‰ ç³»ç»Ÿæ€§èƒ½ä¼˜ç§€ï¼Œå½“å‰é…ç½®å¯ç¨³å®šæ”¯æŒ {max_qps} QPS",
                "ğŸ’¡ å¯è€ƒè™‘è¿›ä¸€æ­¥æå‡QPSç›®æ ‡æˆ–ä¼˜åŒ–æˆæœ¬æ•ˆç‡",
                "ğŸ“Š å»ºè®®å®šæœŸç›‘æ§ä»¥ç»´æŒå½“å‰æ€§èƒ½æ°´å¹³"
            ])
        else:
            # åŸºäºå…·ä½“ç“¶é¢ˆç±»å‹çš„é’ˆå¯¹æ€§å»ºè®®
            if 'CPU' in bottleneck_types:
                recommendations.append("ğŸ”§ CPUç“¶é¢ˆï¼šè€ƒè™‘å‡çº§CPUæˆ–ä¼˜åŒ–è®¡ç®—å¯†é›†å‹è¿›ç¨‹")
            if 'Memory' in bottleneck_types:
                recommendations.append("ğŸ”§ å†…å­˜ç“¶é¢ˆï¼šè€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
            if 'EBS' in bottleneck_types:
                recommendations.append("ğŸ”§ å­˜å‚¨ç“¶é¢ˆï¼šè€ƒè™‘å‡çº§EBSç±»å‹æˆ–ä¼˜åŒ–I/Oæ¨¡å¼")
            if 'Network' in bottleneck_types:
                recommendations.append("ğŸ”§ ç½‘ç»œç“¶é¢ˆï¼šè€ƒè™‘å‡çº§ç½‘ç»œå¸¦å®½æˆ–ä¼˜åŒ–ç½‘ç»œé…ç½®")
            if 'RPC' in bottleneck_types:
                recommendations.append("ğŸ”§ RPCç“¶é¢ˆï¼šè€ƒè™‘ä¼˜åŒ–RPCé…ç½®æˆ–å¢åŠ RPCè¿æ¥æ± ")
        
        return recommendations

    def generate_performance_report(self, df: pd.DataFrame, max_qps: int, 
                                  bottlenecks: Dict[str, Any], benchmark_mode: str = "standard") -> str:
        """ç”ŸæˆåŸºäºç“¶é¢ˆåˆ†æçš„æ€§èƒ½æŠ¥å‘Š"""
        print("\nğŸ“„ Generating performance report...")

        # åŸºæœ¬æ€§èƒ½æŒ‡æ ‡
        avg_cpu = df['cpu_usage'].mean() if len(df) > 0 and 'cpu_usage' in df.columns else 0
        avg_mem = df['mem_usage'].mean() if len(df) > 0 and 'mem_usage' in df.columns else 0
        avg_rpc = df['rpc_latency_ms'].mean() if len(df) > 0 and 'rpc_latency_ms' in df.columns else 0

        # åŸºäºåŸºå‡†æµ‹è¯•æ¨¡å¼å’Œç“¶é¢ˆåˆ†æçš„æ€§èƒ½è¯„ä¼°
        performance_evaluation = self._evaluate_performance_by_bottleneck_analysis(
            benchmark_mode, max_qps, bottlenecks, avg_cpu, avg_mem, avg_rpc
        )

        report = f"""# Solana QPS Performance Analysis Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Executive Summary
- **Maximum QPS Achieved**: {max_qps:,}
- **Performance Grade**: {performance_evaluation['performance_grade']}
- **Performance Level**: {performance_evaluation['performance_level']}
- **Benchmark Mode**: {benchmark_mode}
- **Test Duration**: {len(df)} monitoring points

## Performance Evaluation
- **Evaluation Basis**: {performance_evaluation['evaluation_basis']}
- **Evaluation Reason**: {performance_evaluation['evaluation_reason']}

## System Performance Metrics
- **Average CPU Usage**: {avg_cpu:.1f}%
- **Average Memory Usage**: {avg_mem:.1f}%
- **Average RPC Latency**: {avg_rpc:.1f}ms
- **CPU Peak**: {(df['cpu_usage'].max() if len(df) > 0 and 'cpu_usage' in df.columns else 0):.1f}%
- **Memory Peak**: {(df['mem_usage'].max() if len(df) > 0 and 'mem_usage' in df.columns else 0):.1f}%
- **RPC Latency Peak**: {(df['rpc_latency_ms'].max() if len(df) > 0 and 'rpc_latency_ms' in df.columns else 0):.1f}ms

## Performance Bottlenecks Analysis
"""

        if performance_evaluation.get('bottleneck_types'):
            report += f"- **Bottleneck Score**: {performance_evaluation.get('bottleneck_score', 0):.3f}\n"
            report += f"- **Detected Bottlenecks**: {', '.join(performance_evaluation['bottleneck_types'])}\n"
            for bottleneck_type in performance_evaluation['bottleneck_types']:
                qps = bottlenecks.get(bottleneck_type, 'Unknown')
                report += f"  - **{bottleneck_type}**: First detected at {qps:,} QPS\n" if isinstance(qps, int) else f"  - **{bottleneck_type}**: {qps}\n"
        else:
            report += "- âœ… No critical bottlenecks detected in tested range\n"

        report += f"""
## Optimization Recommendations

### Based on Bottleneck Analysis
"""

        # ä½¿ç”¨åŸºäºç“¶é¢ˆåˆ†æçš„å»ºè®®
        for recommendation in performance_evaluation.get('recommendations', []):
            report += f"- {recommendation}\n"

        report += f"""
### Production Deployment Guidelines
- **Recommended Production QPS**: {int(max_qps * 0.8):,} (80% of maximum tested)
- **Monitoring Thresholds**: 
  - Alert if CPU usage > 85%
  - Alert if Memory usage > 90%
  - Alert if RPC latency > 1000ms sustained
- **Capacity Assessment**: {self._generate_capacity_assessment(performance_evaluation, max_qps)}

## Files Generated
- **Performance Charts**: `{self.output_dir}/reports/qps_performance_analysis.png`
- **Raw QPS Monitoring Data**: `{self.csv_file or 'N/A'}`
- **Vegeta Test Reports**: `{self.output_dir}/reports/`

---
*Report generated by Solana QPS Analyzer*
"""

        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.output_dir, 'reports', 'qps_performance_report.md')
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        with open(report_file, 'w') as f:
            f.write(report)

        print(f"âœ… Performance report saved: {report_file}")
        return report

    def run_qps_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„QPSåˆ†æ"""
        print("ğŸš€ Starting Solana QPS Performance Analysis")
        print("=" * 60)

        # åŠ è½½QPSç›‘æ§æ•°æ®
        df = self.load_and_clean_data()

        # æ‰§è¡ŒQPSæ€§èƒ½åˆ†æ
        qps_stats, max_qps = self.analyze_performance_metrics(df)
        bottlenecks = self.identify_bottlenecks(df)

        # ç”Ÿæˆå›¾è¡¨å’ŒæŠ¥å‘Š
        self.generate_performance_charts(df)
        vegeta_analysis = self.analyze_vegeta_reports()
        report = self.generate_performance_report(df, max_qps, bottlenecks, self.benchmark_mode)

        analysis_results = {
            'dataframe': df,
            'qps_stats': qps_stats,
            'max_qps': max_qps,
            'bottlenecks': bottlenecks,
            'vegeta_analysis': vegeta_analysis,
            'report': report
        }

        print("\nğŸ‰ QPS Analysis Completed Successfully!")
        print("Generated files:")
        print(f"  ğŸ“Š Charts: {self.output_dir}/reports/qps_performance_analysis.png")
        print(f"  ğŸ“„ Report: {self.output_dir}/reports/qps_performance_report.md")

        return analysis_results


def main():
    """ä¸»æ‰§è¡Œå‡½æ•° - æ”¯æŒç“¶é¢ˆæ¨¡å¼å’Œæ€§èƒ½æ‚¬å´–åˆ†æ"""
    parser = argparse.ArgumentParser(description='QPSåˆ†æå™¨ - æ”¯æŒç“¶é¢ˆæ¨¡å¼')
    parser.add_argument('csv_file', help='CSVæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--benchmark-mode', default='standard', choices=['quick', 'standard', 'intensive'], 
                       help='åŸºå‡†æµ‹è¯•æ¨¡å¼ (é»˜è®¤: standard)')
    parser.add_argument('--bottleneck-mode', action='store_true', help='å¯ç”¨ç“¶é¢ˆåˆ†ææ¨¡å¼')
    parser.add_argument('--cliff-analysis', action='store_true', help='å¯ç”¨æ€§èƒ½æ‚¬å´–åˆ†æ')
    parser.add_argument('--max-qps', type=int, help='æœ€å¤§æˆåŠŸQPS')
    parser.add_argument('--bottleneck-qps', type=int, help='ç“¶é¢ˆè§¦å‘QPS')
    parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    try:
        if not os.path.exists(args.csv_file):
            logger.error(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {args.csv_file}")
            return 1
        
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = NodeQPSAnalyzer(args.output_dir, args.benchmark_mode, args.bottleneck_mode)
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(args.csv_file)
        logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æ¡è®°å½•")
        
        # æ€§èƒ½æ‚¬å´–åˆ†æ
        if args.cliff_analysis and args.max_qps and args.bottleneck_qps:
            logger.info("ğŸ“‰ æ‰§è¡Œæ€§èƒ½æ‚¬å´–åˆ†æ")
            cliff_analysis = analyzer.analyze_performance_cliff(df, args.max_qps, args.bottleneck_qps)
            
            # ç”Ÿæˆæ‚¬å´–åˆ†æå›¾è¡¨
            cliff_chart = analyzer.generate_cliff_analysis_chart(df, cliff_analysis)
            
            # ä¿å­˜åˆ†æç»“æœ
            cliff_result_file = os.path.join(analyzer.reports_dir, 'performance_cliff_analysis.json')
            with open(cliff_result_file, 'w') as f:
                json.dump(cliff_analysis, f, indent=2, default=str)
            logger.info(f"ğŸ“Š æ€§èƒ½æ‚¬å´–åˆ†æç»“æœå·²ä¿å­˜: {cliff_result_file}")
        
        # æ‰§è¡Œæ ‡å‡†QPSåˆ†æ
        result = analyzer.run_qps_analysis()
        
        if result:
            logger.info("âœ… QPSåˆ†æå®Œæˆ")
            return 0
        else:
            logger.error("âŒ QPSåˆ†æå¤±è´¥")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ QPSåˆ†ææ‰§è¡Œå¤±è´¥: {e}")
        return 1

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    if len(sys.argv) == 1:
        print("ğŸ“‹ QPSåˆ†æå™¨ä½¿ç”¨ç¤ºä¾‹:")
        print("python qps_analyzer.py data.csv")
        print("python qps_analyzer.py data.csv --bottleneck-mode")
        print("python qps_analyzer.py data.csv --cliff-analysis --max-qps 5000 --bottleneck-qps 3000")
    else:
        sys.exit(main())
    print("analyzer = NodeQPSAnalyzer('/path/to/output/dir', 'standard', False)")
    print("results = analyzer.run_qps_analysis()")
    
    # æ¼”ç¤ºåŠŸèƒ½
    try:
        analyzer = NodeQPSAnalyzer(benchmark_mode="standard", bottleneck_mode=False)
        results = analyzer.run_qps_analysis()
        if results:
            print("ğŸ¯ QPSåˆ†æå™¨æ¼”ç¤ºå®Œæˆ")
            print(f"ğŸ“Š ç”Ÿæˆäº† {len(results)} ä¸ªåˆ†æç»„ä»¶")
            print(f"ğŸ“ˆ æœ€å¤§QPS: {results.get('max_qps', 'N/A')}")
        else:
            print("âš ï¸ åˆ†ææœªäº§ç”Ÿç»“æœ")
    except Exception as e:
        print(f"âš ï¸  æ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")
        print("è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºå¯èƒ½æ²¡æœ‰å®é™…çš„æµ‹è¯•æ•°æ®")
