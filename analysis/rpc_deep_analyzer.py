#!/usr/bin/env python3
"""
RPCæ·±åº¦åˆ†æå™¨ - ä»comprehensive_analysis.pyæ‹†åˆ†å‡ºæ¥çš„ç‹¬ç«‹æ¨¡å—
ä¸“é—¨è´Ÿè´£RPCæ€§èƒ½çš„æ·±åº¦åˆ†æï¼ŒåŒ…æ‹¬å»¶è¿Ÿè¶‹åŠ¿ã€å¼‚å¸¸æ£€æµ‹ã€æ€§èƒ½æ‚¬å´–æ£€æµ‹ç­‰
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
sys.path.insert(0, project_root)

import pandas as pd
import numpy as np
from utils.unified_logger import get_logger
import traceback
from typing import Dict, Any, Optional

# é…ç½®æ—¥å¿—
logger = get_logger(__name__)


class RpcAnalysisConfig:
    """RPCåˆ†æé…ç½®ç±» - é›†ä¸­ç®¡ç†æ‰€æœ‰é˜ˆå€¼å’Œå‚æ•°"""
    
    # å¼‚å¸¸æ£€æµ‹é…ç½®
    IQR_MULTIPLIER = 1.5  # IQRå¼‚å¸¸æ£€æµ‹å€æ•°
    MIN_LATENCY_THRESHOLD = 30  # æœ€å°å»¶è¿Ÿé˜ˆå€¼(ms)
    SIGMA_MULTIPLIER = 2  # æ ‡å‡†å·®å€æ•°
    
    # åŒæ­¥åˆ†æé…ç½®
    SLOT_SYNC_THRESHOLD = 20  # SlotåŒæ­¥åç§»é˜ˆå€¼
    
    # æ€§èƒ½æ‚¬å´–æ£€æµ‹é…ç½®
    CLIFF_THRESHOLD_ABSOLUTE = 10  # ç»å¯¹å»¶è¿Ÿå¢é•¿é˜ˆå€¼(ms)
    CLIFF_THRESHOLD_PERCENTAGE = 50  # ç›¸å¯¹å»¶è¿Ÿå¢é•¿é˜ˆå€¼(%)
    
    # ç“¶é¢ˆåˆ†ç±»é…ç½®
    HIGH_CPU_THRESHOLD = 85  # é«˜CPUä½¿ç”¨ç‡é˜ˆå€¼(%)
    HIGH_MEMORY_THRESHOLD = 90  # é«˜å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼(%)
    LOW_CPU_THRESHOLD = 30  # ä½CPUä½¿ç”¨ç‡é˜ˆå€¼(%)
    HIGH_LATENCY_THRESHOLD = 50  # é«˜å»¶è¿Ÿé˜ˆå€¼(ms)
    VERY_HIGH_LATENCY_THRESHOLD = 100  # æé«˜å»¶è¿Ÿé˜ˆå€¼(ms)
    SPECIAL_QPS_ANALYSIS = 75000  # ç‰¹æ®ŠQPSåˆ†æç‚¹
    
    # ç›¸å…³æ€§åˆ†æé…ç½®
    STRONG_CORRELATION_THRESHOLD = 0.7  # å¼ºç›¸å…³æ€§é˜ˆå€¼
    MODERATE_CORRELATION_THRESHOLD = 0.5  # ä¸­ç­‰ç›¸å…³æ€§é˜ˆå€¼
    WEAK_CORRELATION_THRESHOLD = 0.3  # å¼±ç›¸å…³æ€§é˜ˆå€¼
    MIN_SAMPLES_FOR_SIGNIFICANCE = 30  # ç»Ÿè®¡æ˜¾è‘—æ€§æœ€å°æ ·æœ¬æ•°
    
    # é«˜QPSåˆ†æé…ç½®
    HIGH_QPS_QUANTILE = 0.8  # é«˜QPSåˆ†ä½æ•°(å‰20%)

# é”™è¯¯å¤„ç†è£…é¥°å™¨
def handle_errors(func):
    """é”™è¯¯å¤„ç†è£…é¥°å™¨ï¼Œä¿è¯ç¨‹åºä¸ä¼šå› ä¸ºå•ä¸ªåŠŸèƒ½å¤±è´¥è€Œå®Œå…¨å´©æºƒ"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥: {str(e)}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼è€Œä¸æ˜¯å´©æºƒ
            if 'analyze' in func.__name__:
                return {}  # åˆ†æå‡½æ•°è¿”å›ç©ºå­—å…¸
            elif 'generate' in func.__name__:
                return ""  # ç”Ÿæˆå‡½æ•°è¿”å›ç©ºå­—ç¬¦ä¸²
            else:
                return None
    return wrapper


class RpcDeepAnalyzer:
    """RPCæ·±åº¦åˆ†æå™¨ - åŸºäºCSVç›‘æ§æ•°æ®è¿›è¡ŒRPCæ€§èƒ½æ·±åº¦åˆ†æ"""

    def __init__(self, csv_file: Optional[str] = None, config: Optional[RpcAnalysisConfig] = None):
        """
        åˆå§‹åŒ–RPCæ·±åº¦åˆ†æå™¨
        
        Args:
            csv_file: CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            config: åˆ†æé…ç½®å¯¹è±¡ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨RpcAnalysisConfigï¼‰
        """
        self.csv_file = csv_file
        self.config = config or RpcAnalysisConfig()
        logger.info(f"ğŸ” åˆå§‹åŒ–RPCæ·±åº¦åˆ†æå™¨ï¼ŒCSVæ–‡ä»¶: {csv_file}")

    @handle_errors
    def analyze_rpc_deep_performance(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        æ‰§è¡ŒRPCæ·±åº¦æ€§èƒ½åˆ†æ
        
        Args:
            df: åŒ…å«ç›‘æ§æ•°æ®çš„DataFrame
            
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        logger.info("ğŸ” å¼€å§‹RPCæ·±åº¦æ€§èƒ½åˆ†æ")
        print("\nğŸ” RPC Deep Performance Analysis")
        print("=" * 50)

        if df is None or len(df) == 0:
            logger.warning("âŒ RPCæ·±åº¦åˆ†ææ— å¯ç”¨æ•°æ®")
            print("âŒ No data available for RPC deep analysis")
            return {}

        # å‡†å¤‡æ•°å€¼æ•°æ®
        numeric_df = self._prepare_numeric_data(df)
        if len(numeric_df) == 0:
            print("âŒ No valid numeric QPS data found")
            return {}

        # æ‰§è¡Œå„é¡¹åˆ†æ
        analysis_results = {
            'latency_trend': self._analyze_latency_trends(numeric_df),
            'anomaly_detection': self._detect_latency_anomalies(numeric_df),
            'slot_sync_analysis': self._analyze_slot_synchronization(numeric_df),
            'correlation_analysis': self._analyze_qps_latency_correlation(numeric_df),
            'performance_cliff': self._detect_performance_cliff(numeric_df),
            'bottleneck_classification': self._classify_bottleneck_type(numeric_df)
        }

        return analysis_results

    def _prepare_numeric_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å‡†å¤‡æ•°å€¼æ•°æ®ï¼Œå¤„ç†æ··åˆç±»å‹é—®é¢˜"""
        print(f"ğŸ“‹ Processing {len(df)} raw data points...")

        # å¤„ç†æ··åˆç±»å‹çš„current_qpsåˆ—
        df_copy = df.copy()
        df_copy['current_qps_str'] = df_copy['current_qps'].astype(str)
        numeric_mask = df_copy['current_qps_str'].str.isdigit()
        numeric_df = df_copy[numeric_mask].copy()

        if len(numeric_df) > 0:
            numeric_df['current_qps'] = pd.to_numeric(numeric_df['current_qps_str'])

            # ç¡®ä¿å…¶ä»–æ•°å€¼åˆ—ä¹Ÿæ˜¯æ•°å€¼ç±»å‹
            numeric_cols = ['rpc_latency_ms', 'cpu_usage', 'mem_usage']
            for col in numeric_cols:
                if col in numeric_df.columns:
                    numeric_df[col] = pd.to_numeric(numeric_df[col], errors='coerce')

        print(f"ğŸ“Š Valid numeric data points: {len(numeric_df)}")
        return numeric_df

    def _analyze_latency_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå»¶è¿Ÿè¶‹åŠ¿"""
        latency_stats = df.groupby('current_qps')['rpc_latency_ms'].agg([
            'mean', 'max', 'std', 'count', 'median'
        ]).round(2)

        return {
            'latency_by_qps': latency_stats,
            'overall_avg_latency': df['rpc_latency_ms'].mean(),
            'overall_max_latency': df['rpc_latency_ms'].max(),
            'latency_std': df['rpc_latency_ms'].std()
        }

    def _detect_latency_anomalies(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ£€æµ‹å»¶è¿Ÿå¼‚å¸¸ - ä½¿ç”¨IQRæ–¹æ³•æ›¿ä»£2Ïƒè§„åˆ™"""
        # IQRå¼‚å¸¸æ£€æµ‹æ–¹æ³• (æ›´é²æ£’ï¼Œé€‚åˆéæ­£æ€åˆ†å¸ƒ)
        Q1 = df['rpc_latency_ms'].quantile(0.25)
        Q3 = df['rpc_latency_ms'].quantile(0.75)
        IQR = Q3 - Q1
        
        # IQRå¼‚å¸¸æ£€æµ‹é˜ˆå€¼
        lower_bound = Q1 - self.config.IQR_MULTIPLIER * IQR
        upper_bound = Q3 + self.config.IQR_MULTIPLIER * IQR
        
        # ç¡®ä¿ä¸‹ç•Œä¸ä¸ºè´Ÿï¼Œä¸Šç•Œè‡³å°‘ä¸ºé…ç½®çš„æœ€å°é˜ˆå€¼
        lower_bound = max(0, lower_bound)
        upper_bound = max(self.config.MIN_LATENCY_THRESHOLD, upper_bound)
        
        # æ£€æµ‹å¼‚å¸¸ (åªå…³æ³¨é«˜å»¶è¿Ÿå¼‚å¸¸)
        high_latency = df[df['rpc_latency_ms'] > upper_bound]
        
        # åŒæ—¶ä¿ç•™2Ïƒæ–¹æ³•ä½œä¸ºå¯¹æ¯”
        avg_latency = df['rpc_latency_ms'].mean()
        std_latency = df['rpc_latency_ms'].std()
        sigma2_threshold = max(self.config.MIN_LATENCY_THRESHOLD, 
                              avg_latency + self.config.SIGMA_MULTIPLIER * std_latency)
        sigma2_anomalies = df[df['rpc_latency_ms'] > sigma2_threshold]

        anomaly_analysis = {
            'method': 'IQR',
            'iqr_threshold': upper_bound,
            'iqr_anomaly_count': len(high_latency),
            'iqr_anomaly_percentage': (len(high_latency) / len(df)) * 100 if len(df) > 0 else 0,
            
            # å¯¹æ¯”2Ïƒæ–¹æ³•
            'sigma2_threshold': sigma2_threshold,
            'sigma2_anomaly_count': len(sigma2_anomalies),
            'sigma2_anomaly_percentage': (len(sigma2_anomalies) / len(df)) * 100 if len(df) > 0 else 0,
            
            # ç»Ÿè®¡ä¿¡æ¯
            'Q1': Q1,
            'Q3': Q3,
            'IQR': IQR,
            'median': df['rpc_latency_ms'].median(),
            'mean': avg_latency,
            'std': std_latency,
            
            'anomaly_samples': [],
            'system_state_during_anomalies': {}
        }

        if len(high_latency) > 0:
            # è®°å½•å¼‚å¸¸æ ·æœ¬ (ä½¿ç”¨IQRæ£€æµ‹ç»“æœ)
            anomaly_analysis['anomaly_samples'] = high_latency[
                ['current_qps', 'rpc_latency_ms', 'cpu_usage', 'mem_usage', 'timestamp']
            ].to_dict('records')

            # åˆ†æå¼‚å¸¸æœŸé—´çš„ç³»ç»ŸçŠ¶æ€
            anomaly_analysis['system_state_during_anomalies'] = {
                'avg_cpu': float(high_latency['cpu_usage'].mean()) if 'cpu_usage' in high_latency.columns else 0.0,
                'avg_memory': float(high_latency['mem_usage'].mean()) if 'mem_usage' in high_latency.columns else 0.0,
                'avg_qps': float(high_latency['current_qps'].mean()) if 'current_qps' in high_latency.columns else 0.0,
                'most_affected_qps_ranges': high_latency['current_qps'].value_counts().head(3).to_dict()
            }

            print(f"âš ï¸  Anomaly latency detection (>{upper_bound:.1f}ms):")
            print(f"Found {len(high_latency)} high latency samples")
            print(f"Average system state during high latency periods:")
            print(f"CPU usage: {anomaly_analysis.get('system_state_during_anomalies', {}).get('avg_cpu', 0):.1f}%")
            print(f"Memory usage: {anomaly_analysis.get('system_state_during_anomalies', {}).get('avg_memory', 0):.1f}%")

        return anomaly_analysis

    def _analyze_slot_synchronization(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æSlotåŒæ­¥çŠ¶æ€"""
        slot_analysis = {
            'sync_data_available': False,
            'sync_issues_count': 0,
            'avg_slot_offset': 0,
            'max_slot_offset': 0,
            'sync_issues_samples': []
        }

        # å¤„ç†slot_diffæ•°æ®
        if 'slot_diff' in df.columns:
            slot_data = df[df['slot_diff'] != 'N/A'].copy()

            if len(slot_data) > 0:
                slot_data['slot_diff_numeric'] = pd.to_numeric(slot_data['slot_diff'], errors='coerce')
                slot_data = slot_data.dropna(subset=['slot_diff_numeric'])

                if len(slot_data) > 0:
                    slot_analysis['sync_data_available'] = True
                    slot_analysis['avg_slot_offset'] = slot_data['slot_diff_numeric'].mean()
                    slot_analysis['max_slot_offset'] = slot_data['slot_diff_numeric'].max()

                    # æ£€æŸ¥åŒæ­¥é—®é¢˜ï¼ˆåç§» > é…ç½®é˜ˆå€¼ï¼‰
                    sync_issues = slot_data[slot_data['slot_diff_numeric'] > self.config.SLOT_SYNC_THRESHOLD]
                    slot_analysis['sync_issues_count'] = len(sync_issues)

                    if len(sync_issues) > 0:
                        slot_analysis['sync_issues_samples'] = sync_issues[
                            ['current_qps', 'slot_diff_numeric', 'rpc_latency_ms']
                        ].to_dict('records')
                        print(f"\nâš ï¸  Found {len(sync_issues)} sync delay samples (Slot offset >{self.config.SLOT_SYNC_THRESHOLD}):")

        return slot_analysis

    def _analyze_qps_latency_correlation(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æQPSä¸å»¶è¿Ÿçš„ç›¸å…³æ€§"""
        correlation = df['current_qps'].corr(df['rpc_latency_ms'])

        correlation_analysis = {
            'correlation_coefficient': correlation,
            'correlation_strength': 'weak',
            'correlation_direction': 'none',
            'interpretation': '',
            'statistical_significance': False
        }

        # ç¡®å®šç›¸å…³æ€§å¼ºåº¦
        abs_correlation = abs(correlation)
        if abs_correlation > self.config.STRONG_CORRELATION_THRESHOLD:
            correlation_analysis['correlation_strength'] = 'strong'
        elif abs_correlation > self.config.MODERATE_CORRELATION_THRESHOLD:
            correlation_analysis['correlation_strength'] = 'moderate'
        elif abs_correlation > self.config.WEAK_CORRELATION_THRESHOLD:
            correlation_analysis['correlation_strength'] = 'weak'
        else:
            correlation_analysis['correlation_strength'] = 'very_weak'

        # ç¡®å®šæ–¹å‘å’Œè§£é‡Š
        if correlation > self.config.MODERATE_CORRELATION_THRESHOLD:
            correlation_analysis['correlation_direction'] = 'positive'
            correlation_analysis['interpretation'] = 'ğŸ” Strong positive correlation found: latency increases significantly with QPS'
        elif correlation < -self.config.MODERATE_CORRELATION_THRESHOLD:
            correlation_analysis['correlation_direction'] = 'negative'
            correlation_analysis['interpretation'] = 'ğŸ” Strong negative correlation found: latency decreases significantly with QPS (unusual phenomenon)'
        elif abs(correlation) > self.config.WEAK_CORRELATION_THRESHOLD:
            correlation_analysis['interpretation'] = 'ğŸ” Moderate correlation: some relationship exists between latency and QPS'
        else:
            correlation_analysis['interpretation'] = 'ğŸ” Weak correlation: latency may be influenced by other factors'

        # ç»Ÿè®¡æ˜¾è‘—æ€§
        if len(df) > self.config.MIN_SAMPLES_FOR_SIGNIFICANCE and abs_correlation > self.config.WEAK_CORRELATION_THRESHOLD:
            correlation_analysis['statistical_significance'] = True

        print(f"\nğŸ“ˆ QPS-Latency correlation analysis:")
        print(f"QPS vs RPC latency correlation coefficient: {correlation:.3f}")
        print(correlation_analysis['interpretation'])

        return correlation_analysis

    def _detect_performance_cliff(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ£€æµ‹æ€§èƒ½æ‚¬å´–"""
        qps_latency_summary = df.groupby('current_qps')['rpc_latency_ms'].agg([
            'mean', 'max', 'count'
        ]).reset_index()
        qps_latency_summary = qps_latency_summary.sort_values('current_qps')

        # è®¡ç®—å»¶è¿Ÿå¢é•¿
        qps_latency_summary['latency_increase'] = qps_latency_summary['mean'].diff()
        qps_latency_summary['latency_increase_percentage'] = (
                qps_latency_summary['latency_increase'] / qps_latency_summary['mean'].shift(1) * 100
        )

        # æ£€æµ‹æ‚¬å´–ç‚¹ï¼ˆå»¶è¿Ÿå¢é•¿ > é…ç½®é˜ˆå€¼ï¼‰
        cliff_points = qps_latency_summary[
            (qps_latency_summary['latency_increase'] > self.config.CLIFF_THRESHOLD_ABSOLUTE) |
            (qps_latency_summary['latency_increase_percentage'] > self.config.CLIFF_THRESHOLD_PERCENTAGE)
        ]

        cliff_analysis = {
            'cliff_points_detected': len(cliff_points),
            'cliff_details': [],
            'performance_degradation_qps': []
        }

        if len(cliff_points) > 0:
            cliff_analysis['cliff_details'] = cliff_points[
                ['current_qps', 'mean', 'latency_increase', 'latency_increase_percentage']
            ].to_dict('records')

            cliff_analysis['performance_degradation_qps'] = cliff_points['current_qps'].tolist()

            print(f"\nğŸ“Š Performance cliff detection:")
            print("âš ï¸  Performance cliff points detected:")
            for _, row in cliff_points.iterrows():
                print(f"QPS {row['current_qps']:,.0f}: latency spike +{row['latency_increase']:.1f}ms")
        else:
            print(f"\nğŸ“Š Performance cliff detection:")
            print("âœ… No significant performance cliff detected")

        return cliff_analysis

    def _classify_bottleneck_type(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç“¶é¢ˆç±»å‹åˆ†ç±»"""
        bottleneck_classification = {
            'primary_bottleneck': 'unknown',
            'bottleneck_confidence': 0.0,  # ä½¿ç”¨ float ç±»å‹ä¿æŒä¸€è‡´æ€§
            'evidence': [],
            'recommendations': []
        }

        # åˆ†æé«˜QPSé˜¶æ®µçš„ç³»ç»ŸçŠ¶æ€
        high_qps_threshold = df['current_qps'].quantile(self.config.HIGH_QPS_QUANTILE)  # é…ç½®çš„é«˜QPSåˆ†ä½æ•°
        high_qps_data = df[df['current_qps'] >= high_qps_threshold]

        if len(high_qps_data) > 0:
            avg_cpu = high_qps_data['cpu_usage'].mean() if 'cpu_usage' in high_qps_data.columns else 0.0
            avg_latency = high_qps_data['rpc_latency_ms'].mean() if 'rpc_latency_ms' in high_qps_data.columns else 0.0
            avg_memory = high_qps_data['mem_usage'].mean() if 'mem_usage' in high_qps_data.columns else 0.0

            print(f"\nğŸ¯ Bottleneck type classification:")

            # ç‰¹æ®Šåˆ†æé…ç½®çš„QPSé˜¶æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            qps_special = df[df['current_qps'] == self.config.SPECIAL_QPS_ANALYSIS] if 'current_qps' in df.columns else pd.DataFrame()
            if len(qps_special) > 0:
                avg_cpu_special = qps_special['cpu_usage'].mean() if 'cpu_usage' in qps_special.columns else 0.0
                avg_latency_special = qps_special['rpc_latency_ms'].mean() if 'rpc_latency_ms' in qps_special.columns else 0.0
                print(f"{self.config.SPECIAL_QPS_ANALYSIS} QPS phase analysis:")
                print(f"  Average CPU: {avg_cpu_special:.1f}%")
                print(f"  Average latency: {avg_latency_special:.1f}ms")

                if avg_latency_special > 20 and avg_cpu_special < self.config.LOW_CPU_THRESHOLD:
                    bottleneck_classification['primary_bottleneck'] = 'rpc_processing'
                    bottleneck_classification['bottleneck_confidence'] = 0.8
                    bottleneck_classification['evidence'].append(
                        f"High latency ({avg_latency_special:.1f}ms) at {self.config.SPECIAL_QPS_ANALYSIS} QPS with low CPU ({avg_cpu_special:.1f}%)")
                    print("ğŸ” Bottleneck type: RPC processing capacity limitation (non-CPU bottleneck)")
                    print("ğŸ’¡ Optimization suggestions:")
                    print("  - Increase RPC thread count")
                    print("  - Optimize network configuration")
                    print("  - Check RPC connection pool settings")

            # é€šç”¨ç“¶é¢ˆåˆ†ç±»é€»è¾‘
            if avg_cpu > self.config.HIGH_CPU_THRESHOLD:
                bottleneck_classification['primary_bottleneck'] = 'cpu'
                bottleneck_classification['bottleneck_confidence'] = 0.8
                bottleneck_classification['evidence'].append(f"High CPU usage: {avg_cpu:.1f}%")
                bottleneck_classification['recommendations'].extend([
                    "Upgrade CPU or optimize CPU-intensive operations",
                    "Consider horizontal scaling"
                ])

            elif avg_memory > self.config.HIGH_MEMORY_THRESHOLD:
                bottleneck_classification['primary_bottleneck'] = 'memory'
                bottleneck_classification['bottleneck_confidence'] = 0.8
                bottleneck_classification['evidence'].append(f"High memory usage: {avg_memory:.1f}%")
                bottleneck_classification['recommendations'].extend([
                    "Increase system memory",
                    "Optimize memory usage patterns"
                ])

            elif avg_latency > self.config.HIGH_LATENCY_THRESHOLD and avg_cpu < self.config.LOW_CPU_THRESHOLD:
                if bottleneck_classification['primary_bottleneck'] == 'unknown':
                    bottleneck_classification['primary_bottleneck'] = 'rpc_processing'
                    bottleneck_classification['bottleneck_confidence'] = 0.7
                bottleneck_classification['evidence'].extend([
                    f"High RPC latency: {avg_latency:.1f}ms",
                    f"Low CPU usage: {avg_cpu:.1f}%"
                ])
                bottleneck_classification['recommendations'].extend([
                    "Increase RPC thread pool size",
                    "Optimize network configuration",
                    "Check RPC connection pool settings"
                ])

            elif avg_latency > self.config.VERY_HIGH_LATENCY_THRESHOLD:
                bottleneck_classification['primary_bottleneck'] = 'network_io'
                bottleneck_classification['bottleneck_confidence'] = 0.6
                bottleneck_classification['evidence'].append(f"Very high latency: {avg_latency:.1f}ms")
                bottleneck_classification['recommendations'].extend([
                    "Check network configuration",
                    "Optimize storage I/O",
                    "Review validator configuration"
                ])
            else:
                bottleneck_classification['primary_bottleneck'] = 'balanced'
                bottleneck_classification['bottleneck_confidence'] = 0.5
                bottleneck_classification['evidence'].append("No clear bottleneck detected")
                bottleneck_classification['recommendations'].append("System appears well-balanced")

        return bottleneck_classification

    def generate_rpc_deep_analysis_report(self, analysis_results: Dict[str, Any]) -> str:
        """ç”ŸæˆRPCæ·±åº¦åˆ†ææŠ¥å‘Š"""
        report = "\n## ğŸ” RPC Deep Performance Analysis Report\n"
        report += "=" * 60 + "\n"

        # å»¶è¿Ÿè¶‹åŠ¿
        latency_trend = analysis_results.get('latency_trend', {})
        if latency_trend:
            report += f"""
### ğŸ“Š Latency Trend Analysis
- **Overall Average Latency**: {latency_trend.get('overall_avg_latency', 0):.1f}ms
- **Maximum Latency**: {latency_trend.get('overall_max_latency', 0):.1f}ms
- **Latency Standard Deviation**: {latency_trend.get('latency_std', 0):.1f}ms

#### Latency by QPS Levels:
"""
            latency_by_qps = latency_trend.get('latency_by_qps')
            if latency_by_qps is not None and len(latency_by_qps) > 0:
                for qps, stats in latency_by_qps.head(10).iterrows():
                    report += f"- **{qps:,} QPS**: Avg {stats['mean']:.1f}ms, Max {stats['max']:.1f}ms, Samples {stats['count']}\n"

        # å¼‚å¸¸æ£€æµ‹
        anomaly = analysis_results.get('anomaly_detection', {})
        if anomaly:
            report += f"""
### âš ï¸ Latency Anomaly Detection
- **Detection Method**: {anomaly.get('method', 'Unknown')}
- **IQR Threshold**: {anomaly.get('iqr_threshold', 0):.1f}ms
- **2Ïƒ Threshold**: {anomaly.get('sigma2_threshold', 0):.1f}ms
- **IQR Anomalies Detected**: {anomaly.get('iqr_anomaly_count', 0)} ({anomaly.get('iqr_anomaly_percentage', 0):.1f}% of samples)
- **2Ïƒ Anomalies Detected**: {anomaly.get('sigma2_anomaly_count', 0)} ({anomaly.get('sigma2_anomaly_percentage', 0):.1f}% of samples)
"""
            if anomaly.get('iqr_anomaly_count', 0) > 0:
                system_state = anomaly.get('system_state_during_anomalies', {})
                report += f"""
#### System State During Anomalies:
- **Average CPU**: {system_state.get('avg_cpu', 0):.1f}%
- **Average Memory**: {system_state.get('avg_memory', 0):.1f}%
- **Average QPS**: {system_state.get('avg_qps', 0):,.0f}
"""

        # ç›¸å…³æ€§åˆ†æ
        correlation = analysis_results.get('correlation_analysis', {})
        if correlation:
            report += f"""
### ğŸ“ˆ QPS-Latency Correlation Analysis
- **Correlation Coefficient**: {correlation.get('correlation_coefficient', 0):.3f}
- **Correlation Strength**: {correlation.get('correlation_strength', 'unknown').title()}
- **Interpretation**: {correlation.get('interpretation', 'No analysis available')}
- **Statistical Significance**: {'Yes' if correlation.get('statistical_significance') else 'No'}
"""

        # æ€§èƒ½æ‚¬å´–
        cliff = analysis_results.get('performance_cliff', {})
        if cliff and cliff.get('cliff_points_detected', 0) > 0:
            report += f"""
### ğŸ“‰ Performance Cliff Detection
- **Cliff Points Detected**: {cliff.get('cliff_points_detected', 0)}

#### Critical QPS Thresholds:
"""
            for cliff_point in cliff.get('cliff_details', []):
                report += f"- **{cliff_point['current_qps']:,} QPS**: Latency spike +{cliff_point['latency_increase']:.1f}ms ({cliff_point.get('latency_increase_percentage', 0):.1f}%)\n"

        # ç“¶é¢ˆåˆ†ç±»
        bottleneck = analysis_results.get('bottleneck_classification', {})
        if bottleneck:
            report += f"""
### ğŸ¯ Bottleneck Classification
- **Primary Bottleneck**: {bottleneck.get('primary_bottleneck', 'unknown').replace('_', ' ').title()}
- **Confidence Level**: {bottleneck.get('bottleneck_confidence', 0) * 100:.0f}%

#### Evidence:
"""
            for evidence in bottleneck.get('evidence', []):
                report += f"- {evidence}\n"

            report += "\n#### Recommendations:\n"
            for rec in bottleneck.get('recommendations', []):
                report += f"- {rec}\n"

        return report


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ“‹ RPCæ·±åº¦åˆ†æå™¨ä½¿ç”¨ç¤ºä¾‹:")
    print("from rpc_deep_analyzer import RpcDeepAnalyzer")
    print("analyzer = RpcDeepAnalyzer('data.csv')")
    print("results = analyzer.analyze_rpc_deep_performance(df)")
    print("report = analyzer.generate_rpc_deep_analysis_report(results)")
